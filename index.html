<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Protein Design</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Protein Design</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.20958.pdf' target='_blank'>https://arxiv.org/pdf/2512.20958.pdf</a></span>   <span><a href='https://github.com/YadunandanRaman/ReACT-Drug/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>R Yadunandan, Nimisha Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20958">ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.15410.pdf' target='_blank'>https://arxiv.org/pdf/2512.15410.pdf</a></span>   <span><a href='https://github.com/SimonBon/CIM-S' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Gutwein, Arthur Longuefosse, Jun Seita, Sabine Taschner-Mandl, Roxane Licandro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15410">Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.06752.pdf' target='_blank'>https://arxiv.org/pdf/2512.06752.pdf</a></span>   <span><a href='https://github.com/VirtualProteins/GNN_UNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Vivian Li, Linus Leong, Vladimir Radenkovic, Pietro Liò, Chaitanya K. Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06752">Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.05245.pdf' target='_blank'>https://arxiv.org/pdf/2512.05245.pdf</a></span>   <span><a href='https://github.com/boun-tabi-lifelu/stargo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Efe Akça, Gökçe Uludoğan, Arzucan Özgür, İnci M. Baytaş
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05245">STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.05080.pdf' target='_blank'>https://arxiv.org/pdf/2512.05080.pdf</a></span>   <span><a href='https://github.com/gnina/OMTRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Dunn, Liv Toft, Tyler Katz, Juhi Gupta, Riya Shah, Ramith Hettiarachchi, David R. Koes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05080">OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.00708.pdf' target='_blank'>https://arxiv.org/pdf/2512.00708.pdf</a></span>   <span><a href='https://github.com/ZhiGroup/DAVIS-complete' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Hsiu Wu, Ziqian Xie, Shuiwang Ji, Degui Zhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00708">Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2511.11758.pdf' target='_blank'>https://arxiv.org/pdf/2511.11758.pdf</a></span>   <span><a href='https://github.com/shiningsunnyday/PT-BPE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, Marinka Zitnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11758">Protein Structure Tokenization via Geometric Byte Pair Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2511.10056.pdf' target='_blank'>https://arxiv.org/pdf/2511.10056.pdf</a></span>   <span><a href='https://github.com/IDEA-XL/TokenMD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijing Liu, Bin Feng, He Cao, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10056">From Static Structures to Ensembles: Studying and Harnessing Protein Structure Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure tokenization converts 3D structures into discrete or vectorized representations, enabling the integration of structural and sequence data. Despite many recent works on structure tokenization, the properties of the underlying discrete representations are not well understood. In this work, we first demonstrate that the successful utilization of structural tokens in a language model for structure prediction depends on using rich, pre-trained sequence embeddings to bridge the semantic gap between the sequence and structural "language". The analysis of the structural vocabulary itself then reveals significant semantic redundancy, where multiple distinct tokens correspond to nearly identical local geometries, acting as "structural synonyms". This redundancy, rather than being a flaw, can be exploited with a simple "synonym swap" strategy to generate diverse conformational ensembles by perturbing a predicted structure with its structural synonyms. This computationally lightweight method accurately recapitulates protein flexibility, performing competitively with state-of-the-art models. Our study provides fundamental insights into the nature of discrete protein structure representations and introduces a powerful, near-instantaneous method for modeling protein dynamics. Source code is available in https://github.com/IDEA-XL/TokenMD.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2511.07390.pdf' target='_blank'>https://arxiv.org/pdf/2511.07390.pdf</a></span>   <span><a href='https://github.com/baronet2/SCISOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Baron, Alan N. Amin, Ruben Weitzman, Debora Marks, Andrew Gordon Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07390">A Diffusion Model to Shrink Proteins While Maintaining Their Function</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many proteins useful in modern medicine or bioengineering are challenging to make in the lab, fuse with other proteins in cells, or deliver to tissues in the body, because their sequences are too long. Shortening these sequences typically involves costly, time-consuming experimental campaigns. Ideally, we could instead use modern models of massive databases of sequences from nature to learn how to propose shrunken proteins that resemble sequences found in nature. Unfortunately, these models struggle to efficiently search the combinatorial space of all deletions, and are not trained with inductive biases to learn how to delete. To address this gap, we propose SCISOR, a novel discrete diffusion model that deletes letters from sequences to generate protein samples that resemble those found in nature. To do so, SCISOR trains a de-noiser to reverse a forward noising process that adds random insertions to natural sequences. As a generative model, SCISOR fits evolutionary sequence data competitively with previous large models. In evaluation, SCISOR achieves state-of-the-art predictions of the functional effects of deletions on ProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein sequences, and show that its suggested deletions result in significantly more realistic proteins and more often preserve functional motifs than previous models of evolutionary sequences.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.25132.pdf' target='_blank'>https://arxiv.org/pdf/2510.25132.pdf</a></span>   <span><a href='https://github.com/Vecteur-libre/EnzyControl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Song, Zhiyuan Liu, Han Huang, Liang Wang, Qiong Wang, Jianyu Shi, Hui Yu, Yihang Zhou, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25132">EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\% in designability and 13\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.24826.pdf' target='_blank'>https://arxiv.org/pdf/2510.24826.pdf</a></span>   <span><a href='https://github.com/COLA-Laboratory/GraphFLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Huang, Shasha Zhou, Ke Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24826">Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models increasingly map biological sequence-fitness landscapes to predict mutational effects. Effective evaluation of these models requires benchmarks curated from empirical data. Despite their impressive scales, existing benchmarks lack topographical information regarding the underlying fitness landscapes, which hampers interpretation and comparison of model performance beyond averaged scores. Here, we introduce GraphFLA, a Python framework that constructs and analyzes fitness landscapes from mutagensis data in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions of mutants. GraphFLA calculates 20 biologically relevant features that characterize 4 fundamental aspects of landscape topography. By applying GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we demonstrate its utility in interpreting and comparing the performance of dozens of fitness prediction models, highlighting factors influencing model accuracy and respective advantages of different models. In addition, we release 155 combinatorially complete empirical fitness landscapes, encompassing over 2.2 million sequences across various modalities. All the codes and datasets are available at https://github.com/COLA-Laboratory/GraphFLA.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.24053.pdf' target='_blank'>https://arxiv.org/pdf/2510.24053.pdf</a></span>   <span><a href='https://github.com/JBEI/foldy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob B. Roberts, Catherine R. Ji, Isaac Donnell, Thomas D. Young, Allison N. Pearson, Graham A. Hudson, Leah S. Keiser, Mia Wesselkamper, Peter H. Winegar, Janik Ludwig, Sarah H. Klass, Isha V. Sheth, Ezechinyere C. Ukabiala, Maria C. T. Astolfi, Benjamin Eysenbach, Jay D. Keasling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24053">Low-N Protein Activity Optimization with FolDE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are traditionally optimized through the costly construction and measurement of many mutants. Active Learning-assisted Directed Evolution (ALDE) alleviates that cost by predicting the best improvements and iteratively testing mutants to inform predictions. However, existing ALDE methods face a critical limitation: selecting the highest-predicted mutants in each round yields homogeneous training data insufficient for accurate prediction models in subsequent rounds. Here we present FolDE, an ALDE method designed to maximize end-of-campaign success. In simulations across 20 protein targets, FolDE discovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005) and is 55% more likely to find top 1% mutants. FolDE achieves this primarily through naturalness-based warm-starting, which augments limited activity measurements with protein language model outputs to improve activity prediction. We also introduce a constant-liar batch selector, which improves batch diversity; this is important in multi-mutation campaigns but had limited effect in our benchmarks. The complete workflow is freely available as open-source software, making efficient protein optimization accessible to any laboratory.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.19474.pdf' target='_blank'>https://arxiv.org/pdf/2510.19474.pdf</a></span>   <span><a href='https://nips2025fm4ls.github.io/pages/accepted-paper.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Constance Ferragu, Jonathan D. Ziegler, Nicolas Deutschmann, Arthur Lindoulsi, Eli Bixby, Cradle ML Team
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19474">g-DPO: Scalable Preference Optimization for Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Direct Preference Optimization (DPO) is an effective approach for aligning protein language models with experimental design goals. However, DPO faces a scalability bottleneck: the number of possible training pairs grows quadratically with the number of labeled sequences, leading to prohibitive training times even for modestly sized datasets. We introduce g-DPO, a framework that (i) uses sequence space clustering to prune redundant pairs while preserving training signal, and (ii) amortizes likelihood computations with group-based approximations. Across three protein engineering tasks, g-DPO maintains in-silico and in-vitro performance that is statistically indistinguishable from standard DPO, while converging 1.8 to 3.7 times faster, with greater gains expected as the size of the dataset increases.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.14586.pdf' target='_blank'>https://arxiv.org/pdf/2510.14586.pdf</a></span>   <span><a href='https://github.com/LigandPro/Matcha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daria Frolova, Talgat Daulbaev, Egor Sevryugov, Sergei A. Nikolenko, Dmitry N. Ivankov, Ivan Oseledets, Marina A. Pak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14586">Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding poses is crucial for structure-based drug design, yet existing methods struggle to balance speed, accuracy, and physical plausibility. We introduce Matcha, a novel molecular docking pipeline that combines multi-stage flow matching with learned scoring and physical validity filtering. Our approach consists of three sequential stages applied consecutively to refine docking predictions, each implemented as a flow matching model operating on appropriate geometric spaces ($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the prediction quality through a dedicated scoring model and apply unsupervised physical validity filters to eliminate unrealistic poses. Compared to various approaches, Matcha demonstrates superior performance on Astex and PDBbind test sets in terms of docking success rate and physical plausibility. Moreover, our method works approximately 25 times faster than modern large-scale co-folding models. The model weights and inference code to reproduce our results are available at https://github.com/LigandPro/Matcha.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.13926.pdf' target='_blank'>https://arxiv.org/pdf/2510.13926.pdf</a></span>   <span><a href='https://github.com/CyL-ucas/BioMed_Search' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Congying Liu, Xingyuan Wei, Peipei Liu, Yiqing Shen, Yanxu Mao, Tiehan Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13926">BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biomedical queries often rely on a deep understanding of specialized knowledge such as gene regulatory mechanisms and pathological processes of diseases. They require detailed analysis of complex physiological processes and effective integration of information from multiple data sources to support accurate retrieval and reasoning. Although large language models (LLMs) perform well in general reasoning tasks, their generated biomedical content often lacks scientific rigor due to the inability to access authoritative biomedical databases and frequently fabricates protein functions, interactions, and structural details that deviate from authentic information. Therefore, we present BioMedSearch, a multi-source biomedical information retrieval framework based on LLMs. The method integrates literature retrieval, protein database and web search access to support accurate and efficient handling of complex biomedical queries. Through sub-queries decomposition, keywords extraction, task graph construction, and multi-source information filtering, BioMedSearch generates high-quality question-answering results. To evaluate the accuracy of question answering, we constructed a multi-level dataset, BioMedMCQs, consisting of 3,000 questions. The dataset covers three levels of reasoning: mechanistic identification, non-adjacent semantic integration, and temporal causal reasoning, and is used to assess the performance of BioMedSearch and other methods on complex QA tasks. Experimental results demonstrate that BioMedSearch consistently improves accuracy over all baseline models across all levels. Specifically, at Level 1, the average accuracy increases from 59.1% to 91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging Level 3, the average accuracy improves from 36.3% to 73.4%. The code and BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.11752.pdf' target='_blank'>https://arxiv.org/pdf/2510.11752.pdf</a></span>   <span><a href='https://github.com/ZW471/PLASMA-Protein-Local-Alignment.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Wang, Bingxin Zhou, Jing Wang, Yang Tan, Weishu Zhao, Pietro Liò, Liang Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11752">Fast and Interpretable Protein Substructure Alignment via Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are essential biological macromolecules that execute life functions. Local motifs within protein structures, such as active sites, are the most critical components for linking structure to function and are key to understanding protein evolution and enabling protein engineering. Existing computational methods struggle to identify and compare these local structures, which leaves a significant gap in understanding protein structures and harnessing their functions. This study presents PLASMA, the first deep learning framework for efficient and interpretable residue-level protein substructure alignment. We reformulate the problem as a regularized optimal transport task and leverage differentiable Sinkhorn iterations. For a pair of input protein structures, PLASMA outputs a clear alignment matrix with an interpretable overall similarity score. Through extensive quantitative evaluations and three biological case studies, we demonstrate that PLASMA achieves accurate, lightweight, and interpretable residue-level alignment. Additionally, we introduce PLASMA-PF, a training-free variant that provides a practical alternative when training data are unavailable. Our method addresses a critical gap in protein structure analysis tools and offers new opportunities for functional annotation, evolutionary studies, and structure-based drug design. Reproducibility is ensured via our official implementation at https://github.com/ZW471/PLASMA-Protein-Local-Alignment.git.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.10634.pdf' target='_blank'>https://arxiv.org/pdf/2510.10634.pdf</a></span>   <span><a href='https://github.com/OnlyLoveKFC/ProteinAE_v1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoning Li, Le Zhuo, Yusong Wang, Mingyu Li, Xinheng He, Fandi Wu, Hongsheng Li, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10634">ProteinAE: Protein Diffusion Autoencoders for Structure Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing effective representations of protein structures is essential for advancing protein science, particularly for protein generative modeling. Current approaches often grapple with the complexities of the SE(3) manifold, rely on discrete tokenization, or the need for multiple training objectives, all of which can hinder the model optimization and generalization. We introduce ProteinAE, a novel and streamlined protein diffusion autoencoder designed to overcome these challenges by directly mapping protein backbone coordinates from E(3) into a continuous, compact latent space. ProteinAE employs a non-equivariant Diffusion Transformer with a bottleneck design for efficient compression and is trained end-to-end with a single flow matching objective, substantially simplifying the optimization pipeline. We demonstrate that ProteinAE achieves state-of-the-art reconstruction quality, outperforming existing autoencoders. The resulting latent space serves as a powerful foundation for a latent diffusion model that bypasses the need for explicit equivariance. This enables efficient, high-quality structure generation that is competitive with leading structure-based approaches and significantly outperforms prior latent-based methods. Code is available at https://github.com/OnlyLoveKFC/ProteinAE_v1.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.10020.pdf' target='_blank'>https://arxiv.org/pdf/2510.10020.pdf</a></span>   <span><a href='https://github.com/smithhenryd/cgm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry D. Smith, Nathaniel L. Diamant, Brian L. Trippe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10020">Calibrating Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models frequently suffer miscalibration, wherein class probabilities and other statistics of the sampling distribution deviate from desired values. We frame calibration as a constrained optimization problem and seek the closest model in Kullback-Leibler divergence satisfying calibration constraints. To address the intractability of imposing these constraints exactly, we introduce two surrogate objectives for fine-tuning: (1) the relax loss, which replaces the constraint with a miscalibration penalty, and (2) the reward loss, which converts calibration into a reward fine-tuning problem. We demonstrate that these approaches substantially reduce calibration error across hundreds of simultaneous constraints and models with up to one billion parameters, spanning applications in protein design, image generation, and language modeling.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.08169.pdf' target='_blank'>https://arxiv.org/pdf/2510.08169.pdf</a></span>   <span><a href='https://github.com/BEAM-Labs/denovo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Jiaqi Wei, Zijie Qiu, Sheng Xu, Zhi Jin, ZhiQiang Gao, Nanqing Dong, Siqi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08169">Bidirectional Representations Augmented Autoregressive Biological Sequence Generation:Application in De Novo Peptide Sequencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive (AR) models, common in sequence generation, are limited in many biological tasks such as de novo peptide sequencing and protein modeling by their unidirectional nature, failing to capture crucial global bidirectional token dependencies. Non-Autoregressive (NAR) models offer holistic, bidirectional representations but face challenges with generative coherence and scalability. To transcend this, we propose a hybrid framework enhancing AR generation by dynamically integrating rich contextual information from non-autoregressive mechanisms. Our approach couples a shared input encoder with two decoders: a non-autoregressive one learning latent bidirectional biological features, and an AR decoder synthesizing the biological sequence by leveraging these bidirectional features. A novel cross-decoder attention module enables the AR decoder to iteratively query and integrate these bidirectional features, enriching its predictions. This synergy is cultivated via a tailored training strategy with importance annealing for balanced objectives and cross-decoder gradient blocking for stable, focused learning. Evaluations on a demanding nine-species benchmark of de novo peptide sequencing show that our model substantially surpasses AR and NAR baselines. It uniquely harmonizes AR stability with NAR contextual awareness, delivering robust, superior performance on diverse downstream data. This research advances biological sequence modeling techniques and contributes a novel architectural paradigm for augmenting AR models with enhanced bidirectional understanding for complex sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.07286.pdf' target='_blank'>https://arxiv.org/pdf/2510.07286.pdf</a></span>   <span><a href='https://github.com/aim-uofa/EvoIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Xiaoran Jiao, Shengdong Lin, Zhanming Liang, Weian Mao, Chenchen Jing, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07286">Evolutionary Profiles for Protein Fitness Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.01571.pdf' target='_blank'>https://arxiv.org/pdf/2510.01571.pdf</a></span>   <span><a href='https://github.com/chq1155/RL-PLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqun Cao, Hongrui Zhang, Junde Xu, Zhou Zhang, Lingdong Shen, Minghao Sun, Ge Liu, Jinbo Xu, Wu-Jun Li, Jinren Ni, Cesar de la Fuente-Nunez, Tianfan Fu, Yejin Choi, Pheng-Ann Heng, Fang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01571">From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.00351.pdf' target='_blank'>https://arxiv.org/pdf/2510.00351.pdf</a></span>   <span><a href='https://github.com/rdilip/kanzi/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Dilip, Evan Zhang, Ayush Varshney, David Van Valen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00351">Flow Autoencoders are Effective Protein Tokenizers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models. Code is available here: https://github.com/rdilip/kanzi/.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2509.25225.pdf' target='_blank'>https://arxiv.org/pdf/2509.25225.pdf</a></span>   <span><a href='https://github.com/xulong0826/MSCoD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xu, Yongcai Chen, Fengshuo Liu, Yuzhong Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25225">MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-Based Drug Design (SBDD) is a powerful strategy in computational drug discovery, utilizing three-dimensional protein structures to guide the design of molecules with improved binding affinity. However, capturing complex protein-ligand interactions across multiple scales remains challenging, as current methods often overlook the hierarchical organization and intrinsic asymmetry of these interactions. To address these limitations, we propose MSCoD, a novel Bayesian updating-based generative framework for structure-based drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was developed, which enables semantic compression at multiple abstraction levels for efficient hierarchical feature extraction. Furthermore, a multi-head cooperative attention (MHCA) mechanism was developed, which employs asymmetric protein-to-ligand attention to capture diverse interaction types while addressing the dimensionality disparity between proteins and ligands. Empirical studies showed that MSCoD outperforms state-of-the-art methods on the benchmark dataset. Case studies on challenging targets such as KRAS G12D further demonstrate its applicability in real-world scenarios. The code and data underlying this article are freely available at https://github.com/xulong0826/MSCoD.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2509.24262.pdf' target='_blank'>https://arxiv.org/pdf/2509.24262.pdf</a></span>   <span><a href='https://github.com/NimishaGhosh/LAMP-PRo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nimisha Ghosh, Dheeran Sankaran, Rahul Balakrishnan Adhi, Sharath S, Amrut Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24262">LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the understanding of cell function, molecular interactions as well as regulatory functions. Owing to their high similarity, most of the existing approaches face challenges in differentiating between DBPs and RBPs leading to high cross-prediction errors. Moreover, identifying proteins which bind to both DNA and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a novel framework viz. LAMP-PRo which is based on pre-trained protein language model (PLM), attention mechanisms and multi-label learning to mitigate these issues. First, pre-trained PLM such ESM-2 is used for embedding the protein sequences followed by convolutional neural network (CNN). Subsequently multi-head self-attention mechanism is applied for the contextual information while label-aware attention is used to compute class-specific representations by attending to the sequence in a way that is tailored to each label (DBP, RBP and non-NABP) in a multi-label setup. We have also included a novel cross-label attention mechanism to explicitly capture dependencies between DNA- and RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a linear layer followed by a sigmoid function are used for the final prediction. Extensive experiments are carried out to compare LAMP-PRo with the existing methods wherein the proposed model shows consistent competent performance. Furthermore, we also provide visualization to showcase model interpretability, highlighting which parts of the sequence are most relevant for a predicted label. The original datasets are available at http://bliulab.net/iDRBP\_MMC and the codes are available at https://github.com/NimishaGhosh/LAMP-PRo.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2509.08707.pdf' target='_blank'>https://arxiv.org/pdf/2509.08707.pdf</a></span>   <span><a href='https://github.com/prescient-design/igloo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ada Fang, Robert G. Alberstein, Simon Kelow, FrÃ©dÃ©ric A. Dreyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08707">Tokenizing Loops of Antibodies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complementarity-determining regions of antibodies are loop structures that are key to their interactions with antigens, and of high importance to the design of novel biologics. Since the 1980s, categorizing the diversity of CDR structures into canonical clusters has enabled the identification of key structural motifs of antibodies. However, existing approaches have limited coverage and cannot be readily incorporated into protein foundation models. Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody loop tokenizer that encodes backbone dihedral angles and sequence. Igloo is trained using a contrastive learning objective to map loops with similar backbone dihedral angles closer together in latent space. Igloo can efficiently retrieve the closest matching loop structures from a structural antibody database, outperforming existing methods on identifying similar H3 loops by 5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issue of canonical clusters, while retaining the ability to recover canonical loop conformations. To demonstrate the versatility of Igloo tokens, we show that they can be incorporated into protein language models with IglooLM and IglooALM. On predicting binding affinity of heavy chain variants, IglooLM outperforms the base protein language model on 8 out of 10 antibody-antigen targets. Additionally, it is on par with existing state-of-the-art sequence-based and multimodal protein language models, performing comparably to models with $7\times$ more parameters. IglooALM samples antibody loops which are diverse in sequence and more consistent in structure than state-of-the-art antibody inverse folding models. Igloo demonstrates the benefit of introducing multimodal tokens for antibody loops for encoding the diverse landscape of antibody loops, improving protein foundation models, and for antibody CDR design.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.05757.pdf' target='_blank'>https://arxiv.org/pdf/2509.05757.pdf</a></span>   <span><a href='https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05757">Hyperbolic Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2509.03487.pdf' target='_blank'>https://arxiv.org/pdf/2509.03487.pdf</a></span>   <span><a href='https://github.com/jigang-fan/SafeProtein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang, Zaixi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03487">SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.18211.pdf' target='_blank'>https://arxiv.org/pdf/2508.18211.pdf</a></span>   <span><a href='https://github.com/graeter-group/flips' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vsevolod Viliuga, Leif Seute, Nicolas Wolf, Simon Wagner, Arne Elofsson, Jan StÃ¼hmer, Frauke GrÃ¤ter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18211">Flexibility-Conditioned Protein Structure Design with Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in geometric deep learning and generative modeling have enabled the design of novel proteins with a wide range of desired properties. However, current state-of-the-art approaches are typically restricted to generating proteins with only static target properties, such as motifs and symmetries. In this work, we take a step towards overcoming this limitation by proposing a framework to condition structure generation on flexibility, which is crucial for key functionalities such as catalysis or molecular recognition. We first introduce BackFlip, an equivariant neural network for predicting per-residue flexibility from an input backbone structure. Relying on BackFlip, we propose FliPS, an SE(3)-equivariant conditional flow matching model that solves the inverse problem, that is, generating backbones that display a target flexibility profile. In our experiments, we show that FliPS is able to generate novel and diverse protein backbones with the desired flexibility, verified by Molecular Dynamics (MD) simulations. FliPS and BackFlip are available at https://github.com/graeter-group/flips .
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.17389.pdf' target='_blank'>https://arxiv.org/pdf/2508.17389.pdf</a></span>   <span><a href='https://github.com/Bokai-Zhao/NPF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bokai Zhao, Weiyang Shi, Hanqing Chao, Zijiang Yang, Yiyang Zhang, Ming Song, Tianzi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17389">Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.17345.pdf' target='_blank'>https://arxiv.org/pdf/2508.17345.pdf</a></span>   <span><a href='https://github.com/GenSI-THUAIR/SLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Song, Zhe Zhang, Yu Pei, Jingjing Gong, Qiying Yu, Zheng Zhang, Mingxuan Wang, Hao Zhou, Jingjing Liu, Wei-Ying Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17345">ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling of discrete variables is challenging yet crucial for applications in natural language processing and biological sequence design. We introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model inspired by progressive candidate pruning. SLM operates on simplex centroids, reducing generation complexity and enhancing scalability. Additionally, SLM incorporates a flexible implementation of classifier-free guidance, enhancing unconditional generation performance. Extensive experiments on DNA promoter and enhancer design, protein design, character-level and large-vocabulary language modeling demonstrate the competitive performance and strong potential of SLM. Our code can be found at https://github.com/GenSI-THUAIR/SLM
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.06021.pdf' target='_blank'>https://arxiv.org/pdf/2508.06021.pdf</a></span>   <span><a href='https://github.com/utkuozbulak/svp-generative-ai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Utku Ozbulak, Michaela Cohrs, Hristo L. Svilenov, Joris Vankerschaver, Wesley De Neve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06021">Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2507.21260.pdf' target='_blank'>https://arxiv.org/pdf/2507.21260.pdf</a></span>   <span><a href='https://github.com/amartya21/Adam-PnP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartya Banerjee, Xingyu Xu, Caroline MoosmÃ¼ller, Harlin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21260">Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2507.20925.pdf' target='_blank'>https://arxiv.org/pdf/2507.20925.pdf</a></span>   <span><a href='https://github.com/Hoch-Zhang/PSRP-CPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongzhi Zhang, Zhonglie Liu, Kun Meng, Jiameng Chen, Jia Wu, Bo Du, Di Lin, Yan Che, Wenbin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20925">Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the vastness of chemical space and the ongoing emergence of previously uncharacterized proteins, zero-shot compound-protein interaction (CPI) prediction better reflects the practical challenges and requirements of real-world drug development. Although existing methods perform adequately during certain CPI tasks, they still face the following challenges: (1) Representation learning from local or complete protein sequences often overlooks the complex interdependencies between subsequences, which are essential for predicting spatial structures and binding properties. (2) Dependence on large-scale or scarce multimodal protein datasets demands significant training data and computational resources, limiting scalability and efficiency. To address these challenges, we propose a novel approach that pretrains protein representations for CPI prediction tasks using subsequence reordering, explicitly capturing the dependencies between protein subsequences. Furthermore, we apply length-variable protein augmentation to ensure excellent pretraining performance on small training datasets. To evaluate the model's effectiveness and zero-shot learning ability, we combine it with various baseline methods. The results demonstrate that our approach can improve the baseline model's performance on the CPI task, especially in the challenging zero-shot scenario. Compared to existing pre-training models, our model demonstrates superior performance, particularly in data-scarce scenarios where training samples are limited. Our implementation is available at https://github.com/Hoch-Zhang/PSRP-CPI.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2507.20243.pdf' target='_blank'>https://arxiv.org/pdf/2507.20243.pdf</a></span>   <span><a href='https://github.com/BruthYU/protein-se3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Yu, Zhangyang Gao, Cheng Tan, Qin Chen, Jie Zhou, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20243">Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SE(3)-based generative models have shown great promise in protein geometry modeling and effective structure design. However, the field currently lacks a modularized benchmark to enable comprehensive investigation and fair comparison of different methods. In this paper, we propose Protein-SE(3), a new benchmark based on a unified training framework, which comprises protein scaffolding tasks, integrated generative models, high-level mathematical abstraction, and diverse evaluation metrics. Recent advanced generative models designed for protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2), Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and FrameFlow) are integrated into our framework. All integrated methods are fairly investigated with the same training dataset and evaluation metrics. Furthermore, we provide a high-level abstraction of the mathematical foundations behind the generative models, enabling fast prototyping of future algorithms without reliance on explicit protein structures. Accordingly, we release the first comprehensive benchmark built upon unified training framework for SE(3)-based protein structure design, which is publicly accessible at https://github.com/BruthYU/protein-se3.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2507.19523.pdf' target='_blank'>https://arxiv.org/pdf/2507.19523.pdf</a></span>   <span><a href='https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Su, Xiner Li, Yuchao Lin, Ziqian Xie, Degui Zhi, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19523">Language Models for Controllable DNA Sequence Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2507.17731.pdf' target='_blank'>https://arxiv.org/pdf/2507.17731.pdf</a></span>   <span><a href='https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17731">Flow Matching Meets Biology and Life Science: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2507.14156.pdf' target='_blank'>https://arxiv.org/pdf/2507.14156.pdf</a></span>   <span><a href='https://github.com/ykiiiiii/ADFLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yi, Kiarash Jamali, Sjors H. W. Scheres
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14156">All-atom inverse protein folding through discrete flow matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2507.10136.pdf' target='_blank'>https://arxiv.org/pdf/2507.10136.pdf</a></span>   <span><a href='https://github.com/Liu-Zhonglin/pbn-melanoma-project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonglin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10136">A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic Strategy in Melanoma</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ''hit-and-run" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2507.09054.pdf' target='_blank'>https://arxiv.org/pdf/2507.09054.pdf</a></span>   <span><a href='https://github.com/prescient-design/ibex,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>FrÃ©dÃ©ric A. Dreyer, Jan Ludwiczak, Karolis Martinkus, Brennan Abanades, Robert G. Alberstein, Pan Kessel, Pranav Rao, Jae Hyeon Lee, Richard Bonneau, Andrew M. Watkins, Franziska Seeger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09054">Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Ibex, a pan-immunoglobulin structure prediction model that achieves state-of-the-art accuracy in modeling the variable domains of antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex explicitly distinguishes between bound and unbound protein conformations by training on labeled apo and holo structural pairs, enabling accurate prediction of both states at inference time. Using a comprehensive private dataset of high-resolution antibody structures, we demonstrate superior out-of-distribution performance compared to existing specialized and general protein structure prediction tools. Ibex combines the accuracy of cutting-edge models with significantly reduced computational requirements, providing a robust foundation for accelerating large molecule design and therapeutic development.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.08980.pdf' target='_blank'>https://arxiv.org/pdf/2507.08980.pdf</a></span>   <span><a href='https://github.com/ChenyuWang-Monica/REED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Wang, Cai Zhou, Sharut Gupta, Zongyu Lin, Stefanie Jegelka, Stephen Bates, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08980">Learning Diffusion Models with Flexible Representation Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.05503.pdf' target='_blank'>https://arxiv.org/pdf/2507.05503.pdf</a></span>   <span><a href='https://github.com/huang3170/MolForm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Huang, Daiheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05503">MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) seeks to generate molecules that bind effectively to protein targets by leveraging their 3D structural information. While diffusion-based generative models have become the predominant approach for SBDD, alternative non-autoregressive frameworks remain relatively underexplored. In this work, we introduce MolFORM, a novel generative framework that jointly models discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. To further enhance generation quality, we incorporate a preference-guided fine-tuning stage based on Direct Preference Optimization (DPO), using Vina score as a reward signal. We propose a multi-modal flow DPO co-modeling strategy that simultaneously aligns discrete and continuous modalities, leading to consistent improvements across multiple evaluation metrics. The code is provided at: https://github.com/huang3170/MolForm.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.05502.pdf' target='_blank'>https://arxiv.org/pdf/2507.05502.pdf</a></span>   <span><a href='https://github.com/LDeng0205/StaB-ddG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Deng, Karsten Householder, Fang Wu, Sebastian Thrun, K. Christopher Garcia, Brian Trippe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05502">Predicting mutational effects on protein binding from folding energy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate estimation of mutational effects on protein-protein binding energies is an open problem with applications in structural biology and therapeutic design. Several deep learning predictors for this task have been proposed, but, presumably due to the scarcity of binding data, these methods underperform computationally expensive estimates based on empirical force fields. In response, we propose a transfer-learning approach that leverages advances in protein sequence modeling and folding stability prediction for this task. The key idea is to parameterize the binding energy as the difference between the folding energy of the protein complex and the sum of the folding energies of its binding partners. We show that using a pre-trained inverse-folding model as a proxy for folding energy provides strong zero-shot performance, and can be fine-tuned with (1) copious folding energy measurements and (2) more limited binding energy measurements. The resulting predictor, StaB-ddG, is the first deep learning predictor to match the accuracy of the state-of-the-art empirical force-field method FoldX, while offering an over 1,000x speed-up.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.05101.pdf' target='_blank'>https://arxiv.org/pdf/2507.05101.pdf</a></span>   <span><a href='https://github.com/SophieSarceau/PRING' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05101">PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2506.20686.pdf' target='_blank'>https://arxiv.org/pdf/2506.20686.pdf</a></span>   <span><a href='https://github.com/Supercomputing-System-AI-Lab/MegaFold/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20686">MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2506.19482.pdf' target='_blank'>https://arxiv.org/pdf/2506.19482.pdf</a></span>   <span><a href='https://github.com/GLAD-RUC/DistEGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Zhang, Jiacheng Cen, Jiaqi Han, Wenbing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19482">Fast and Distributed Equivariant Graph Neural Networks by Virtual Node Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equivariant Graph Neural Networks (GNNs) have achieved remarkable success across diverse scientific applications. However, existing approaches face critical efficiency challenges when scaling to large geometric graphs and suffer significant performance degradation when the input graphs are sparsified for computational tractability. To address these limitations, we introduce FastEGNN and DistEGNN, two novel enhancements to equivariant GNNs for large-scale geometric graphs. FastEGNN employs a key innovation: a small ordered set of virtual nodes that effectively approximates the large unordered graph of real nodes. Specifically, we implement distinct message passing and aggregation mechanisms for different virtual nodes to ensure mutual distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual and real coordinates to achieve global distributedness. This design enables FastEGNN to maintain high accuracy while efficiently processing large-scale sparse graphs. For extremely large-scale geometric graphs, we present DistEGNN, a distributed extension where virtual nodes act as global bridges between subgraphs in different devices, maintaining consistency while dramatically reducing memory and computational overhead. We comprehensively evaluate our models across four challenging domains: N-body systems (100 nodes), protein dynamics (800 nodes), Water-3D (8,000 nodes), and our new Fluid113K benchmark (113,000 nodes). Results demonstrate superior efficiency and performance, establishing new capabilities in large-scale equivariant graph learning. Code is available at https://github.com/GLAD-RUC/DistEGNN.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2506.14796.pdf' target='_blank'>https://arxiv.org/pdf/2506.14796.pdf</a></span>   <span><a href='https://github.com/biomap-research/PFMBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/biomap-research/PFMBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Gao, Hao Wang, Cheng Tan, Chenrui Xu, Mengdi Liu, Bozhen Hu, Linlin Chao, Xiaoming Zhang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14796">PFMBench: Protein Foundation Model Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the current landscape and future directions of protein foundation model research. While recent advancements have transformed protein science and engineering, the field lacks a comprehensive benchmark for fair evaluation and in-depth understanding. Since ESM-1B, numerous protein foundation models have emerged, each with unique datasets and methodologies. However, evaluations often focus on limited tasks tailored to specific models, hindering insights into broader generalization and limitations. Specifically, researchers struggle to understand the relationships between tasks, assess how well current models perform across them, and determine the criteria in developing new foundation models. To fill this gap, we present PFMBench, a comprehensive benchmark evaluating protein foundation models across 38 tasks spanning 8 key areas of protein science. Through hundreds of experiments on 17 state-of-the-art models across 38 tasks, PFMBench reveals the inherent correlations between tasks, identifies top-performing models, and provides a streamlined evaluation protocol. Code is available at \href{https://github.com/biomap-research/PFMBench}{\textcolor{blue}{GitHub}}.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2506.08316.pdf' target='_blank'>https://arxiv.org/pdf/2506.08316.pdf</a></span>   <span><a href='https://github.com/AlanNawzadAmin/SCUD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan N. Amin, Nate Gruver, Andrew Gordon Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08316">Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discrete diffusion models, like continuous diffusion models, generate high-quality samples by gradually undoing noise applied to datapoints with a Markov process. Gradual generation in theory comes with many conceptual benefits; for example, inductive biases can be incorporated into the noising Markov process, and access to improved sampling algorithms. In practice, however, the consistently best performing discrete diffusion model is, surprisingly, masking diffusion, which does not denoise gradually. Here we explain the superior performance of masking diffusion by noting that it makes use of a fundamental difference between continuous and discrete Markov processes: discrete Markov processes evolve by discontinuous jumps at a fixed rate and, unlike other discrete diffusion models, masking diffusion builds in the known distribution of jump times and only learns where to jump to. We show that we can similarly bake in the known distribution of jump times into any discrete diffusion model. The resulting models - schedule-conditioned discrete diffusion (SCUD) - generalize classical discrete diffusion and masking diffusion. By applying SCUD to models with noising processes that incorporate inductive biases on images, text, and protein data, we build models that outperform masking.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2506.07833.pdf' target='_blank'>https://arxiv.org/pdf/2506.07833.pdf</a></span>   <span><a href='https://github.com/michaelchen-lab/caft-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael K. Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07833">Improving Large Language Models with Concept-Aware Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2506.06701.pdf' target='_blank'>https://arxiv.org/pdf/2506.06701.pdf</a></span>   <span><a href='https://github.com/fudong03/BioIntelligence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fudong Lin, Wanrou Du, Jinchan Liu, Tarikul Milon, Shelby Meche, Wu Xu, Xiaoqi Qin, Xu Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06701">Do Protein Transformers Have Biological Intelligence?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks, particularly Transformers, have been widely adopted for predicting the functional properties of proteins. In this work, we focus on exploring whether Protein Transformers can capture biological intelligence among protein sequences. To achieve our goal, we first introduce a protein function dataset, namely Protein-FN, providing over 9000 protein data with meaningful labels. Second, we devise a new Transformer architecture, namely Sequence Protein Transformers (SPT), for computationally efficient protein function predictions. Third, we develop a novel Explainable Artificial Intelligence (XAI) technique called Sequence Score, which can efficiently interpret the decision-making processes of protein models, thereby overcoming the difficulty of deciphering biological intelligence bided in Protein Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only 5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3% on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all accomplished by training from scratch. Besides, our Sequence Score technique helps reveal that our SPT models can discover several meaningful patterns underlying the sequence structures of protein data, with these patterns aligning closely with the domain knowledge in the biology community. We have officially released our Protein-FN dataset on Hugging Face Datasets https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at https://github.com/fudong03/BioIntelligence.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2506.05427.pdf' target='_blank'>https://arxiv.org/pdf/2506.05427.pdf</a></span>   <span><a href='https://github.com/ZishanShu/MTPNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zishan Shu, Yufan Deng, Hongyu Zhang, Zhiwei Nie, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05427">MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.03373.pdf' target='_blank'>https://arxiv.org/pdf/2506.03373.pdf</a></span>   <span><a href='https://github.com/mahmoodlab/KRONOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Shaban, Yuzhou Chang, Huaying Qiu, Yao Yu Yeo, Andrew H. Song, Guillaume Jaume, Yuchen Wang, Luca L. Weishaupt, Tong Ding, Anurag Vaidya, Abdallah Lamane, Daniel Shao, Mohammed Zidane, Yunhao Bai, Paige McCallum, Shuli Luo, Wenrui Wu, Yang Wang, Precious Cramer, Chi Ngai Chan, Pierre Stephan, Johanna Schaffenrath, Jia Le Lee, Hendrik A. Michel, Caiwei Tian, Cristina Almagro-Perez, Sophia J. Wagner, Sharifa Sahai, Ming Y. Lu, Richard J. Chen, Andrew Zhang, Mark Edward M. Gonzales, Ahmad Makky, Jia-Ying Joey Lee, Hao Cheng, Nourhan El Ahmar, Sayed Matar, Maximilian Haist, Darci Phillips, Yuqi Tan, Garry P. Nolan, W. Richard Burack, Jacob D. Estes, Jonathan T. C. Liu, Toni K Choueiri, Neeraj Agarwal, Marc Barry, Scott J. Rodig, Long Phi Le, Georg Gerber, Christian M. SchÃ¼rch, Fabian J. Theis, Youn H Kim, Joe Yeong, Sabina Signoretti, Brooke E. Howitt, Lit-Hsin Loo, Qin Ma, Sizun Jiang, Faisal Mahmood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03373">A Foundation Model for Spatial Proteomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have begun to transform image analysis by acting as pretrained generalist backbones that can be adapted to many tasks even when post-training data are limited, yet their impact on spatial proteomics, imaging that maps proteins at single-cell resolution, remains limited. Here, we introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was trained in a self-supervised manner on over 47 million image patches covering 175 protein markers, 16 tissue types, and 8 fluorescence-based imaging platforms. We introduce key architectural adaptations to address the high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging. We demonstrate that KRONOS learns biologically meaningful representations across multiple scales, ranging from cellular and microenvironment to tissue levels, enabling it to address diverse downstream tasks, including cell phenotyping, region classification, and patient stratification. Evaluated across 11 independent cohorts, KRONOS achieves state-of-the-art performance across cell phenotyping, treatment response prediction, and retrieval tasks, and is highly data-efficient. KRONOS also introduces the paradigm of segmentation-free patch-level processing for efficient and scalable spatial proteomics analysis, allowing cross-institutional comparisons, and as an image reverse search engine for spatial patterns. Together, these results position KRONOS as a flexible and scalable tool for spatial proteomics. The model is publicly accessible at https://github.com/mahmoodlab/KRONOS.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.03237.pdf' target='_blank'>https://arxiv.org/pdf/2506.03237.pdf</a></span>   <span><a href='https://github.com/quanlin-wu/unisite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Quanlin Wu, Shengjie Luo, Liwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03237">UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2506.02203.pdf' target='_blank'>https://arxiv.org/pdf/2506.02203.pdf</a></span>   <span><a href='https://github.com/Stranja572/constrainedswe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid NaderiAlizadeh, Darian Salehi, Xinran Liu, Soheil Kolouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02203">Constrained Sliced Wasserstein Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sliced Wasserstein (SW) distances offer an efficient method for comparing high-dimensional probability measures by projecting them onto multiple 1-dimensional probability distributions. However, identifying informative slicing directions has proven challenging, often necessitating a large number of slices to achieve desirable performance and thereby increasing computational complexity. We introduce a constrained learning approach to optimize the slicing directions for SW distances. Specifically, we constrain the 1D transport plans to approximate the optimal plan in the original space, ensuring meaningful slicing directions. By leveraging continuous relaxations of these transport plans, we enable a gradient-based primal-dual approach to train the slicer parameters, alongside the remaining model parameters. We demonstrate how this constrained slicing approach can be applied to pool high-dimensional embeddings into fixed-length permutation-invariant representations. Numerical results on foundation models trained on images, point clouds, and protein sequences showcase the efficacy of the proposed constrained learning approach in learning more informative slicing directions. Our implementation code can be found at https://github.com/Stranja572/constrainedswe.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2506.02052.pdf' target='_blank'>https://arxiv.org/pdf/2506.02052.pdf</a></span>   <span><a href='https://github.com/Trust-App-AI-Lab/protap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Yan, Yuliang Yan, Bin Ma, Chenao Li, Haochun Tang, Jiahua Lu, Minhua Lin, Yuyuan Feng, Hui Xiong, Enyan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02052">Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. Additionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce $\textbf{Protap}$, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. Specifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. For each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. Our empirical studies imply that: (i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. (ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora. (iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. Code and datasets are publicly available at https://github.com/Trust-App-AI-Lab/protap.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2506.01918.pdf' target='_blank'>https://arxiv.org/pdf/2506.01918.pdf</a></span>   <span><a href='https://github.com/UNITES-Lab/Spatial2Sentence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Jane Chen, Yuhang Chen, Sukwon Yun, Natalie Stanley, Tianlong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01918">Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry's analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information: they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently: they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates single-cell expression and spatial information into natural language using a multi-sentence approach. Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations enable LLMs to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: https://github.com/UNITES-Lab/Spatial2Sentence.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2505.23862.pdf' target='_blank'>https://arxiv.org/pdf/2505.23862.pdf</a></span>   <span><a href='https://github.com/HudenJear/RPLoss' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Gong, Ziyi Jiang, Weihao Gao, Deng Zhuo, Lan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23862">A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at https://github.com/HudenJear/RPLoss.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2505.23839.pdf' target='_blank'>https://arxiv.org/pdf/2505.23839.pdf</a></span>   <span><a href='https://github.com/zaixizhang/GeneBreaker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaixi Zhang, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23839">GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA, encoding genetic instructions for almost all living organisms, fuels groundbreaking advances in genomics and synthetic biology. Recently, DNA Foundation Models have achieved success in designing synthetic functional DNA sequences, even whole genomes, but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes. In this paper, we introduce GeneBreaker, the first framework to systematically evaluate jailbreak vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent with customized bioinformatic tools to design high-homology, non-pathogenic jailbreaking prompts, (2) beam search guided by PathoLM and log-probability heuristics to steer generation toward pathogen-like sequences, and (3) a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench) to detect successful jailbreaks. Evaluated on our JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60\% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity risks. Our findings also reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms. Our code is at https://github.com/zaixizhang/GeneBreaker.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2505.22869.pdf' target='_blank'>https://arxiv.org/pdf/2505.22869.pdf</a></span>   <span><a href='https://github.com/yinjunbo/cfpgen' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yinjunbo/cfpgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junbo Yin, Chao Zha, Wenjia He, Chencheng Xu, Xin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22869">CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2505.22674.pdf' target='_blank'>https://arxiv.org/pdf/2505.22674.pdf</a></span>   <span><a href='https://github.com/BioinfoMachineLearning/PSBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pawan Neupane, Jian Liu, Jianlin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22674">PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein complex structures is essential for protein function analysis, protein design, and drug discovery. While AI methods like AlphaFold can predict accurate structural models for many protein complexes, reliably estimating the quality of these predicted models (estimation of model accuracy, or EMA) for model ranking and selection remains a major challenge. A key barrier to developing effective machine learning-based EMA methods is the lack of large, diverse, and well-annotated datasets for training and evaluation. To address this gap, we introduce PSBench, a benchmark suite comprising four large-scale, labeled datasets generated during the 15th and 16th community-wide Critical Assessment of Protein Structure Prediction (CASP15 and CASP16). PSBench includes over one million structural models covering a wide range of protein sequence lengths, complex stoichiometries, functional classes, and modeling difficulties. Each model is annotated with multiple complementary quality scores at the global, local, and interface levels. PSBench also provides multiple evaluation metrics and baseline EMA methods to facilitate rigorous comparisons. To demonstrate PSBench's utility, we trained and evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE was blindly tested in CASP16 (2024), where it ranked among the top-performing EMA methods. These results highlight PSBench as a valuable resource for advancing EMA research in protein complex modeling. PSBench is publicly available at: https://github.com/BioinfoMachineLearning/PSBench.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2505.22525.pdf' target='_blank'>https://arxiv.org/pdf/2505.22525.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/thinking-with-generated-images' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22525">Thinking with Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2505.20589.pdf' target='_blank'>https://arxiv.org/pdf/2505.20589.pdf</a></span>   <span><a href='https://github.com/mahdip72/prot2token' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Pourmirzaei, Farzaneh Esmaili, Salhuldin Alqarghuli, Mohammadreza Pourmirzaei, Ye Han, Kai Chen, Mohsen Rezaei, Duolin Wang, Dong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20589">Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.20354.pdf' target='_blank'>https://arxiv.org/pdf/2505.20354.pdf</a></span>   <span><a href='https://github.com/IDEA-XL/RAPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20354">Rethinking Text-based Protein Understanding: Retrieval or LLM?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.17866.pdf' target='_blank'>https://arxiv.org/pdf/2505.17866.pdf</a></span>   <span><a href='https://github.com/MetaEvo/DesignX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17866">DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective black-box optimizers is hampered by limited problem-specific knowledge and manual control that spans months for almost every detail. In this paper, we present DesignX, the first automated algorithm design framework that generates an effective optimizer specific to a given black-box optimization problem within seconds. Rooted in the first principles, we identify two key sub-tasks: 1) algorithm structure generation and 2) hyperparameter control. To enable systematic construction, a comprehensive modular algorithmic space is first built, embracing hundreds of algorithm components collected from decades of research. We then introduce a dual-agent reinforcement learning system that collaborates on structural and parametric design through a novel cooperative training objective, enabling large-scale meta-training across 10k diverse instances. Remarkably, through days of autonomous learning, the DesignX-generated optimizers continuously surpass human-crafted optimizers by orders of magnitude, either on synthetic testbed or on realistic optimization scenarios such as Protein-docking, AutoML and UAV path planning. Further in-depth analysis reveals DesignX's capability to discover non-trivial algorithm patterns beyond expert intuition, which, conversely, provides valuable design insights for the optimization community. We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.12044.pdf' target='_blank'>https://arxiv.org/pdf/2505.12044.pdf</a></span>   <span><a href='https://github.com/thuml/FlashBias' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haixu Wu, Minghao Guo, Yuezhou Ma, Yuanxu Sun, Jianmin Wang, Wojciech Matusik, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12044">FlashBias: Fast Computation of Attention with Bias</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attention with bias, which extends standard attention by introducing prior knowledge as an additive bias matrix to the query-key scores, has been widely deployed in vision, language, protein-folding and other advanced scientific models, underscoring its status as a key evolution of this foundational module. However, introducing bias terms creates a severe efficiency bottleneck in attention computation. It disrupts the tightly fused memory-compute pipeline that underlies the speed of accelerators like FlashAttention, thereby stripping away most of their performance gains and leaving biased attention computationally expensive. Surprisingly, despite its common usage, targeted efficiency optimization for attention with bias remains absent, which seriously hinders its application in complex tasks. Diving into the computation of FlashAttention, we prove that its optimal efficiency is determined by the rank of the attention weight matrix. Inspired by this theoretical result, this paper presents FlashBias based on the low-rank compressed sensing theory, which can provide fast-exact computation for many widely used attention biases and a fast-accurate approximation for biases in general formalizations. FlashBias can fully take advantage of the extremely optimized matrix multiplication operation in modern GPUs, achieving 1.5$\times$ speedup for Pairformer in AlphaFold 3, and over 2$\times$ speedup for attention with bias in vision and language models without loss of accuracy. Code is available at this repository: https://github.com/thuml/FlashBias.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.11812.pdf' target='_blank'>https://arxiv.org/pdf/2505.11812.pdf</a></span>   <span><a href='https://github.com/ai4protein/VenusX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tan, Wenrui Gou, Bozitao Zhong, Liang Hong, Huiqun Yu, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11812">VenusX: Unlocking Fine-Grained Functional Understanding of Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have driven significant progress in predicting protein function and interactions at the protein level. While these advancements have been invaluable for many biological applications such as enzyme engineering and function annotation, a more detailed perspective is essential for understanding protein functional mechanisms and evaluating the biological knowledge captured by models. To address this demand, we introduce VenusX, the first large-scale benchmark for fine-grained functional annotation and function-based protein pairing at the residue, fragment, and domain levels. VenusX comprises three major task categories across six types of annotations, including residue-level binary classification, fragment-level multi-class classification, and pairwise functional similarity scoring for identifying critical active sites, binding sites, conserved sites, motifs, domains, and epitopes. The benchmark features over 878,000 samples curated from major open-source databases such as InterPro, BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three sequence identity thresholds, our benchmark enables a comprehensive assessment of model performance on both in-distribution and out-of-distribution scenarios. For baseline evaluation, we assess a diverse set of popular and open-source models, including pre-trained protein language models, sequence-structure hybrids, structure-based methods, and alignment-based techniques. Their performance is reported across all benchmark datasets and evaluation settings using multiple metrics, offering a thorough comparison and a strong foundation for future research. Code and data are publicly available at https://github.com/ai4protein/VenusX.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.11580.pdf' target='_blank'>https://arxiv.org/pdf/2505.11580.pdf</a></span>   <span><a href='https://github.com/flagshippioneering/flash_ipa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Liu, Axel Elaldi, Nicholas T Franklin, Nathan Russell, Gurinder S Atwal, Yih-En A Ban, Olivia Viessmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11580">Flash Invariant Point Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2505.01700.pdf' target='_blank'>https://arxiv.org/pdf/2505.01700.pdf</a></span>   <span><a href='https://github.com/CataAI/PoseX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yize Jiang, Xinze Li, Yuanyuan Zhang, Jin Han, Youjun Xu, Ayush Pandit, Zaixi Zhang, Mengdi Wang, Mengyang Wang, Chong Liu, Guang Yang, Yejin Choi, Wu-Jun Li, Tianfan Fu, Fang Wu, Junhong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01700">PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing protein-ligand docking studies typically focus on the self-docking scenario, which is less practical in real applications. Moreover, some studies involve heavy frameworks requiring extensive training, posing challenges for convenient and efficient assessment of docking methods. To fill these gaps, we design PoseX, an open-source benchmark to evaluate both self-docking and cross-docking, enabling a practical and comprehensive assessment of algorithmic advances. Specifically, we curated a novel dataset comprising 718 entries for self-docking and 1,312 entries for cross-docking; second, we incorporated 23 docking methods in three methodological categories, including physics-based methods (e.g., SchrÃ¶dinger Glide), AI docking methods (e.g., DiffDock) and AI co-folding methods (e.g., AlphaFold3); third, we developed a relaxation method for post-processing to minimize conformational energy and refine binding poses; fourth, we built a leaderboard to rank submitted models in real-time. We derived some key insights and conclusions from extensive experiments: (1) AI approaches have consistently outperformed physics-based methods in overall docking success rate. (2) Most intra- and intermolecular clashes of AI approaches can be greatly alleviated with relaxation, which means combining AI modeling with physics-based post-processing could achieve excellent performance. (3) AI co-folding methods exhibit ligand chirality issues, except for Boltz-1x, which introduced physics-inspired potentials to fix hallucinations, suggesting modeling on stereochemistry improves the structural plausibility markedly. (4) Specifying binding pockets significantly promotes docking performance, indicating that pocket information can be leveraged adequately, particularly for AI co-folding methods, in future modeling efforts. The code, dataset, and leaderboard are released at https://github.com/CataAI/PoseX.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2504.13075.pdf' target='_blank'>https://arxiv.org/pdf/2504.13075.pdf</a></span>   <span><a href='https://github.com/bytedance/apm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhe Chen, Dongyu Xue, Xiangxin Zhou, Zaixiang Zheng, Xiangxiang Zeng, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13075">An All-Atom Generative Model for Designing Protein Complexes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins typically exist in complexes, interacting with other proteins or biomolecules to perform their specific biological roles. Research on single-chain protein modeling has been extensively and deeply explored, with advancements seen in models like the series of ESM and AlphaFold2. Despite these developments, the study and modeling of multi-chain proteins remain largely uncharted, though they are vital for understanding biological functions. Recognizing the importance of these interactions, we introduce APM (All-Atom Protein Generative Model), a model specifically designed for modeling multi-chain proteins. By integrating atom-level information and leveraging data on multi-chain proteins, APM is capable of precisely modeling inter-chain interactions and designing protein complexes with binding capabilities from scratch. It also performs folding and inverse-folding tasks for multi-chain proteins. Moreover, APM demonstrates versatility in downstream applications: it achieves enhanced performance through supervised fine-tuning (SFT) while also supporting zero-shot sampling in certain tasks, achieving state-of-the-art results. We released our code at https://github.com/bytedance/apm.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2504.11454.pdf' target='_blank'>https://arxiv.org/pdf/2504.11454.pdf</a></span>   <span><a href='https://bytedance.github.io/dplm/dplm-2.1/' target='_blank'>  GitHub</a></span> <span><a href='https://bytedance.github.io/dplm/dplm-2.1/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei Ye, Shujian Huang, Zaixiang Zheng, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11454">Elucidating the Design Space of Multimodal Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models. Project page and code: https://bytedance.github.io/dplm/dplm-2.1/.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2504.06925.pdf' target='_blank'>https://arxiv.org/pdf/2504.06925.pdf</a></span>   <span><a href='https://github.com/AI4Food/FoodNExtDB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Romero-Tapiador, Ruben Tolosana, Blanca Lacruz-Pleguezuelos, Laura Judith Marcos Zambrano, Guadalupe X. BazÃ¡n, Isabel Espinosa-Salinas, Julian Fierrez, Javier Ortega-Garcia, Enrique Carrillo de Santa Pau, Aythami Morales
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06925">Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic dietary assessment based on food images remains a challenge, requiring precise food detection, segmentation, and classification. Vision-Language Models (VLMs) offer new possibilities by integrating visual and textual reasoning. In this study, we evaluate six state-of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their capabilities in food recognition at different levels. For the experimental framework, we introduce the FoodNExTDB, a unique food image database that contains 9,263 expert-labeled images across 10 categories (e.g., "protein source"), 62 subcategories (e.g., "poultry"), and 9 cooking styles (e.g., "grilled"). In total, FoodNExTDB includes 50k nutritional labels generated by seven experts who manually annotated all images in the database. Also, we propose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts for the inter-annotator variability. Results show that closed-source models outperform open-source ones, achieving over 90% EWR in recognizing food products in images containing a single product. Despite their potential, current VLMs face challenges in fine-grained food recognition, particularly in distinguishing subtle differences in cooking styles and visually similar food items, which limits their reliability for automatic dietary assessment. The FoodNExTDB database is publicly available at https://github.com/AI4Food/FoodNExtDB.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2504.03278.pdf' target='_blank'>https://arxiv.org/pdf/2504.03278.pdf</a></span>   <span><a href='https://github.com/compbiomed-unito/JanusDDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guido Barducci, Ivan Rossi, Francesco CodicÃ¨, Cesare Rollo, Valeria Repetto, Corrado Pancotti, Virginia Iannibelli, Tiziana Sanavia, Piero Fariselli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03278">JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how residue variations affect protein stability is crucial for designing functional proteins and deciphering the molecular mechanisms underlying disease-related mutations. Recent advances in protein language models (PLMs) have revolutionized computational protein analysis, enabling, among other things, more accurate predictions of mutational effects. In this work, we introduce JanusDDG, a deep learning framework that leverages PLM-derived embeddings and a bidirectional cross-attention transformer architecture to predict $ÎÎG$ of single and multiple-residue mutations while simultaneously being constrained to respect fundamental thermodynamic properties, such as antisymmetry and transitivity. Unlike conventional self-attention, JanusDDG computes queries (Q) and values (V) as the difference between wild-type and mutant embeddings, while keys (K) alternate between the two. This cross-interleaved attention mechanism enables the model to capture mutation-induced perturbations while preserving essential contextual information. Experimental results show that JanusDDG achieves state-of-the-art performance in predicting $ÎÎG$ from sequence alone, matching or exceeding the accuracy of structure-based methods for both single and multiple mutations. Code Availability:https://github.com/compbiomed-unito/JanusDDG
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.02148.pdf' target='_blank'>https://arxiv.org/pdf/2504.02148.pdf</a></span>   <span><a href='https://github.com/FuhaiLiAiLab/OmniCellTOSG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zhang, Tim Xu, Dekang Cao, Shunning Liang, Lars Schimmelpfennig, Levi Kaster, Di Huang, Carlos Cruchaga, Guangfu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02148">OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs. These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins. Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions. Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key contributions. First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling. This approach calls for new joint models combining large language models and graph neural networks. Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch. This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine. The OmniCellTOSG dataset is continuously expanding and will be updated regularly. The dataset and code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2504.00748.pdf' target='_blank'>https://arxiv.org/pdf/2504.00748.pdf</a></span>   <span><a href='https://github.com/knowlab/IHC-LLMiner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunsoo Kim, Michal W. S. Ong, Daniel W. Rogalsky, Manuel Rodriguez-Justo, Honghan Wu, Adam P. Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00748">IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from PubMed abstracts using large language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immunohistochemistry (IHC) is essential in diagnostic pathology and biomedical research, offering critical insights into protein expression and tumour biology. This study presents an automated pipeline, IHC-LLMiner, for extracting IHC-tumour profiles from PubMed abstracts, leveraging advanced biomedical text mining. There are two subtasks: abstract classification (include/exclude as relevant) and IHC-tumour profile extraction on relevant included abstracts. The best-performing model, "Gemma-2 finetuned", achieved 91.5% accuracy and an F1 score of 91.4, outperforming GPT4-O by 9.5% accuracy with 5.9 times faster inference time. From an initial dataset of 107,759 abstracts identified for 50 immunohistochemical markers, the classification task identified 30,481 relevant abstracts (Include) using the Gemma-2 finetuned model. For IHC-tumour profile extraction, the Gemma-2 finetuned model achieved the best performance with 63.3% Correct outputs. Extracted IHC-tumour profiles (tumour types and markers) were normalised to Unified Medical Language System (UMLS) concepts to ensure consistency and facilitate IHC-tumour profile landscape analysis. The extracted IHC-tumour profiles demonstrated excellent concordance with available online summary data and provided considerable added value in terms of both missing IHC-tumour profiles and quantitative assessments. Our proposed LLM based pipeline provides a practical solution for large-scale IHC-tumour profile data mining, enhancing the accessibility and utility of such data for research and clinical applications as well as enabling the generation of quantitative and structured data to support cancer-specific knowledge base development. Models and training datasets are available at https://github.com/knowlab/IHC-LLMiner.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2503.23014.pdf' target='_blank'>https://arxiv.org/pdf/2503.23014.pdf</a></span>   <span><a href='https://github.com/blingbell/MSNGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beibei Wang, Boyue Cui, Shiqu Chen, Xuan Wang, Yadong Wang, Junyi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23014">MSNGO: multi-species protein function annotation based on 3D protein structure and network propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: In recent years, protein function prediction has broken through the bottleneck of sequence features, significantly improving prediction accuracy using high-precision protein structures predicted by AlphaFold2. While single-species protein function prediction methods have achieved remarkable success, multi-species protein function prediction methods are still in the stage of using PPI networks and sequence features. Providing effective cross-species label propagation for species with sparse protein annotations remains a challenging issue. To address this problem, we propose the MSNGO model, which integrates structural features and network propagation methods. Our validation shows that using structural features can significantly improve the accuracy of multi-species protein function prediction. Results: We employ graph representation learning techniques to extract amino acid representations from protein structure contact maps and train a structural model using a graph convolution pooling module to derive protein-level structural features. After incorporating the sequence features from ESM-2, we apply a network propagation algorithm to aggregate information and update node representations within a heterogeneous network. The results demonstrate that MSNGO outperforms previous multi-species protein function prediction methods that rely on sequence features and PPI networks. Availability: https://github.com/blingbell/MSNGO.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2503.21788.pdf' target='_blank'>https://arxiv.org/pdf/2503.21788.pdf</a></span>   <span><a href='https://github.com/PharMolix/OpenBioMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhen Luo, Jiashuo Wang, Siqi Fan, Zaiqing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21788">PharMolixFM: All-Atom Foundation Models for Molecular Modeling and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural biology relies on accurate three-dimensional biomolecular structures to advance our understanding of biological functions, disease mechanisms, and therapeutics. While recent advances in deep learning have enabled the development of all-atom foundation models for molecular modeling and generation, existing approaches face challenges in generalization due to the multi-modal nature of atomic data and the lack of comprehensive analysis of training and sampling strategies. To address these limitations, we propose PharMolixFM, a unified framework for constructing all-atom foundation models based on multi-modal generative techniques. Our framework includes three variants using state-of-the-art multi-modal generative models. By formulating molecular tasks as a generalized denoising process with task-specific priors, PharMolixFM achieves robust performance across various structural biology applications. Experimental results demonstrate that PharMolixFM-Diff achieves competitive prediction accuracy in protein-small-molecule docking (83.9% vs. 90.2% RMSD < 2Ã, given pocket) with significantly improved inference speed. Moreover, we explore the empirical inference scaling law by introducing more sampling repeats or steps. Our code and model are available at https://github.com/PharMolix/OpenBioMed.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2503.21450.pdf' target='_blank'>https://arxiv.org/pdf/2503.21450.pdf</a></span>   <span><a href='https://github.com/HPC-NEAU/PhysChemDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changjian Zhou, Yuexi Qiu, Tongtong Ling, Jiafeng Li, Shuanghe Liu, Xiangjing Wang, Jia Song, Wensheng Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21450">CMADiff: Cross-Modal Aligned Diffusion for Controllable Protein Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-assisted protein design has emerged as a critical tool for advancing biotechnology, as deep generative models have demonstrated their reliability in this domain. However, most existing models primarily utilize protein sequence or structural data for training, neglecting the physicochemical properties of proteins.Moreover, they are deficient to control the generation of proteins in intuitive conditions. To address these limitations,we propose CMADiff here, a novel framework that enables controllable protein generation by aligning the physicochemical properties of protein sequences with text-based descriptions through a latent diffusion process. Specifically, CMADiff employs a Conditional Variational Autoencoder (CVAE) to integrate physicochemical features as conditional input, forming a robust latent space that captures biological traits. In this latent space, we apply a conditional diffusion process, which is guided by BioAligner, a contrastive learning-based module that aligns text descriptions with protein features, enabling text-driven control over protein sequence generation. Validated by a series of evaluations including AlphaFold3, the experimental results indicate that CMADiff outperforms protein sequence generation benchmarks and holds strong potential for future applications. The implementation and code are available at https://github.com/HPC-NEAU/PhysChemDiff.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2503.20291.pdf' target='_blank'>https://arxiv.org/pdf/2503.20291.pdf</a></span>   <span><a href='https://github.com/chenwei-zhang/CryoSAMU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenwei Zhang, Khanh Dao Duc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20291">CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at intermediate resolution (4-8 Ã) is crucial in protein structure determination. Recent advances in deep learning have led to the development of automated approaches for enhancing experimental cryo-EM density maps. Yet, these methods are not optimized for intermediate-resolution maps and rely on map density features alone. To address this, we propose CryoSAMU, a novel method designed to enhance 3D cryo-EM density maps of protein structures using structure-aware multimodal U-Nets and trained on curated intermediate-resolution density maps. We comprehensively evaluate CryoSAMU across various metrics and demonstrate its competitive performance compared to state-of-the-art methods. Notably, CryoSAMU achieves significantly faster processing speed, showing promise for future practical applications. Our code is available at https://github.com/chenwei-zhang/CryoSAMU.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2503.15438.pdf' target='_blank'>https://arxiv.org/pdf/2503.15438.pdf</a></span>   <span><a href='https://github.com/tyang816/VenusFactory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tan, Chen Liu, Jingyuan Gao, Banghao Wu, Mingchen Li, Ruilin Wang, Lingrong Zhang, Huiqun Yu, Guisheng Fan, Liang Hong, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15438">VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural language processing (NLP) has significantly influenced scientific domains beyond human language, including protein engineering, where pre-trained protein language models (PLMs) have demonstrated remarkable success. However, interdisciplinary adoption remains limited due to challenges in data collection, task benchmarking, and application. This work presents VenusFactory, a versatile engine that integrates biological data retrieval, standardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory supports both computer science and biology communities with choices of both a command-line execution and a Gradio-based no-code interface, integrating $40+$ protein-related datasets and $40+$ popular PLMs. All implementations are open-sourced on https://github.com/tyang816/VenusFactory.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2503.13517.pdf' target='_blank'>https://arxiv.org/pdf/2503.13517.pdf</a></span>   <span><a href='https://github.com/google/curie' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, Subhashini Venugopalan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13517">CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2503.08764.pdf' target='_blank'>https://arxiv.org/pdf/2503.08764.pdf</a></span>   <span><a href='https://github.com/johnyang101/reticular-sae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Parsan, David J. Yang, John J. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08764">Towards Interpretable Protein Structure Prediction with Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models have revolutionized structure prediction, but their nonlinear nature obscures how sequence representations inform structure prediction. While sparse autoencoders (SAEs) offer a path to interpretability here by learning linear representations in high-dimensional space, their application has been limited to smaller protein language models unable to perform structure prediction. In this work, we make two key advances: (1) we scale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic interpretability of protein structure prediction for the first time, and (2) we adapt Matryoshka SAEs for protein language models, which learn hierarchically organized features by forcing nested groups of latents to reconstruct inputs independently. We demonstrate that our Matryoshka SAEs achieve comparable or better performance than standard architectures. Through comprehensive evaluations, we show that SAEs trained on ESM2-3B significantly outperform those trained on smaller models for both biological concept discovery and contact map prediction. Finally, we present an initial case study demonstrating how our approach enables targeted steering of ESMFold predictions, increasing structure solvent accessibility while fixing the input sequence. To facilitate further investigation by the broader community, we open-source our code, dataset, pretrained models https://github.com/johnyang101/reticular-sae , and visualizer https://sae.reticular.ai .
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2503.04650.pdf' target='_blank'>https://arxiv.org/pdf/2503.04650.pdf</a></span>   <span><a href='https://github.com/lijfrank-open/JmcPPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiang Li, Xiaoping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04650">Joint Masked Reconstruction and Contrastive Learning for Mining Interactions Between Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interaction (PPI) prediction is an instrumental means in elucidating the mechanisms underlying cellular operations, holding significant practical implications for the realms of pharmaceutical development and clinical treatment. Presently, the majority of research methods primarily concentrate on the analysis of amino acid sequences, while investigations predicated on protein structures remain in the nascent stages of exploration. Despite the emergence of several structure-based algorithms in recent years, these are still confronted with inherent challenges: (1) the extraction of intrinsic structural information of proteins typically necessitates the expenditure of substantial computational resources; (2) these models are overly reliant on seen protein data, struggling to effectively unearth interaction cues between unknown proteins. To further propel advancements in this domain, this paper introduces a novel PPI prediction method jointing masked reconstruction and contrastive learning, termed JmcPPI. This methodology dissects the PPI prediction task into two distinct phases: during the residue structure encoding phase, JmcPPI devises two feature reconstruction tasks and employs graph attention mechanism to capture structural information between residues; during the protein interaction inference phase, JmcPPI perturbs the original PPI graph and employs a multi-graph contrastive learning strategy to thoroughly mine extrinsic interaction information of novel proteins. Extensive experiments conducted on three widely utilized PPI datasets demonstrate that JmcPPI surpasses existing optimal baseline models across various data partition schemes. The associated code can be accessed via https://github.com/lijfrank-open/JmcPPI.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2503.00089.pdf' target='_blank'>https://arxiv.org/pdf/2503.00089.pdf</a></span>   <span><a href='https://github.com/KatarinaYuan/StructTokenBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Yuan, Zichen Wang, Marcus Collins, Huzefa Rangwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00089">Protein Structure Tokenization: Benchmarking and New Recipe</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a surge in the development of protein structural tokenization methods, which chunk protein 3D structures into discrete or continuous representations. Structure tokenization enables the direct application of powerful techniques like language modeling for protein structures, and large multimodal models to integrate structures with protein sequences and functional texts. Despite the progress, the capabilities and limitations of these methods remain poorly understood due to the lack of a unified evaluation framework. We first introduce StructTokenBench, a framework that comprehensively evaluates the quality and efficiency of structure tokenizers, focusing on fine-grained local substructures rather than global structures, as typical in existing benchmarks. Our evaluations reveal that no single model dominates all benchmarking perspectives. Observations of codebook under-utilization led us to develop AminoAseed, a simple yet effective strategy that enhances codebook gradient updates and optimally balances codebook size and dimension for improved tokenizer utilization and quality. Compared to the leading model ESM3, our method achieves an average of 6.31% performance improvement across 24 supervised tasks, with sensitivity and utilization rates increased by 12.83% and 124.03%, respectively. Source code and model weights are available at https://github.com/KatarinaYuan/StructTokenBench
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2502.17504.pdf' target='_blank'>https://arxiv.org/pdf/2502.17504.pdf</a></span>   <span><a href='https://github.com/Yijia-Xiao/Protein-LLM-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17504">Protein Large Language Models: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at https://github.com/Yijia-Xiao/Protein-LLM-Survey.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2502.16189.pdf' target='_blank'>https://arxiv.org/pdf/2502.16189.pdf</a></span>   <span><a href='https://github.com/SRastegari/MBGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayedmohammadreza Rastegari, Sina Tabakhi, Xianyuan Liu, Wei Sang, Haiping Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16189">Co-evolution-based Metal-binding Residue Prediction with Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computational structural biology, predicting metal-binding sites and their corresponding metal types is challenging due to the complexity of protein structures and interactions. Conventional sequence- and structure-based prediction approaches cannot capture the complex evolutionary relationships driving these interactions to facilitate understanding, while recent co-evolution-based approaches do not fully consider the entire structure of the co-evolved residue network. In this paper, we introduce MBGNN (Metal-Binding Graph Neural Network) that utilizes the entire co-evolved residue network and effectively captures the complex dependencies within protein structures via graph neural networks to enhance the prediction of co-evolved metal-binding residues and their associated metal types. Experimental results on a public dataset show that MBGNN outperforms existing co-evolution-based metal-binding prediction methods, and it is also competitive against recent sequence-based methods, showing the potential of integrating co-evolutionary insights with advanced machine learning to deepen our understanding of protein-metal interactions. The MBGNN code is publicly available at https://github.com/SRastegari/MBGNN.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2502.15610.pdf' target='_blank'>https://arxiv.org/pdf/2502.15610.pdf</a></span>   <span><a href='https://github.com/fondress/PDeepPP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiu Zhai, Zikun Wang, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Shengrui Xu, Jingwan Wang, Dan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15610">A general language model for peptide identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification of bioactive peptides (BPs) and protein post-translational modifications (PTMs) is essential for understanding protein function and advancing therapeutic discovery. However, most computational methods remain limited in their generalizability across diverse peptide functions. Here, we present PDeepPP, a unified deep learning framework that integrates pretrained protein language models with a hybrid transformer-convolutional architecture, enabling robust identification across diverse peptide classes and PTM sites. We curated comprehensive benchmark datasets and implemented strategies to address data imbalance, allowing PDeepPP to systematically extract both global and local sequence features. Through extensive analyses-including dimensionality reduction and comparison studies-PDeepPP demonstrates strong, interpretable peptide representations and achieves state-of-the-art performance in 25 of the 33 biological identification tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and phosphorylation site (0.9984) identification, with 99.5% specificity in glycosylation site prediction and substantial reduction in false negatives in antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP supports biomedical research and the discovery of novel therapeutic targets for disease treatment. All code, datasets, and pretrained models are publicly available via GitHub:https://github.com/fondress/PDeepPP and Hugging Face:https://huggingface.co/fondress/PDeppPP.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2502.14944.pdf' target='_blank'>https://arxiv.org/pdf/2502.14944.pdf</a></span>   <span><a href='https://github.com/masa-ue/ProDifEvo-Refinement' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/masa-ue/ProDifEvo-Refinement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masatoshi Uehara, Xingyu Su, Yulai Zhao, Xiner Li, Aviv Regev, Shuiwang Ji, Sergey Levine, Tommaso Biancalani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14944">Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To fully leverage the capabilities of diffusion models, we are often interested in optimizing downstream reward functions during inference. While numerous algorithms for reward-guided generation have been recently proposed due to their significance, current approaches predominantly focus on single-shot generation, transitioning from fully noised to denoised states. We propose a novel framework for inference-time reward optimization with diffusion models inspired by evolutionary algorithms. Our approach employs an iterative refinement process consisting of two steps in each iteration: noising and reward-guided denoising. This sequential refinement allows for the gradual correction of errors introduced during reward optimization. Besides, we provide a theoretical guarantee for our framework. Finally, we demonstrate its superior empirical performance in protein and cell-type-specific regulatory DNA design. The code is available at \href{https://github.com/masa-ue/ProDifEvo-Refinement}{https://github.com/masa-ue/ProDifEvo-Refinement}.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2502.14934.pdf' target='_blank'>https://arxiv.org/pdf/2502.14934.pdf</a></span>   <span><a href='https://github.com/tmlr-group/FABFlex' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhuo Zhang, Lijun Wu, Kaiyuan Gao, Jiangchao Yao, Tao Qin, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14934">Fast and Accurate Blind Flexible Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking that predicts the bound structures of small molecules (ligands) to their protein targets, plays a vital role in drug discovery. However, existing docking methods often face limitations: they either overlook crucial structural changes by assuming protein rigidity or suffer from low computational efficiency due to their reliance on generative models for structure sampling. To address these challenges, we propose FABFlex, a fast and accurate regression-based multi-task learning model designed for realistic blind flexible docking scenarios, where proteins exhibit flexibility and binding pocket sites are unknown (blind). Specifically, FABFlex's architecture comprises three specialized modules working in concert: (1) A pocket prediction module that identifies potential binding sites, addressing the challenges inherent in blind docking scenarios. (2) A ligand docking module that predicts the bound (holo) structures of ligands from their unbound (apo) states. (3) A pocket docking module that forecasts the holo structures of protein pockets from their apo conformations. Notably, FABFlex incorporates an iterative update mechanism that serves as a conduit between the ligand and pocket docking modules, enabling continuous structural refinements. This approach effectively integrates the three subtasks of blind flexible docking-pocket identification, ligand conformation prediction, and protein flexibility modeling-into a unified, coherent framework. Extensive experiments on public benchmark datasets demonstrate that FABFlex not only achieves superior effectiveness in predicting accurate binding modes but also exhibits a significant speed advantage (208 $\times$) compared to existing state-of-the-art methods. Our code is released at https://github.com/tmlr-group/FABFlex.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2502.14637.pdf' target='_blank'>https://arxiv.org/pdf/2502.14637.pdf</a></span>   <span><a href='https://github.com/AngxiaoYue/ReQFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Angxiao Yue, Zichong Wang, Hongteng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14637">ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves on-par performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 63x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2502.12049.pdf' target='_blank'>https://arxiv.org/pdf/2502.12049.pdf</a></span>   <span><a href='https://github.com/Shef-AIRE/StoicIML' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayang Zhang, Xianyuan Liu, Wei Wu, Sina Tabakhi, Wenrui Fan, Shuo Zhou, Kang Lan Tee, Tuck Seng Wong, Haiping Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12049">Classifying the Stoichiometry of Virus-like Particles with Interpretable Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virus-like particles (VLPs) are valuable for vaccine development due to their immune-triggering properties. Understanding their stoichiometry, the number of protein subunits to form a VLP, is critical for vaccine optimisation. However, current experimental methods to determine stoichiometry are time-consuming and require highly purified proteins. To efficiently classify stoichiometry classes in proteins, we curate a new dataset and propose an interpretable, data-driven pipeline leveraging linear machine learning models. We also explore the impact of feature encoding on model performance and interpretability, as well as methods to identify key protein sequence features influencing classification. The evaluation of our pipeline demonstrates that it can classify stoichiometry while revealing protein features that possibly influence VLP assembly. The data and code used in this work are publicly available at https://github.com/Shef-AIRE/StoicIML.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2502.07384.pdf' target='_blank'>https://arxiv.org/pdf/2502.07384.pdf</a></span>   <span><a href='https://github.com/ZhangJJ26/SAGEPhos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjie Zhang, Hanqun Cao, Zijun Gao, Xiaorui Wang, Chunbin Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07384">SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2502.07272.pdf' target='_blank'>https://arxiv.org/pdf/2502.07272.pdf</a></span>   <span><a href='https://github.com/GenerTeam/GENERator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wu, Qiuyi Li, Mingyang Li, Kun Fu, Fuli Feng, Jieping Ye, Hui Xiong, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07272">GENERator: A Long-Context Generative Genomic Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of enhancer sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions. Implementation details and supplementary resources are available at https://github.com/GenerTeam/GENERator.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2502.06999.pdf' target='_blank'>https://arxiv.org/pdf/2502.06999.pdf</a></span>   <span><a href='https://github.com/HyperPotatoNeo/Outsourced_Diffusion_Sampling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Venkatraman, Mohsin Hasan, Minsu Kim, Luca Scimeca, Marcin Sendera, Yoshua Bengio, Glen Berseth, Nikolay Malkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06999">Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous ('outsourced') Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_Î¸(\mathbf{z})$. In such a model (\eg, a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_Î¸(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_Î¸(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable. We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_Î¸(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). For many models and constraints, the posterior in noise space is smoother than in data space, making it more suitable for amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably with other inference methods. We demonstrate the proposed outsourced diffusion sampling in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2502.06379.pdf' target='_blank'>https://arxiv.org/pdf/2502.06379.pdf</a></span>   <span><a href='https://github.com/filipekstrm/ddsmc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip EkstrÃ¶m Kelvinius, Zheng Zhao, Fredrik Lindsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06379">Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on "decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic as well as protein and image data. Further, we demonstrate how the approach can be extended to discrete data.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2502.05107.pdf' target='_blank'>https://arxiv.org/pdf/2502.05107.pdf</a></span>   <span><a href='https://github.com/HXYfighter/3DMolFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuyuan Hu, Guoqing Liu, Can Chen, Yang Zhao, Hao Zhang, Xue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05107">3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug discovery, encompassing the tasks of protein-ligand docking and pocket-aware 3D drug design, represents a core challenge in drug discovery. However, no existing work can deal with both tasks to effectively leverage the duality between them, and current methods for each task are hindered by challenges in modeling 3D information and the limitations of available data. To address these issues, we propose 3DMolFormer, a unified dual-channel transformer-based framework applicable to both docking and 3D drug design tasks, which exploits their duality by utilizing docking functionalities within the drug design process. Specifically, we represent 3D pocket-ligand complexes using parallel sequences of discrete tokens and continuous numbers, and we design a corresponding dual-channel transformer model to handle this format, thereby overcoming the challenges of 3D information modeling. Additionally, we alleviate data limitations through large-scale pre-training on a mixed dataset, followed by supervised and reinforcement learning fine-tuning techniques respectively tailored for the two tasks. Experimental results demonstrate that 3DMolFormer outperforms previous approaches in both protein-ligand docking and pocket-aware 3D drug design, highlighting its promising application in structure-based drug discovery. The code is available at: https://github.com/HXYfighter/3DMolFormer .
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2501.18278.pdf' target='_blank'>https://arxiv.org/pdf/2501.18278.pdf</a></span>   <span><a href='https://github.com/amitaysicherman/ReactEmbed' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/amitaysicherman/ReactEmbed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitay Sicherman, Kira Radinsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18278">ReactEmbed: A Cross-Domain Framework for Protein-Molecule Representation Learning via Biochemical Reaction Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The challenge in computational biology and drug discovery lies in creating comprehensive representations of proteins and molecules that capture their intrinsic properties and interactions. Traditional methods often focus on unimodal data, such as protein sequences or molecular structures, limiting their ability to capture complex biochemical relationships. This work enhances these representations by integrating biochemical reactions encompassing interactions between molecules and proteins. By leveraging reaction data alongside pre-trained embeddings from state-of-the-art protein and molecule models, we develop ReactEmbed, a novel method that creates a unified embedding space through contrastive learning. We evaluate ReactEmbed across diverse tasks, including drug-target interaction, protein-protein interaction, protein property prediction, and molecular property prediction, consistently surpassing all current state-of-the-art models. Notably, we showcase ReactEmbed's practical utility through successful implementation in lipid nanoparticle-based drug delivery, enabling zero-shot prediction of blood-brain barrier permeability for protein-nanoparticle complexes. The code and comprehensive database of reaction pairs are available for open use at \href{https://github.com/amitaysicherman/ReactEmbed}{GitHub}.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2501.15631.pdf' target='_blank'>https://arxiv.org/pdf/2501.15631.pdf</a></span>   <span><a href='https://github.com/khodabandeh-ali/BoKDiff.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15631">BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) leverages the 3D structure of biomolecular targets to guide the creation of new therapeutic agents. Recent advances in generative models, including diffusion models and geometric deep learning, have demonstrated promise in optimizing ligand generation. However, the scarcity of high-quality protein-ligand complex data and the inherent challenges in aligning generated ligands with target proteins limit the effectiveness of these methods. We propose BoKDiff, a novel framework that enhances ligand generation by combining multi-objective optimization and Best-of-K alignment methodologies. Built upon the DecompDiff model, BoKDiff generates diverse candidates and ranks them using a weighted evaluation of molecular properties such as QED, SA, and docking scores. To address alignment challenges, we introduce a method that relocates the center of mass of generated ligands to their docking poses, enabling accurate sub-component extraction. Additionally, we integrate a Best-of-N (BoN) sampling approach, which selects the optimal ligand from multiple generated candidates without requiring fine-tuning. BoN achieves exceptional results, with QED values exceeding 0.6, SA scores above 0.75, and a success rate surpassing 35%, demonstrating its efficiency and practicality. BoKDiff achieves state-of-the-art results on the CrossDocked2020 dataset, including a -8.58 average Vina docking score and a 26% success rate in molecule generation. This study is the first to apply Best-of-K alignment and Best-of-N sampling to SBDD, highlighting their potential to bridge generative modeling with practical drug discovery requirements. The code is provided at https://github.com/khodabandeh-ali/BoKDiff.git.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2501.12706.pdf' target='_blank'>https://arxiv.org/pdf/2501.12706.pdf</a></span>   <span><a href='https://github.com/renero/causalgraph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesus Renero, Idoia Ochoa, Roberto Maestre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12706">REX: Causal Discovery based on Machine Learning and Explainability techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainability techniques hold significant potential for enhancing the causal discovery process, which is crucial for understanding complex systems in areas like healthcare, economics, and artificial intelligence. However, no causal discovery methods currently incorporate explainability into their models to derive causal graphs. Thus, in this paper we explore this innovative approach, as it offers substantial potential and represents a promising new direction worth investigating. Specifically, we introduce REX, a causal discovery method that leverages machine learning (ML) models coupled with explainability techniques, specifically Shapley values, to identify and interpret significant causal relationships among variables.
  Comparative evaluations on synthetic datasets comprising continuous tabular data reveal that REX outperforms state-of-the-art causal discovery methods across diverse data generation processes, including non-linear and additive noise models. Moreover, REX was tested on the Sachs single-cell protein-signaling dataset, achieving a precision of 0.952 and recovering key causal relationships with no incorrect edges. Taking together, these results showcase REX's effectiveness in accurately recovering true causal structures while minimizing false positive predictions, its robustness across diverse datasets, and its applicability to real-world problems. By combining ML and explainability techniques with causal discovery, REX bridges the gap between predictive modeling and causal inference, offering an effective tool for understanding complex causal structures. REX is publicly available at https://github.com/renero/causalgraph.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2501.09685.pdf' target='_blank'>https://arxiv.org/pdf/2501.09685.pdf</a></span>   <span><a href='https://github.com/masa-ue/AlignInversePro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09685">Inference-Time Alignment in Diffusion Models with Reward-Guided Generation: Tutorial and Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models. While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures). In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning. This tutorial explores the foundational aspects of such inference-time algorithms. We review these methods from a unified perspective, demonstrating that current techniques -- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance -- aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards. Within this framework, we present several novel algorithms not yet covered in the literature. Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models. The code of this tutorial on protein design is available at https://github.com/masa-ue/AlignInversePro
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2501.09064.pdf' target='_blank'>https://arxiv.org/pdf/2501.09064.pdf</a></span>   <span><a href='https://github.com/kantamasuki/RGDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanta Masuki, Yuto Ashida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09064">Generative diffusion model with inverse renormalization group flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models represent a class of generative models that produce data by denoising a sample corrupted by white noise. Despite the success of diffusion models in computer vision, audio synthesis, and point cloud generation, so far they overlook inherent multiscale structures in data and have a slow generation process due to many iteration steps. In physics, the renormalization group offers a fundamental framework for linking different scales and giving an accurate coarse-grained model. Here we introduce a renormalization group-based diffusion model that leverages multiscale nature of data distributions for realizing a high-quality data generation. In the spirit of renormalization group procedures, we define a flow equation that progressively erases data information from fine-scale details to coarse-grained structures. Through reversing the renormalization group flows, our model is able to generate high-quality samples in a coarse-to-fine manner. We validate the versatility of the model through applications to protein structure prediction and image generation. Our model consistently outperforms conventional diffusion models across standard evaluation metrics, enhancing sample quality and/or accelerating sampling speed by an order of magnitude. The proposed method alleviates the need for data-dependent tuning of hyperparameters in the generative diffusion models, showing promise for systematically increasing sample efficiency based on the concept of the renormalization group.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2501.06848.pdf' target='_blank'>https://arxiv.org/pdf/2501.06848.pdf</a></span>   <span><a href='https://github.com/zacharyhorvitz/Fk-Diffusion-Steering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06848">A General Framework for Inference-time Scaling and Steering of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we present Feynman-Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models - even with off-the-shelf rewards - can provide significant sample quality gains and controllability benefits. Code is available at https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2501.05644.pdf' target='_blank'>https://arxiv.org/pdf/2501.05644.pdf</a></span>   <span><a href='https://github.com/yangzhao1230/ProtDETR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Yang, Bing Su, Jiahao Chen, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05644">Interpretable Enzyme Function Prediction via Residue-Level Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting multiple functions labeled with Enzyme Commission (EC) numbers from the enzyme sequence is of great significance but remains a challenge due to its sparse multi-label classification nature, i.e., each enzyme is typically associated with only a few labels out of more than 6000 possible EC numbers. However, existing machine learning algorithms generally learn a fixed global representation for each enzyme to classify all functions, thereby they lack interpretability and the fine-grained information of some function-specific local residue fragments may be overwhelmed. Here we present an attention-based framework, namely ProtDETR (Protein Detection Transformer), by casting enzyme function prediction as a detection problem. It uses a set of learnable functional queries to adaptatively extract different local representations from the sequence of residue-level features for predicting different EC numbers. ProtDETR not only significantly outperforms existing deep learning-based enzyme function prediction methods, but also provides a new interpretable perspective on automatically detecting different local regions for identifying different functions through cross-attentions between queries and residue-level features. Code is available at https://github.com/yangzhao1230/ProtDETR.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2501.01458.pdf' target='_blank'>https://arxiv.org/pdf/2501.01458.pdf</a></span>   <span><a href='https://github.com/george-yuanji-wang/GAN-TAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>George Yuanji Wang, Srisharan Murugesan, Aditya Prince Rohatgi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01458">GAN-TAT: A Novel Framework Using Protein Interaction Networks in Druggable Gene Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying druggable genes is essential for developing effective pharmaceuticals. With the availability of extensive, high-quality data, computational methods have become a significant asset. Protein Interaction Network (PIN) is valuable but challenging to implement due to its high dimensionality and sparsity. Previous methods relied on indirect integration, leading to resolution loss. This study proposes GAN-TAT, a framework utilizing an advanced graph embedding technology, ImGAGN, to directly integrate PIN for druggable gene inference work. Tested on three Pharos datasets, GAN-TAT achieved the highest AUC-ROC score of 0.951 on Tclin. Further evaluation shows that GAN-TAT's predictions are supported by clinical evidence, highlighting its potential practical applications in pharmacogenomics. This research represents a methodological attempt with the direct utilization of PIN, expanding potential new solutions for developing drug targets. The source code of GAN-TAT is available at (https://github.com/george-yuanji-wang/GAN-TAT).
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2412.10966.pdf' target='_blank'>https://arxiv.org/pdf/2412.10966.pdf</a></span>   <span><a href='https://github.com/BioinfoMachineLearning/FlowDock' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/BioinfoMachineLearning/FlowDock' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Morehead, Jianlin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10966">FlowDock: Geometric Flow Matching for Generative Protein-Ligand Docking and Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powerful generative AI models of protein-ligand structure have recently been proposed, but few of these methods support both flexible protein-ligand docking and affinity estimation. Of those that do, none can directly model multiple binding ligands concurrently or have been rigorously benchmarked on pharmacologically relevant drug targets, hindering their widespread adoption in drug discovery efforts. In this work, we propose FlowDock, the first deep geometric generative model based on conditional flow matching that learns to directly map unbound (apo) structures to their bound (holo) counterparts for an arbitrary number of binding ligands. Furthermore, FlowDock provides predicted structural confidence scores and binding affinity values with each of its generated protein-ligand complex structures, enabling fast virtual screening of new (multi-ligand) drug targets. For the well-known PoseBusters Benchmark dataset, FlowDock outperforms single-sequence AlphaFold 3 with a 51% blind docking success rate using unbound (apo) protein input structures and without any information derived from multiple sequence alignments, and for the challenging new DockGen-E dataset, FlowDock outperforms single-sequence AlphaFold 3 and matches single-sequence Chai-1 for binding pocket generalization. Additionally, in the ligand category of the 16th community-wide Critical Assessment of Techniques for Structure Prediction (CASP16), FlowDock ranked among the top-5 methods for pharmacological binding affinity estimation across 140 protein-ligand complexes, demonstrating the efficacy of its learned representations in virtual screening. Source code, data, and pre-trained models are available at https://github.com/BioinfoMachineLearning/FlowDock.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2412.08649.pdf' target='_blank'>https://arxiv.org/pdf/2412.08649.pdf</a></span>   <span><a href='https://github.com/kansil/HOPER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>SerbÃ¼lent Ãnsal, Sinem Ãzdemir, BÃ¼nyamin Kasap, M. ErÅan KalaycÄ±, Kemal Turhan, Tunca DoÄan, Aybar C. Acar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08649">Multi-modal Representation Learning Enables Accurate Protein Function Prediction in Low-Data Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose HOPER (HOlistic ProtEin Representation), a novel multimodal learning framework designed to enhance protein function prediction (PFP) in low-data settings. The challenge of predicting protein functions is compounded by the limited availability of labeled data. Traditional machine learning models already struggle in such cases, and while deep learning models excel with abundant data, they also face difficulties when data is scarce. HOPER addresses this issue by integrating three distinct modalities - protein sequences, biomedical text, and protein-protein interaction (PPI) networks - to create a comprehensive protein representation. The model utilizes autoencoders to generate holistic embeddings, which are then employed for PFP tasks using transfer learning. HOPER outperforms existing methods on a benchmark dataset across all Gene Ontology categories, i.e., molecular function, biological process, and cellular component. Additionally, we demonstrate its practical utility by identifying new immune-escape proteins in lung adenocarcinoma, offering insights into potential therapeutic targets. Our results highlight the effectiveness of multimodal representation learning for overcoming data limitations in biological research, potentially enabling more accurate and scalable protein function prediction. HOPER source code and datasets are available at https://github.com/kansil/HOPER
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2412.05788.pdf' target='_blank'>https://arxiv.org/pdf/2412.05788.pdf</a></span>   <span><a href='https://github.com/matsagad/mres-project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>James Matthew Young, O. Deniz Akyildiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05788">On Diffusion Posterior Sampling via Sequential Monte Carlo for Zero-Shot Scaffolding of Protein Motifs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of diffusion models, new proteins can be generated at an unprecedented rate. The \textit{motif scaffolding problem} requires steering this generative process to yield proteins with a desirable functional substructure -- a motif. While models have been trained to take the motif as conditional input, recent techniques in diffusion posterior sampling can be leveraged as zero-shot alternatives whose approximations can be corrected with sequential Monte Carlo (SMC) algorithms. In this work, we introduce a new set of guidance potentials to describe and solve scaffolding tasks by adapting SMC-aided diffusion posterior samplers with an unconditional model, Genie, acting as a prior. Against established benchmarks, we successfully scaffold several single-motif and multi-motif problems. The latter is possible by pairing reconstruction guidance with $\mathrm{SE}(3)$-invariant potentials. In the single-motif case, we find these potentials perform comparably to the conventional masking approach and that reconstruction guidance outperforms replacement methods when aided with SMC. We additionally consider a guidance potential for point symmetry constraints and produce designable internally symmetric monomers with our setup. Overall, this work highlights the capabilities and areas for improvement of zero-shot posterior samplers in motif scaffolding tasks. Code is available at: https://github.com/matsagad/mres-project
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2412.05430.pdf' target='_blank'>https://arxiv.org/pdf/2412.05430.pdf</a></span>   <span><a href='https://github.com/kundajelab/DART-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aman Patel, Arpita Singhal, Austin Wang, Anusri Pampari, Maya Kasowski, Anshul Kundaje
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05430">DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in self-supervised models for natural language, vision, and protein sequences have inspired the development of large genomic DNA language models (DNALMs). These models aim to learn generalizable representations of diverse DNA elements, potentially enabling various genomic prediction, interpretation and design tasks. Despite their potential, existing benchmarks do not adequately assess the capabilities of DNALMs on key downstream applications involving an important class of non-coding DNA elements critical for regulating gene activity. In this study, we introduce DART-Eval, a suite of representative benchmarks specifically focused on regulatory DNA to evaluate model performance across zero-shot, probed, and fine-tuned scenarios against contemporary ab initio models as baselines. Our benchmarks target biologically meaningful downstream tasks such as functional sequence feature discovery, predicting cell-type specific regulatory activity, and counterfactual prediction of the impacts of genetic variants. We find that current DNALMs exhibit inconsistent performance and do not offer compelling gains over alternative baseline models for most tasks, while requiring significantly more computational resources. We discuss potentially promising modeling, data curation, and evaluation strategies for the next generation of DNALMs. Our code is available at https://github.com/kundajelab/DART-Eval.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2412.01108.pdf' target='_blank'>https://arxiv.org/pdf/2412.01108.pdf</a></span>   <span><a href='https://github.com/DeepGraphLearning/S3F' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuobai Zhang, Pascal Notin, Yining Huang, AurÃ©lie Lozano, Vijil Chenthamarakshan, Debora Marks, Payel Das, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01108">Multi-Scale Representation Learning for Protein Fitness Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing novel functional proteins crucially depends on accurately modeling their fitness landscape. Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets. While initial protein representation learning studies solely focused on either sequence or structural features, recent hybrid architectures have sought to merge these modalities to harness their respective strengths. However, these sequence-structure models have so far achieved only incremental improvements when compared to the leading sequence-only approaches, highlighting unresolved challenges effectively leveraging these modalities together. Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models. To address these limitations, we introduce the Sequence-Structure-Surface Fitness (S3F) model - a novel multimodal representation learning framework that integrates protein features across several scales. Our approach combines sequence representations from a protein language model with Geometric Vector Perceptron networks encoding protein backbone and detailed surface topology. The proposed method achieves state-of-the-art fitness prediction on the ProteinGym benchmark encompassing 217 substitution deep mutational scanning assays, and provides insights into the determinants of protein function. Our code is at https://github.com/DeepGraphLearning/S3F.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2411.18463.pdf' target='_blank'>https://arxiv.org/pdf/2411.18463.pdf</a></span>   <span><a href='https://github.com/Ced3-han/PepHAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahan Li, Tong Chen, Shitong Luo, Chaoran Cheng, Jiaqi Guan, Ruihan Guo, Sheng Wang, Ge Liu, Jian Peng, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18463">Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptides, short chains of amino acids, interact with target proteins, making them a unique class of protein-based therapeutics for treating human diseases. Recently, deep generative models have shown great promise in peptide generation. However, several challenges remain in designing effective peptide binders. First, not all residues contribute equally to peptide-target interactions. Second, the generated peptides must adopt valid geometries due to the constraints of peptide bonds. Third, realistic tasks for peptide drug development are still lacking. To address these challenges, we introduce PepHAR, a hot-spot-driven autoregressive generative model for designing peptides targeting specific proteins. Building on the observation that certain hot spot residues have higher interaction potentials, we first use an energy-based density model to fit and sample these key residues. Next, to ensure proper peptide geometry, we autoregressively extend peptide fragments by estimating dihedral angles between residue frames. Finally, we apply an optimization process to iteratively refine fragment assembly, ensuring correct peptide structures. By combining hot spot sampling with fragment-based extension, our approach enables de novo peptide design tailored to a target protein and allows the incorporation of key hot spot residues into peptide scaffolds. Extensive experiments, including peptide design and peptide scaffold generation, demonstrate the strong potential of PepHAR in computational peptide binder design. Source code will be available at https://github.com/Ced3-han/PepHAR.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2411.17196.pdf' target='_blank'>https://arxiv.org/pdf/2411.17196.pdf</a></span>   <span><a href='https://github.com/BLEACH366/P2DFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaowei Jin, Qi Huang, Ziyang Song, Mingyue Zheng, Dan Teng, Qian Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17196">P2DFlow: A Protein Ensemble Generative Model with SE(3) Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biological processes, functions, and properties are intricately linked to the ensemble of protein conformations, rather than being solely determined by a single stable conformation. In this study, we have developed P2DFlow, a generative model based on SE(3) flow matching, to predict the structural ensembles of proteins. We specifically designed a valuable prior for the flow process and enhanced the model's ability to distinguish each intermediate state by incorporating an additional dimension to describe the ensemble data, which can reflect the physical laws governing the distribution of ensembles, so that the prior knowledge can effectively guide the generation process. When trained and evaluated on the MD datasets of ATLAS, P2DFlow outperforms other baseline models on extensive experiments, successfully capturing the observable dynamic fluctuations as evidenced in crystal structure and MD simulations. As a potential proxy agent for protein molecular simulation, the high-quality ensembles generated by P2DFlow could significantly aid in understanding protein functions across various scenarios. Code is available at https://github.com/BLEACH366/P2DFlow
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2411.16694.pdf' target='_blank'>https://arxiv.org/pdf/2411.16694.pdf</a></span>   <span><a href='https://github.com/WillHua127/GENzyme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqing Hua, Jiarui Lu, Yong Liu, Odin Zhang, Jian Tang, Rex Ying, Wengong Jin, Guy Wolf, Doina Precup, Shuangjia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16694">Reaction-conditioned De Novo Enzyme Design with GENzyme</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of models like RFDiffusionAA, AlphaFold3, AlphaProteo, and Chai1 has revolutionized protein structure modeling and interaction prediction, primarily from a binding perspective, focusing on creating ideal lock-and-key models. However, these methods can fall short for enzyme-substrate interactions, where perfect binding models are rare, and induced fit states are more common. To address this, we shift to a functional perspective for enzyme design, where the enzyme function is defined by the reaction it catalyzes. Here, we introduce \textsc{GENzyme}, a \textit{de novo} enzyme design model that takes a catalytic reaction as input and generates the catalytic pocket, full enzyme structure, and enzyme-substrate binding complex. \textsc{GENzyme} is an end-to-end, three-staged model that integrates (1) a catalytic pocket generation and sequence co-design module, (2) a pocket inpainting and enzyme inverse folding module, and (3) a binding and screening module to optimize and predict enzyme-substrate complexes. The entire design process is driven by the catalytic reaction being targeted. This reaction-first approach allows for more accurate and biologically relevant enzyme design, potentially surpassing structure-based and binding-focused models in creating enzymes capable of catalyzing specific reactions. We provide \textsc{GENzyme} code at https://github.com/WillHua127/GENzyme.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2411.13280.pdf' target='_blank'>https://arxiv.org/pdf/2411.13280.pdf</a></span>   <span><a href='https://github.com/AlgoMole/MolCRAFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyue Qiu, Yuxuan Song, Jie Yu, Hongbo Ma, Ziyao Cao, Zhilong Zhang, Yushuai Wu, Mingyue Zheng, Hao Zhou, Wei-Ying Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13280">Empower Structure-Based Molecule Optimization with Gradient Guided Bayesian Flow Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-Based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets. A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities. To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance. We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization. MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x "Me-Better" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility. Code is available at https://github.com/AlgoMole/MolCRAFT.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2411.10618.pdf' target='_blank'>https://arxiv.org/pdf/2411.10618.pdf</a></span>   <span><a href='https://github.com/smiles724/PeptideDesign' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Wu, Tinson Xu, Shuting Jin, Xiangru Tang, Zerui Xu, James Zou, Brian Hie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10618">D-Flow: Multi-modality Flow Matching for D-peptide Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins play crucial roles in biological processes, with therapeutic peptides emerging as promising pharmaceutical agents. They allow new possibilities to leverage target binding sites that were previously undruggable. While deep learning (DL) has advanced peptide discovery, generating D-proteins composed of D-amino acids remains challenging due to the scarcity of natural examples. This paper proposes D-Flow, a full-atom flow-based framework for {de novo} D-peptide design. D-Flow is conditioned on receptor binding and utilizes a comprehensive representation of peptide structure, incorporating backbone frames, side-chain angles, and discrete amino acid types. A mirror-image algorithm is implemented to address the lack of training data for D-proteins, which converts L-receptors' chirality. Furthermore, we enhance D-Flow's capacity by integrating large protein language models (PLMs) with structural awareness through a lightweight structural adapter. A two-stage training pipeline and a controlling toolkit also enable D-Flow to transition from general protein design to targeted binder design while preserving pretraining knowledge.
  Extensive experimental results on the PepMerge benchmark demonstrate D-Flow's effectiveness, particularly in developing peptides with entire D-residues. This approach represents a significant advancement in computational D-peptide design, offering unique opportunities for bioorthogonal and stable molecular tools and diagnostics. The code is available in~\url{https://github.com/smiles724/PeptideDesign}.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2411.08909.pdf' target='_blank'>https://arxiv.org/pdf/2411.08909.pdf</a></span>   <span><a href='https://github.com/amazon-science/LC-PLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingheng Wang, Zichen Wang, Gil Sadeh, Luca Zancato, Alessandro Achille, George Karypis, Huzefa Rangwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08909">Long-context Protein Language Modeling Using Bidirectional Mamba with Shared Projection Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised training of language models (LMs) has seen great success for protein sequences in learning meaningful representations and for generative drug design. Most protein LMs are based on the Transformer architecture trained on individual proteins with short context lengths. Such protein LMs cannot extrapolate to longer proteins and protein complexes well. They also fail to account for the underlying biological mechanisms carried out by biomolecular interactions and dynamics i.e., proteins often interact with other proteins, molecules, and pathways in complex biological systems. In this work, we propose LC-PLM based on an alternative protein LM architecture, BiMamba-S, built upon selective structured state-space models, to learn high-quality universal protein representations at the amino acid token level using masked language modeling. We also introduce its graph-contextual variant, LC-PLM, which contextualizes protein-protein interaction (PPI) graphs for a second stage of training. LC-PLM demonstrates favorable neural scaling laws, better length extrapolation capability, and up to 30% and 16% improvements on protein downstream tasks compared to Transformer-based ESM-2 when trained with 100B and 1T tokens, respectively. LC-PLM-G further trained within the context of PPI graphs shows promising results on protein structure and function prediction tasks. Our study demonstrates the benefit of increasing the context size with computationally efficient LM architecture (e.g., structured state space models) in learning universal protein representations and incorporating molecular interaction contexts contained in biological graphs.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2411.05316.pdf' target='_blank'>https://arxiv.org/pdf/2411.05316.pdf</a></span>   <span><a href='https://github.com/Tizzzzy/LLM-GDM-alignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Shu, Bingbing Duan, Kai Guo, Kaixiong Zhou, Jiliang Tang, Mengnan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05316">Aligning Large Language Models and Geometric Deep Models for Protein Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent representation alignment has become a foundational technique for constructing multimodal large language models (MLLM) by mapping embeddings from different modalities into a shared space, often aligned with the embedding space of large language models (LLMs) to enable effective cross-modal understanding. While preliminary protein-focused MLLMs have emerged, they have predominantly relied on heuristic approaches, lacking a fundamental understanding of optimal alignment practices across representations. In this study, we explore the alignment of multimodal representations between LLMs and Geometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate three state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with four protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines alignment factors from both model and protein perspectives, identifying challenges in current alignment methodologies and proposing strategies to improve the alignment process. Our key findings reveal that GDMs incorporating both graph and 3D structural information align better with LLMs, larger LLMs demonstrate improved alignment capabilities, and protein rarity significantly impacts alignment performance. We also find that increasing GDM embedding dimensions, using two-layer projection heads, and fine-tuning LLMs on protein-specific data substantially enhance alignment quality. These strategies offer potential enhancements to the performance of protein-related multimodal models. Our code and data are available at https://github.com/Tizzzzy/LLM-GDM-alignment.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2411.02142.pdf' target='_blank'>https://arxiv.org/pdf/2411.02142.pdf</a></span>   <span><a href='https://github.com/cxysteven/ScalingProteinLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyi Cheng, Bo Chen, Pan Li, Jing Gong, Jie Tang, Le Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02142">Training Compute-Optimal Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing primarily on increasing model sizes rather than optimizing the efficient compute frontier that balances performance and compute budgets. Our investigation is grounded in a massive dataset consisting of 939 million protein sequences. We trained over 300 models ranging from 3.5 million to 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate the relations between model sizes, training token numbers, and objectives. First, we observed the effect of diminishing returns for the Causal Language Model (CLM) and that of overfitting for the Masked Language Model~(MLM) when repeating the commonly used Uniref database. To address this, we included metagenomic protein sequences in the training set to increase the diversity and avoid the plateau or overfitting effects. Second, we obtained the scaling laws of CLM and MLM on Transformer, tailored to the specific characteristics of protein sequence data. Third, we observe a transfer scaling phenomenon from CLM to MLM, further demonstrating the effectiveness of transfer through scaling behaviors based on estimated Effectively Transferred Tokens. Finally, to validate our scaling laws, we compare the large-scale versions of ESM-2 and PROGEN2 on downstream tasks, encompassing evaluations of protein generation as well as structure- and function-related tasks, all within less or equivalent pre-training compute budgets.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2411.02120.pdf' target='_blank'>https://arxiv.org/pdf/2411.02120.pdf</a></span>   <span><a href='https://github.com/violet-sto/Bridge-IF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Zhu, Jialu Wu, Qiuyi Li, Jiahuan Yan, Mingze Yin, Wei Wu, Mingyang Li, Jieping Ye, Zheng Wang, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02120">Bridge-IF: Learning Inverse Protein Folding with Markov Bridges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse protein folding is a fundamental task in computational protein design, which aims to design protein sequences that fold into the desired backbone structures. While the development of machine learning algorithms for this task has seen significant success, the prevailing approaches, which predominantly employ a discriminative formulation, frequently encounter the error accumulation issue and often fail to capture the extensive variety of plausible sequences. To fill these gaps, we propose Bridge-IF, a generative diffusion bridge model for inverse folding, which is designed to learn the probabilistic dependency between the distributions of backbone structures and protein sequences. Specifically, we harness an expressive structure encoder to propose a discrete, informative prior derived from structures, and establish a Markov bridge to connect this prior with native sequences. During the inference stage, Bridge-IF progressively refines the prior sequence, culminating in a more plausible design. Moreover, we introduce a reparameterization perspective on Markov bridge models, from which we derive a simplified loss function that facilitates more effective training. We also modulate protein language models (PLMs) with structural conditions to precisely approximate the Markov bridge process, thereby significantly enhancing generation performance while maintaining parameter-efficient training. Extensive experiments on well-established benchmarks demonstrate that Bridge-IF predominantly surpasses existing baselines in sequence recovery and excels in the design of plausible proteins with high foldability. The code is available at https://github.com/violet-sto/Bridge-IF.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2411.01856.pdf' target='_blank'>https://arxiv.org/pdf/2411.01856.pdf</a></span>   <span><a href='https://github.com/A4Bio/MeToken' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Lirong Wu, Siyuan Li, Yufei Huang, Jun Xia, Bozhen Hu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01856">MeToken: Uniform Micro-environment Token Boosts Post-Translational Modification Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Post-translational modifications (PTMs) profoundly expand the complexity and functionality of the proteome, regulating protein attributes and interactions that are crucial for biological processes. Accurately predicting PTM sites and their specific types is therefore essential for elucidating protein function and understanding disease mechanisms. Existing computational approaches predominantly focus on protein sequences to predict PTM sites, driven by the recognition of sequence-dependent motifs. However, these approaches often overlook protein structural contexts. In this work, we first compile a large-scale sequence-structure PTM dataset, which serves as the foundation for fair comparison. We introduce the MeToken model, which tokenizes the micro-environment of each amino acid, integrating both sequence and structural information into unified discrete tokens. This model not only captures the typical sequence motifs associated with PTMs but also leverages the spatial arrangements dictated by protein tertiary structures, thus providing a holistic view of the factors influencing PTM sites. Designed to address the long-tail distribution of PTM types, MeToken employs uniform sub-codebooks that ensure even the rarest PTMs are adequately represented and distinguished. We validate the effectiveness and generalizability of MeToken across multiple datasets, demonstrating its superior performance in accurately identifying PTM types. The results underscore the importance of incorporating structural data and highlight MeToken's potential in facilitating accurate and comprehensive PTM predictions, which could significantly impact proteomics research. The code and datasets are available at https://github.com/A4Bio/MeToken.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2410.22949.pdf' target='_blank'>https://arxiv.org/pdf/2410.22949.pdf</a></span>   <span><a href='https://github.com/PharMolix/MutaPLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhen Luo, Zikun Nie, Massimo Hong, Suyuan Zhao, Hao Zhou, Zaiqing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22949">MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein delta network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2410.21283.pdf' target='_blank'>https://arxiv.org/pdf/2410.21283.pdf</a></span>   <span><a href='https://github.com/jw-chae/pLDDT_Predictor,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joongwon Chae, Zhenyu Wang, Ijaz Gul, Jiansong Ji, Zhenglin Chen, Peiwu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21283">pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in protein structure prediction, particularly AlphaFold2, have revolutionized structural biology by achieving near-experimental accuracy ($\text{average RMSD} < 1.5\textÃ$). However, the computational demands of these models (approximately 30 minutes per protein on an RTX 4090) significantly limit their application in high-throughput protein screening. While large language models like ESM (Evolutionary Scale Modeling) have shown promise in extracting structural information directly from protein sequences, rapid assessment of protein structure quality for large-scale analyses remains a major challenge.
  We introduce pLDDT-Predictor, a high-speed protein screening tool that achieves a $250,000\times$ speedup compared to AlphaFold2 by leveraging pre-trained ESM2 protein embeddings and a Transformer architecture. Our model predicts AlphaFold2's pLDDT (predicted Local Distance Difference Test) scores with a Pearson correlation of 0.7891 and processes proteins in just 0.007 seconds on average. Using a comprehensive dataset of 1.5 million diverse protein sequences (ranging from 50 to 2048 amino acids), we demonstrate that pLDDT-Predictor accurately classifies high-confidence structures (pLDDT $>$ 70) with 91.2\% accuracy and achieves an MSE of 84.8142 compared to AlphaFold2's predictions.
  The source code and pre-trained models are freely available at https://github.com/jw-chae/pLDDT_Predictor, enabling the research community to perform rapid, large-scale protein structure quality assessments.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2410.21127.pdf' target='_blank'>https://arxiv.org/pdf/2410.21127.pdf</a></span>   <span><a href='https://github.com/tyang816/ProtREM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tan, Ruilin Wang, Banghao Wu, Liang Hong, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21127">Retrieval-Enhanced Mutation Mastery: Augmenting Zero-Shot Prediction of Protein Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzyme engineering enables the modification of wild-type proteins to meet industrial and research demands by enhancing catalytic activity, stability, binding affinities, and other properties. The emergence of deep learning methods for protein modeling has demonstrated superior results at lower costs compared to traditional approaches such as directed evolution and rational design. In mutation effect prediction, the key to pre-training deep learning models lies in accurately interpreting the complex relationships among protein sequence, structure, and function. This study introduces a retrieval-enhanced protein language model for comprehensive analysis of native properties from sequence and local structural interactions, as well as evolutionary properties from retrieved homologous sequences. The state-of-the-art performance of the proposed ProtREM is validated on over 2 million mutants across 217 assays from an open benchmark (ProteinGym). We also conducted post-hoc analyses of the model's ability to improve the stability and binding affinity of a VHH antibody. Additionally, we designed 10 new mutants on a DNA polymerase and conducted wet-lab experiments to evaluate their enhanced activity at higher temperatures. Both in silico and experimental evaluations confirmed that our method provides reliable predictions of mutation effects, offering an auxiliary tool for biologists aiming to evolve existing enzymes. The implementation is publicly available at https://github.com/tyang816/ProtREM.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2410.19222.pdf' target='_blank'>https://arxiv.org/pdf/2410.19222.pdf</a></span>   <span><a href='https://github.com/aayush-shah14/PeptideGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Shah, Chakradhar Guntuboina, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19222">Peptide-GPT: Generative Design of Peptides using Generative Pre-trained Transformers and Bio-informatic Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, natural language processing (NLP) models have demonstrated remarkable capabilities in various domains beyond traditional text generation. In this work, we introduce PeptideGPT, a protein language model tailored to generate protein sequences with distinct properties: hemolytic activity, solubility, and non-fouling characteristics. To facilitate a rigorous evaluation of these generated sequences, we established a comprehensive evaluation pipeline consisting of ideas from bioinformatics to retain valid proteins with ordered structures. First, we rank the generated sequences based on their perplexity scores, then we filter out those lying outside the permissible convex hull of proteins. Finally, we predict the structure using ESMFold and select the proteins with pLDDT values greater than 70 to ensure ordered structure. The properties of generated sequences are evaluated using task-specific classifiers - PeptideBERT and HAPPENN. We achieved an accuracy of 76.26% in hemolytic, 72.46% in non-hemolytic, 78.84% in non-fouling, and 68.06% in solubility protein generation. Our experimental results demonstrate the effectiveness of PeptideGPT in de novo protein design and underscore the potential of leveraging NLP-based approaches for paving the way for future innovations and breakthroughs in synthetic biology and bioinformatics. Codes, models, and data used in this study are freely available at: https://github.com/aayush-shah14/PeptideGPT.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2410.16474.pdf' target='_blank'>https://arxiv.org/pdf/2410.16474.pdf</a></span>   <span><a href='https://github.com/aqlaboratory/QuickBind' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojtek Treyde, Seohyun Chris Kim, Nazim Bouatta, Mohammed AlQuraishi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16474">QuickBind: A Light-Weight And Interpretable Molecular Docking Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting a ligand's bound pose to a target protein is a key component of early-stage computational drug discovery. Recent developments in machine learning methods have focused on improving pose quality at the cost of model runtime. For high-throughput virtual screening applications, this exposes a capability gap that can be filled by moderately accurate but fast pose prediction. To this end, we developed QuickBind, a light-weight pose prediction algorithm. We assess QuickBind on widely used benchmarks and find that it provides an attractive trade-off between model accuracy and runtime. To facilitate virtual screening applications, we augment QuickBind with a binding affinity module and demonstrate its capabilities for multiple clinically-relevant drug targets. Finally, we investigate the mechanistic basis by which QuickBind makes predictions and find that it has learned key physicochemical properties of molecular docking, providing new insights into how machine learning models generate protein-ligand poses. By virtue of its simplicity, QuickBind can serve as both an effective virtual screening tool and a minimal test bed for exploring new model architectures and innovations. Model code and weights are available at https://github.com/aqlaboratory/QuickBind .
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2410.15592.pdf' target='_blank'>https://arxiv.org/pdf/2410.15592.pdf</a></span>   <span><a href='https://github.com/GouWenrui/CPE-Pro-main.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Gou, Wenhui Ge, Yang Tan, Mingchen Li, Guisheng Fan, Huiqun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15592">CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein Representation and Origin Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structures are important for understanding their functions and interactions. Currently, many protein structure prediction methods are enriching the structure database. Discriminating the origin of structures is crucial for distinguishing between experimentally resolved and computationally predicted structures, evaluating the reliability of prediction methods, and guiding downstream biological studies. Building on works in structure prediction, We developed a structure-sensitive supervised deep learning model, Crystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent and discriminate the origin of protein structures. CPE-Pro learns the structural information of proteins and captures inter-structural differences to achieve accurate traceability on four data classes, and is expected to be extended to more. Simultaneously, we utilized Foldseek to encode protein structures into "structure-sequences" and trained a protein Structural Sequence Language Model, SSLM. Preliminary experiments demonstrated that, compared to large-scale protein language models pre-trained on vast amounts of amino acid sequences, the "structure-sequence" enables the language model to learn more informative protein features, enhancing and optimizing structural representations. We have provided the code, model weights, and all related materials on https://github.com/GouWenrui/CPE-Pro-main.git.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2410.14621.pdf' target='_blank'>https://arxiv.org/pdf/2410.14621.pdf</a></span>   <span><a href='https://github.com/prescient-design/jamun' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ameya Daigavane, Bodhi P. Vani, Darcy Davidson, Saeed Saremi, Joshua Rackers, Joseph Kleinhenz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14621">JAMUN: Bridging Smoothed Molecular Dynamics and Score-Based Learning for Conformational Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conformational ensembles of protein structures are immensely important both for understanding protein function and drug discovery in novel modalities such as cryptic pockets. Current techniques for sampling ensembles such as molecular dynamics (MD) are computationally inefficient, while many recent machine learning methods do not transfer to systems outside their training data. We propose JAMUN which performs MD in a smoothed, noised space of all-atom 3D conformations of molecules by utilizing the framework of walk-jump sampling. JAMUN enables ensemble generation for small peptides at rates of an order of magnitude faster than traditional molecular dynamics. The physical priors in JAMUN enables transferability to systems outside of its training data, even to peptides that are longer than those originally trained on. Our model, code and weights are available at https://github.com/prescient-design/jamun.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2410.10083.pdf' target='_blank'>https://arxiv.org/pdf/2410.10083.pdf</a></span>   <span><a href='https://github.com/iMoonLab/LLM4Hypergraph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10083">Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise relationships, offer a more robust framework but are still underexplored in the context of LLMs. To address this gap, we introduce LLM4Hypergraph, the first comprehensive benchmark comprising 21,500 problems across eight low-order, five high-order, and two isomorphism tasks, utilizing both synthetic and real-world hypergraphs from citation networks and protein structures. We evaluate six prominent LLMs, including GPT-4o, demonstrating our benchmark's effectiveness in identifying model strengths and weaknesses. Our specialized prompting framework incorporates seven hypergraph languages and introduces two novel techniques, Hyper-BAG and Hyper-COT, which enhance high-order reasoning and achieve an average 4% (up to 9%) performance improvement on structure classification tasks. This work establishes a foundational testbed for integrating hypergraph computational capabilities into LLMs, advancing their comprehension. The source codes are at https://github.com/iMoonLab/LLM4Hypergraph.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2410.08355.pdf' target='_blank'>https://arxiv.org/pdf/2410.08355.pdf</a></span>   <span><a href='https://github.com/instadeepai/metalic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Beck, Shikha Surana, Manus McAuliffe, Oliver Bent, Thomas D. Barrett, Juan Jose Garau Luis, Paul Duckworth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08355">Metalic: Meta-Learning In-Context with Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means that these models often have little, or no, specific data on the desired fitness prediction task. As a result of limited data, protein language models (PLMs) are typically trained on general protein sequence modeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness prediction. When no task data is available, the models make strong assumptions about the correlation between the protein sequence likelihood and fitness scores. In contrast, we propose meta-learning over a distribution of standard fitness prediction tasks, and demonstrate positive transfer to unseen fitness prediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses in-context learning and fine-tuning, when data is available, to adapt to new tasks. Crucially, fine-tuning enables considerable generalization, even though it is not accounted for during meta-training. Our fine-tuned models achieve strong results with 18 times fewer parameters than state-of-the-art models. Moreover, our method sets a new state-of-the-art in low-data settings on ProteinGym, an established fitness-prediction benchmark. Due to data scarcity, we believe meta-learning will play a pivotal role in advancing protein engineering.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2410.05670.pdf' target='_blank'>https://arxiv.org/pdf/2410.05670.pdf</a></span>   <span><a href='https://github.com/xihan-qin/Biologically-Supervised-Graph-Embedding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihan Qin, Li Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05670">Improving Disease Comorbidity Prediction Based on Human Interactome with Biologically Supervised Graph Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comorbidity carries significant implications for disease understanding and management. The genetic causes for comorbidity often trace back to mutations occurred either in the same gene associated with two diseases or in different genes associated with different diseases respectively but coming into connection via protein-protein interactions. Therefore, human interactome has been used in more sophisticated study of disease comorbidity. Human interactome, as a large incomplete graph, presents its own challenges to extracting useful features for comorbidity prediction. In this work, we introduce a novel approach named Biologically Supervised Graph Embedding (BSE) to allow for selecting most relevant features to enhance the prediction accuracy of comorbid disease pairs. Our investigation into BSE's impact on both centered and uncentered embedding methods showcases its consistent superiority over the state-of-the-art techniques and its adeptness in selecting dimensions enriched with vital biological insights, thereby improving prediction performance significantly, up to 50% when measured by ROC for some variations. Further analysis indicates that BSE consistently and substantially improves the ratio of disease associations to gene connectivity, affirming its potential in uncovering latent biological factors affecting comorbidity. The statistically significant enhancements across diverse metrics underscore BSE's potential to introduce novel avenues for precise disease comorbidity predictions and other potential applications. The GitHub repository containing the source code can be accessed at the following link: https://github.com/xihan-qin/Biologically-Supervised-Graph-Embedding.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2410.02647.pdf' target='_blank'>https://arxiv.org/pdf/2410.02647.pdf</a></span>   <span><a href='https://github.com/songleee/VenusVaccine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02647">Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immunogenicity prediction is a central topic in reverse vaccinology for finding candidate vaccines that can trigger protective immune responses. Existing approaches typically rely on highly compressed features and simple model architectures, leading to limited prediction accuracy and poor generalizability. To address these challenges, we introduce VenusVaccine, a novel deep learning solution with a dual attention mechanism that integrates pre-trained latent vector representations of protein sequences and structures. We also compile the most comprehensive immunogenicity dataset to date, encompassing over 7000 antigen sequences, structures, and immunogenicity labels from bacteria, virus, and tumor. Extensive experiments demonstrate that VenusVaccine outperforms existing methods across a wide range of evaluation metrics. Furthermore, we establish a post-hoc validation protocol to assess the practical significance of deep learning models in tackling vaccine design challenges. Our work provides an effective tool for vaccine design and sets valuable benchmarks for future research. The implementation is at https://github.com/songleee/VenusVaccine.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2410.02023.pdf' target='_blank'>https://arxiv.org/pdf/2410.02023.pdf</a></span>   <span><a href='https://github.com/jiaqingxie/DeepProtein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqing Xie, Tianfan Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02023">DeepProtein: Deep Learning Library and Benchmark for Protein Sequence Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has deeply influenced protein science, enabling breakthroughs in predicting protein properties, higher-order structures, and molecular interactions. This paper introduces DeepProtein, a comprehensive and user-friendly deep learning library tailored for protein-related tasks. It enables researchers to seamlessly address protein data with cutting-edge deep learning models. To assess model performance, we establish a benchmark evaluating different deep learning architectures across multiple protein-related tasks, including protein function prediction, subcellular localization prediction, protein-protein interaction prediction, and protein structure prediction. Furthermore, we introduce DeepProt-T5, a series of fine-tuned Prot-T5-based models that achieve state-of-the-art performance on four benchmark tasks, while demonstrating competitive results on six of others. Comprehensive documentation and tutorials are available which could ensure accessibility and support reproducibility. Built upon the widely used drug discovery library DeepPurpose, DeepProtein is publicly available at https://github.com/jiaqingxie/DeepProtein.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2410.00327.pdf' target='_blank'>https://arxiv.org/pdf/2410.00327.pdf</a></span>   <span><a href='https://github.com/WillHua127/EnzymeFlow/blob/main/enzymeflow_demo.ipynb' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/WillHua127/EnzymeFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqing Hua, Yong Liu, Dinghuai Zhang, Odin Zhang, Sitao Luan, Kevin K. Yang, Guy Wolf, Doina Precup, Shuangjia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00327">EnzymeFlow: Generating Reaction-specific Enzyme Catalytic Pockets through Flow Matching and Co-Evolutionary Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzyme design is a critical area in biotechnology, with applications ranging from drug development to synthetic biology. Traditional methods for enzyme function prediction or protein binding pocket design often fall short in capturing the dynamic and complex nature of enzyme-substrate interactions, particularly in catalytic processes. To address the challenges, we introduce EnzymeFlow, a generative model that employs flow matching with hierarchical pre-training and enzyme-reaction co-evolution to generate catalytic pockets for specific substrates and catalytic reactions. Additionally, we introduce a large-scale, curated, and validated dataset of enzyme-reaction pairs, specifically designed for the catalytic pocket generation task, comprising a total of $328,192$ pairs. By incorporating evolutionary dynamics and reaction-specific adaptations, EnzymeFlow becomes a powerful model for designing enzyme pockets, which is capable of catalyzing a wide range of biochemical reactions. Experiments on the new dataset demonstrate the model's effectiveness in designing high-quality, functional enzyme catalytic pockets, paving the way for advancements in enzyme engineering and synthetic biology. We provide EnzymeFlow code at https://github.com/WillHua127/EnzymeFlow with notebook demonstration at https://github.com/WillHua127/EnzymeFlow/blob/main/enzymeflow_demo.ipynb.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2409.20227.pdf' target='_blank'>https://arxiv.org/pdf/2409.20227.pdf</a></span>   <span><a href='https://github.com/Exscientia/plif_validity,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Errington, Constantin Schneider, CÃ©dric Bouysset, FrÃ©dÃ©ric A. Dreyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20227">Assessing interaction recovery of predicted protein-ligand poses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of protein-ligand pose prediction has seen significant advances in recent years, with machine learning-based methods now being commonly used in lieu of classical docking methods or even to predict all-atom protein-ligand complex structures. Most contemporary studies focus on the accuracy and physical plausibility of ligand placement to determine pose quality, often neglecting a direct assessment of the interactions observed with the protein. In this work, we demonstrate that ignoring protein-ligand interaction fingerprints can lead to overestimation of model performance, most notably in recent protein-ligand cofolding models which often fail to recapitulate key interactions.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2409.17808.pdf' target='_blank'>https://arxiv.org/pdf/2409.17808.pdf</a></span>   <span><a href='https://github.com/bjing2016/mdgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Jing, Hannes StÃ¤rk, Tommi Jaakkola, Bonnie Berger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17808">Generative Modeling of Molecular Dynamics Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2409.17265.pdf' target='_blank'>https://arxiv.org/pdf/2409.17265.pdf</a></span>   <span><a href='https://github.com/HannesStark/CodonMPNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannes Stark, Umesh Padia, Julia Balla, Cameron Diao, George Church
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17265">CodonMPNN for Organism Specific and Codon Optimal Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence. Code is available at https://github.com/HannesStark/CodonMPNN.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2408.13659.pdf' target='_blank'>https://arxiv.org/pdf/2408.13659.pdf</a></span>   <span><a href='https://github.com/WillHua127/ReactZyme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqing Hua, Bozitao Zhong, Sitao Luan, Liang Hong, Guy Wolf, Doina Precup, Shuangjia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13659">ReactZyme: A Benchmark for Enzyme-Reaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2408.12419.pdf' target='_blank'>https://arxiv.org/pdf/2408.12419.pdf</a></span>   <span><a href='https://fudan-generative-vision.github.io/AlphaFolding/#/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaihui Cheng, Ce Liu, Qingkun Su, Jun Wang, Liwei Zhang, Yining Tang, Yao Yao, Siyu Zhu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12419">AlphaFolding: 4D Diffusion for Dynamic Protein Structure Prediction with Reference and Motion Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes. URL: https://fudan-generative-vision.github.io/AlphaFolding/#/
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2408.12413.pdf' target='_blank'>https://arxiv.org/pdf/2408.12413.pdf</a></span>   <span><a href='https://fudan-generative-vision.github.io/dynamicPDB/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Liu, Jun Wang, Zhiqiang Cai, Yingxu Wang, Huizhen Kuang, Kaihui Cheng, Liwei Zhang, Qingkun Su, Yining Tang, Fenglei Cao, Limei Han, Siyu Zhu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12413">Dynamic PDB: A New Dataset and a SE(3) Model Extension by Integrating Dynamic Behaviors and Physical Properties in Protein Structures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in static protein structure collection and prediction, the dynamic behavior of proteins, one of their most vital characteristics, has been largely overlooked in prior research. This oversight can be attributed to the limited availability, diversity, and heterogeneity of dynamic protein datasets. To address this gap, we propose to enhance existing prestigious static 3D protein structural databases, such as the Protein Data Bank (PDB), by integrating dynamic data and additional physical properties. Specifically, we introduce a large-scale dataset, Dynamic PDB, encompassing approximately 12.6K proteins, each subjected to all-atom molecular dynamics (MD) simulations lasting 1 microsecond to capture conformational changes. Furthermore, we provide a comprehensive suite of physical properties, including atomic velocities and forces, potential and kinetic energies of proteins, and the temperature of the simulation environment, recorded at 1 picosecond intervals throughout the simulations. For benchmarking purposes, we evaluate state-of-the-art methods on the proposed dataset for the task of trajectory prediction. To demonstrate the value of integrating richer physical properties in the study of protein dynamics and related model design, we base our approach on the SE(3) diffusion model and incorporate these physical properties into the trajectory prediction process. Preliminary results indicate that this straightforward extension of the SE(3) model yields improved accuracy, as measured by MAE and RMSD, when the proposed physical properties are taken into consideration. https://fudan-generative-vision.github.io/dynamicPDB/ .
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2408.11363.pdf' target='_blank'>https://arxiv.org/pdf/2408.11363.pdf</a></span>   <span><a href='https://github.com/ProteinGPT/ProteinGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Xiao, Edward Sun, Yiqiao Jin, Qifan Wang, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11363">ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2408.08252.pdf' target='_blank'>https://arxiv.org/pdf/2408.08252.pdf</a></span>   <span><a href='https://github.com/masa-ue/SVDD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/masa-ue/SVDD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, Masatoshi Uehara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08252">Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (\textit{e.g.}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (\textit{e.g.}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at \href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2408.06050.pdf' target='_blank'>https://arxiv.org/pdf/2408.06050.pdf</a></span>   <span><a href='https://github.com/rafalkarczewski/SimpleSBDD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>RafaÅ Karczewski, Samuel Kaski, Markus Heinonen, Vikas Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06050">What Ails Generative Structure-based Drug Design: Expressivity is Too Little or Too Much?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several generative models with elaborate training and sampling procedures have been proposed to accelerate structure-based drug design (SBDD); however, their empirical performance turns out to be suboptimal. We seek to better understand this phenomenon from both theoretical and empirical perspectives. Since most of these models apply graph neural networks (GNNs), one may suspect that they inherit the representational limitations of GNNs. We analyze this aspect, establishing the first such results for protein-ligand complexes. A plausible counterview may attribute the underperformance of these models to their excessive parameterizations, inducing expressivity at the expense of generalization. We investigate this possibility with a simple metric-aware approach that learns an economical surrogate for affinity to infer an unlabelled molecular graph and optimizes for labels conditioned on this graph and molecular properties. The resulting model achieves state-of-the-art results using 100x fewer trainable parameters and affords up to 1000x speedup. Collectively, our findings underscore the need to reassess and redirect the existing paradigm and efforts for SBDD. Code is available at https://github.com/rafalkarczewski/SimpleSBDD.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2407.18184.pdf' target='_blank'>https://arxiv.org/pdf/2407.18184.pdf</a></span>   <span><a href='https://github.com/biochunan/AsEP-dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunan Liu, Lilian Denzler, Yihong Chen, Andrew Martin, Brooks Paige
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18184">AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2407.16375.pdf' target='_blank'>https://arxiv.org/pdf/2407.16375.pdf</a></span>   <span><a href='https://github.com/haddocking/DeepRank-GNN-esm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Xu, Alexandre M. J. J. Bonvin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16375">Ranking protein-protein models with large language models and graph neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) are associated with various diseases, including cancer, infections, and neurodegenerative disorders. Obtaining three-dimensional structural information on these PPIs serves as a foundation to interfere with those or to guide drug design. Various strategies can be followed to model those complexes, all typically resulting in a large number of models. A challenging step in this process is the identification of good models (near-native PPI conformations) from the large pool of generated models. To address this challenge, we previously developed DeepRank-GNN-esm, a graph-based deep learning algorithm for ranking modelled PPI structures harnessing the power of protein language models. Here, we detail the use of our software with examples. DeepRank-GNN-esm is freely available at https://github.com/haddocking/DeepRank-GNN-esm
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2407.13734.pdf' target='_blank'>https://arxiv.org/pdf/2407.13734.pdf</a></span>   <span><a href='https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13734">Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to explicitly maximize the measure of interest. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2407.07443.pdf' target='_blank'>https://arxiv.org/pdf/2407.07443.pdf</a></span>   <span><a href='https://github.com/riacd/CPDiffusion-SS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Hu, Yang Tan, Andi Han, Lirong Zheng, Liang Hong, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07443">Secondary Structure-Guided Novel Protein Sequence Generation with Latent Graph Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of deep learning has introduced efficient approaches for de novo protein sequence design, significantly improving success rates and reducing development costs compared to computational or experimental methods. However, existing methods face challenges in generating proteins with diverse lengths and shapes while maintaining key structural features. To address these challenges, we introduce CPDiffusion-SS, a latent graph diffusion model that generates protein sequences based on coarse-grained secondary structural information. CPDiffusion-SS offers greater flexibility in producing a variety of novel amino acid sequences while preserving overall structural constraints, thus enhancing the reliability and diversity of generated proteins. Experimental analyses demonstrate the significant superiority of the proposed method in producing diverse and novel sequences, with CPDiffusion-SS surpassing popular baseline methods on open benchmarks across various quantitative measurements. Furthermore, we provide a series of case studies to highlight the biological significance of the generation performance by the proposed method. The source code is publicly available at https://github.com/riacd/CPDiffusion-SS
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2407.01648.pdf' target='_blank'>https://arxiv.org/pdf/2407.01648.pdf</a></span>   <span><a href='https://github.com/MinkaiXu/AliDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Gu, Minkai Xu, Alexander Powers, Weili Nie, Tomas Geffner, Karsten Kreis, Jure Leskovec, Arash Vahdat, Stefano Ermon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01648">Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating ligand molecules for specific protein targets, known as structure-based drug design, is a fundamental problem in therapeutics development and biological discovery. Recently, target-aware generative models, especially diffusion models, have shown great promise in modeling protein-ligand interactions and generating candidate drugs. However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations. In this paper, we propose a novel and general alignment framework to align pretrained target diffusion models with preferred functional properties, named AliDiff. AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach. To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution. Empirical studies on the CrossDocked2020 benchmark show that AliDiff can generate molecules with state-of-the-art binding energies with up to -7.07 Avg. Vina Score, while maintaining strong molecular properties. Code is available at https://github.com/MinkaiXu/AliDiff.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2406.15669.pdf' target='_blank'>https://arxiv.org/pdf/2406.15669.pdf</a></span>   <span><a href='https://github.com/jsunn-y/CARE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Yang, Ariane Mora, Shengchao Liu, Bruce J. Wittmann, Anima Anandkumar, Frances H. Arnold, Yisong Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15669">CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2406.13839.pdf' target='_blank'>https://arxiv.org/pdf/2406.13839.pdf</a></span>   <span><a href='https://github.com/rish-16/rna-backbone-design' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Anand, Chaitanya K. Joshi, Alex Morehead, Arian R. Jamasb, Charles Harris, Simon V. Mathis, Kieran Didi, Rex Ying, Bryan Hooi, Pietro LiÃ²
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13839">RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RNA-FrameFlow, the first generative model for 3D RNA backbone design. We build upon SE(3) flow matching for protein backbone generation and establish protocols for data preparation and evaluation to address unique challenges posed by RNA modeling. We formulate RNA structures as a set of rigid-body frames and associated loss functions which account for larger, more conformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins (4 atoms per residue). Toward tackling the lack of diversity in 3D RNA datasets, we explore training with structural clustering and cropping augmentations. Additionally, we define a suite of evaluation metrics to measure whether the generated RNA structures are globally self-consistent (via inverse folding followed by forward folding) and locally recover RNA-specific structural descriptors. The most performant version of RNA-FrameFlow generates locally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass our validity criteria as measured by a self-consistency TM-score >= 0.45, at which two RNAs have the same global fold. Open-source code: https://github.com/rish-16/rna-backbone-design
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2406.10840.pdf' target='_blank'>https://arxiv.org/pdf/2406.10840.pdf</a></span>   <span><a href='https://github.com/Edapinenut/CBGBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Lin, Guojiang Zhao, Odin Zhang, Yufei Huang, Lirong Wu, Zicheng Liu, Siyuan Li, Cheng Tan, Zhifeng Gao, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10840">CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative heterogeneous graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements various cutting-edge methods. Secondly, a single task on \textit{de novo} molecule generation can hardly reflect their capabilities. To broaden the scope, we have adapted these models to a range of tasks essential in drug design, which are considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of \textit{de novo} molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide the pre-trained versions of the state-of-the-art models and deep insights with analysis from empirical studies. The codebase for CBGBench is publicly accessible at \url{https://github.com/Edapinenut/CBGBench}.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2406.08649.pdf' target='_blank'>https://arxiv.org/pdf/2406.08649.pdf</a></span>   <span><a href='https://github.com/carpenter-singh-lab/motive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>John Arevalo, Ellen Su, Anne E Carpenter, Shantanu Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08649">MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTIVE, a Morphological cOmpound Target Interaction Graph dataset comprising Cell Painting features for 11,000 genes and 3,600 compounds, along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTIVE accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2406.07025.pdf' target='_blank'>https://arxiv.org/pdf/2406.07025.pdf</a></span>   <span><a href='https://github.com/xuefeng-cs/ERP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07025">Entropy-Reinforced Planning with Large Language Models for Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLMs prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2406.06841.pdf' target='_blank'>https://arxiv.org/pdf/2406.06841.pdf</a></span>   <span><a href='https://github.com/BIMSBbioinfo/CompassDock' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Sarigun, Vedran Franke, Bora Uyar, Altuna Akalin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06841">CompassDock: Comprehensive Accurate Assessment Approach for Deep Learning-Based Molecular Docking in Inference and Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Datasets used for molecular docking, such as PDBBind, contain technical variability - they are noisy. Although the origins of the noise have been discussed, a comprehensive analysis of the physical, chemical, and bioactivity characteristics of the datasets is still lacking. To address this gap, we introduce the Comprehensive Accurate Assessment (Compass). Compass integrates two key components: PoseCheck, which examines ligand strain energy, protein-ligand steric clashes, and interactions, and AA-Score, a new empirical scoring function for calculating binding affinity energy. Together, these form a unified workflow that assesses both the physical/chemical properties and bioactivity favorability of ligands and protein-ligand interactions. Our analysis of the PDBBind dataset using Compass reveals substantial noise in the ground truth data. Additionally, we propose CompassDock, which incorporates the Compass module with DiffDock, the state-of-the-art deep learning-based molecular docking method, to enable accurate assessment of docked ligands during inference. Finally, we present a new paradigm for enhancing molecular docking model performance by fine-tuning with Compass Scores, which encompass binding affinity energy, strain energy, and the number of steric clashes identified by Compass. Our results show that, while fine-tuning without Compass improves the percentage of docked poses with RMSD < 2Ã, it leads to a decrease in physical/chemical and bioactivity favorability. In contrast, fine-tuning with Compass shows a limited improvement in RMSD < 2Ã but enhances the physical/chemical and bioactivity favorability of the ligand conformation. The source code is available publicly at https://github.com/BIMSBbioinfo/CompassDock.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2406.04739.pdf' target='_blank'>https://arxiv.org/pdf/2406.04739.pdf</a></span>   <span><a href='https://machinelearninglifescience.github.io/hdbo_benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miguel GonzÃ¡lez-Duque, Richard Michael, Simon Bartels, Yevgen Zainchkovskyy, SÃ¸ren Hauberg, Wouter Boomsma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04739">A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimizing discrete black-box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black-box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2406.03403.pdf' target='_blank'>https://arxiv.org/pdf/2406.03403.pdf</a></span>   <span><a href='https://github.com/zkysfls/2024-sbdd-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangyu Zheng, Yingzhou Lu, Zaixi Zhang, Zhongwei Wan, Yao Ma, Marinka Zitnik, Tianfan Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03403">Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of sixteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. The empirical results show that 1D/2D methods achieve competitive performance compared with 3D-based methods that use the 3D structure of the target protein explicitly. Also, AutoGrow4, a 2D molecular graph-based genetic algorithm, dominates SBDD in terms of optimization ability. The relevant code is available in https://github.com/zkysfls/2024-sbdd-benchmark.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2406.03141.pdf' target='_blank'>https://arxiv.org/pdf/2406.03141.pdf</a></span>   <span><a href='https://github.com/aim-uofa/FADiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Liu, Weian Mao, Shuaike Shen, Xiaoran Jiao, Zheng Sun, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03141">Floating Anchor Diffusion Model for Multi-motif Scaffolding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motif scaffolding seeks to design scaffold structures for constructing proteins with functions derived from the desired motif, which is crucial for the design of vaccines and enzymes. Previous works approach the problem by inpainting or conditional generation. Both of them can only scaffold motifs with fixed positions, and the conditional generation cannot guarantee the presence of motifs. However, prior knowledge of the relative motif positions in a protein is not readily available, and constructing a protein with multiple functions in one protein is more general and significant because of the synergies between functions. We propose a Floating Anchor Diffusion (FADiff) model. FADiff allows motifs to float rigidly and independently in the process of diffusion, which guarantees the presence of motifs and automates the motif position design. Our experiments demonstrate the efficacy of FADiff with high success rates and designable novel scaffolds. To the best of our knowledge, FADiff is the first work to tackle the challenge of scaffolding multiple motifs without relying on the expertise of relative motif positions in the protein. Code is available at https://github.com/aim-uofa/FADiff.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2405.16381.pdf' target='_blank'>https://arxiv.org/pdf/2405.16381.pdf</a></span>   <span><a href='https://github.com/yuchen-zhu-zyc/TDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Zhu, Tianrong Chen, Lingkai Kong, Evangelos A. Theodorou, Molei Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16381">Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generative modeling of data on manifolds is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations. This article demonstrates how a technique called `trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie groups. In particular, an auxiliary momentum variable was algorithmically introduced to help transport the position variable between data distribution and a fixed, easy-to-sample distribution. Normally, this would incur further difficulty for manifold data because momentum lives in a space that changes with the position. However, our trivialization technique creates a new momentum variable that stays in a simple fixed vector space. This design, together with a manifold preserving integrator, simplifies implementation and avoids inaccuracies created by approximations such as projections to tangent space and manifold, which were typically used in prior work, hence facilitating generation with high-fidelity and efficiency. The resulting method achieves state-of-the-art performance on protein and RNA torsion angle generation and sophisticated torus datasets. We also, arguably for the first time, tackle the generation of data on high-dimensional Special Orthogonal and Unitary groups, the latter essential for quantum problems. Code is available at https://github.com/yuchen-zhu-zyc/TDM.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2405.16206.pdf' target='_blank'>https://arxiv.org/pdf/2405.16206.pdf</a></span>   <span><a href='https://github.com/GlycanML/GlycanML' target='_blank'>  GitHub</a></span> <span><a href='https://GlycanML.github.io/project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Xu, Yunteng Geng, Yihang Zhang, Ling Yang, Jian Tang, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16206">GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glycans are basic biomolecules and perform essential functions within living organisms. The rapid increase of functional glycan data provides a good opportunity for machine learning solutions to glycan understanding. However, there still lacks a standard machine learning benchmark for glycan property and function prediction. In this work, we fill this blank by building a comprehensive benchmark for Glycan Machine Learning (GlycanML). The GlycanML benchmark consists of diverse types of tasks including glycan taxonomy prediction, glycan immunogenicity prediction, glycosylation type prediction, and protein-glycan interaction prediction. Glycans can be represented by both sequences and graphs in GlycanML, which enables us to extensively evaluate sequence-based models and graph neural networks (GNNs) on benchmark tasks. Furthermore, by concurrently performing eight glycan taxonomy prediction tasks, we introduce the GlycanML-MTL testbed for multi-task learning (MTL) algorithms. Also, we evaluate how taxonomy prediction can boost other three function prediction tasks by MTL. Experimental results show the superiority of modeling glycans with multi-relational GNNs, and suitable MTL methods can further boost model performance. We provide all datasets and source codes at https://github.com/GlycanML/GlycanML and maintain a leaderboard at https://GlycanML.github.io/project
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2405.15489.pdf' target='_blank'>https://arxiv.org/pdf/2405.15489.pdf</a></span>   <span><a href='https://github.com/aqlaboratory/genie2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeqing Lin, Minji Lee, Zhao Zhang, Mohammed AlQuraishi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15489">Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: https://github.com/aqlaboratory/genie2.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2405.15158.pdf' target='_blank'>https://arxiv.org/pdf/2405.15158.pdf</a></span>   <span><a href='https://github.com/AI-HPC-Research-Team/ProtFAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingqing Wang, Zhiwei Nie, Yonghong He, Athanasios V. Vasilakos, Zhixiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15158">ProtFAD: Introducing function-aware domains as implicit modality towards protein function prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein function prediction is currently achieved by encoding its sequence or structure, where the sequence-to-function transcendence and high-quality structural data scarcity lead to obvious performance bottlenecks. Protein domains are "building blocks" of proteins that are functionally independent, and their combinations determine the diverse biological functions. However, most existing studies have yet to thoroughly explore the intricate functional information contained in the protein domains. To fill this gap, we propose a synergistic integration approach for a function-aware domain representation, and a domain-joint contrastive learning strategy to distinguish different protein functions while aligning the modalities. Specifically, we align the domain semantics with GO terms and text description to pre-train domain embeddings. Furthermore, we partition proteins into multiple sub-views based on continuous joint domains for contrastive training under the supervision of a novel triplet InfoNCE loss. Our approach significantly and comprehensively outperforms the state-of-the-art methods on various benchmarks, and clearly differentiates proteins carrying distinct functions compared to the competitor. Our implementation is available at https://github.com/AI-HPC-Research-Team/ProtFAD.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2405.14108.pdf' target='_blank'>https://arxiv.org/pdf/2405.14108.pdf</a></span>   <span><a href='https://github.com/BioinfoMachineLearning/PoseBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/BioinfoMachineLearning/PoseBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Morehead, Nabin Giri, Jian Liu, Pawan Neupane, Jianlin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14108">Assessing the potential of deep learning for protein-ligand docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effects of ligand binding on protein structures and their in vivo functions carry numerous implications for modern biomedical research and biotechnology development efforts such as drug discovery. Although several deep learning (DL) methods and benchmarks designed for protein-ligand docking have recently been introduced, to date no prior works have systematically studied the behavior of the latest docking and structure prediction methods within the broadly applicable context of (1) using predicted (apo) protein structures for docking (e.g., for applicability to new proteins); (2) binding multiple (cofactor) ligands concurrently to a given target protein (e.g., for enzyme design); and (3) having no prior knowledge of binding pockets (e.g., for generalization to unknown pockets). To enable a deeper understanding of docking methods' real-world utility, we introduce PoseBench, the first comprehensive benchmark for broadly applicable protein-ligand docking. PoseBench enables researchers to rigorously and systematically evaluate DL methods for apo-to-holo protein-ligand docking and protein-ligand structure prediction using both primary ligand and multi-ligand benchmark datasets, the latter of which we introduce for the first time to the DL community. Empirically, using PoseBench, we find that (1) DL co-folding methods generally outperform comparable conventional and DL docking baseline algorithms, yet popular methods such as AlphaFold 3 are still challenged by prediction targets with novel binding poses; (2) certain DL co-folding methods are highly sensitive to their input multiple sequence alignments, while others are not; and (3) DL methods struggle to strike a balance between structural accuracy and chemical specificity when predicting novel or multi-ligand protein targets. Code, data, tutorials, and benchmark results are available at https://github.com/BioinfoMachineLearning/PoseBench.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2405.12564.pdf' target='_blank'>https://arxiv.org/pdf/2405.12564.pdf</a></span>   <span><a href='https://github.com/acharkq/ProtT3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12564">ProtT3: Protein-to-Text Generation for Text-based Protein Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM's representation space and the LM's input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2405.06649.pdf' target='_blank'>https://arxiv.org/pdf/2405.06649.pdf</a></span>   <span><a href='https://github.com/MingyuJ666/ProLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, Yongfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06649">ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. Previous machine learning approaches to PPI prediction mainly focus on direct physical interactions, ignoring the broader context of nonphysical connections through intermediate proteins, thus limiting their effectiveness. The emergence of Large Language Models (LLMs) provides a new opportunity for addressing this complex biological challenge. By transforming structured data into natural language prompts, we can map the relationships between proteins into texts. This approach allows LLMs to identify indirect connections between proteins, tracing the path from upstream to downstream. Therefore, we propose a novel framework ProLLM that employs an LLM tailored for PPI for the first time. Specifically, we propose Protein Chain of Thought (ProCoT), which replicates the biological mechanism of signaling pathways as natural language prompts. ProCoT considers a signaling pathway as a protein reasoning process, which starts from upstream proteins and passes through several intermediate proteins to transmit biological signals to downstream proteins. Thus, we can use ProCoT to predict the interaction between upstream proteins and downstream proteins. The training of ProLLM employs the ProCoT format, which enhances the model's understanding of complex biological problems. In addition to ProCoT, this paper also contributes to the exploration of embedding replacement of protein sites in natural language prompts, and instruction fine-tuning in protein knowledge datasets. We demonstrate the efficacy of ProLLM through rigorous validation against benchmark datasets, showing significant improvement over existing methods in terms of prediction accuracy and generalizability. The code is available at: https://github.com/MingyuJ666/ProLLM.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2405.06645.pdf' target='_blank'>https://arxiv.org/pdf/2405.06645.pdf</a></span>   <span><a href='https://github.com/amirgroup-codes/InteractionRecovery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Darin Tsui, Amirali Aghazadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06645">On Recovering Higher-order Interactions from Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models leverage evolutionary information to perform state-of-the-art 3D structure and zero-shot variant prediction. Yet, extracting and explaining all the mutational interactions that govern model predictions remains difficult as it requires querying the entire amino acid space for $n$ sites using $20^n$ sequences, which is computationally expensive even for moderate values of $n$ (e.g., $n\sim10$). Although approaches to lower the sample complexity exist, they often limit the interpretability of the model to just single and pairwise interactions. Recently, computationally scalable algorithms relying on the assumption of sparsity in the Fourier domain have emerged to learn interactions from experimental data. However, extracting interactions from language models poses unique challenges: it's unclear if sparsity is always present or if it is the only metric needed to assess the utility of Fourier algorithms. Herein, we develop a framework to do a systematic Fourier analysis of the protein language model ESM2 applied on three proteins-green fluorescent protein (GFP), tumor protein P53 (TP53), and G domain B1 (GB1)-across various sites for 228 experiments. We demonstrate that ESM2 is dominated by three regions in the sparsity-ruggedness plane, two of which are better suited for sparse Fourier transforms. Validations on two sample proteins demonstrate recovery of all interactions with $R^2=0.72$ in the more sparse region and $R^2=0.66$ in the more dense region, using only 7 million out of $20^{10}\sim10^{13}$ ESM2 samples, reducing the computational time by a staggering factor of 15,000. All codes and data are available on our GitHub repository https://github.com/amirgroup-codes/InteractionRecovery.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2405.03961.pdf' target='_blank'>https://arxiv.org/pdf/2405.03961.pdf</a></span>   <span><a href='https://github.com/genentech/voxbind/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro O. Pinheiro, Arian Jamasb, Omar Mahmood, Vishnu Sresht, Saeed Saremi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03961">Structure-based drug design by denoising voxel grids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VoxBind, a new score-based generative model for 3D molecules conditioned on protein structures. Our approach represents molecules as 3D atomic density grids and leverages a 3D voxel-denoising network for learning and generation. We extend the neural empirical Bayes formalism (Saremi & Hyvarinen, 2019) to the conditional setting and generate structure-conditioned molecules with a two-step procedure: (i) sample noisy molecules from the Gaussian-smoothed conditional distribution with underdamped Langevin MCMC using the learned score function and (ii) estimate clean molecules from the noisy samples with single-step denoising. Compared to the current state of the art, our model is simpler to train, significantly faster to sample from, and achieves better results on extensive in silico benchmarks -- the generated molecules are more diverse, exhibit fewer steric clashes, and bind with higher affinity to protein pockets. The code is available at https://github.com/genentech/voxbind/.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2404.14858.pdf' target='_blank'>https://arxiv.org/pdf/2404.14858.pdf</a></span>   <span><a href='https://github.com/Advanced-Research-Centre/mRNA-CodonOpt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongfeng Zhang, Aritra Sarkar, Koen Bertels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14858">A resource-efficient variational quantum algorithm for mRNA codon optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimizing the mRNA codon has an essential impact on gene expression for a specific target protein. It is an NP-hard problem; thus, exact solutions to such optimization problems become computationally intractable for realistic problem sizes on both classical and quantum computers. However, approximate solutions via heuristics can substantially impact the application they enable. Quantum approximate optimization is an alternative computation paradigm promising for tackling such problems. Recently, there has been some research in quantum algorithms for bioinformatics, specifically for mRNA codon optimization. This research presents a denser way to encode codons for implementing mRNA codon optimization via the variational quantum eigensolver algorithms on a gate-based quantum computer. This reduces the qubit requirement by half compared to the existing quantum approach, thus allowing longer sequences to be executed on existing quantum processors. The performance of the proposed algorithm is evaluated by comparing its results to exact solutions, showing well-matching results.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2404.14850.pdf' target='_blank'>https://arxiv.org/pdf/2404.14850.pdf</a></span>   <span><a href='https://github.com/tyang816/SES-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tan, Mingchen Li, Bingxin Zhou, Bozitao Zhong, Lirong Zheng, Pan Tan, Ziyi Zhou, Huiqun Yu, Guisheng Fan, Liang Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14850">Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning Pre-trained protein language models (PLMs) has emerged as a prominent strategy for enhancing downstream prediction tasks, often outperforming traditional supervised learning approaches. As a widely applied powerful technique in natural language processing, employing Parameter-Efficient Fine-Tuning techniques could potentially enhance the performance of PLMs. However, the direct transfer to life science tasks is non-trivial due to the different training strategies and data forms. To address this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter method for enhancing the representation learning of PLMs. SES-Adapter incorporates PLM embeddings with structural sequence embeddings to create structure-aware representations. We show that the proposed method is compatible with different PLM architectures and across diverse tasks. Extensive evaluations are conducted on 2 types of folding structures with notable quality differences, 9 state-of-the-art baselines, and 9 benchmark datasets across distinct downstream tasks. Results show that compared to vanilla PLMs, SES-Adapter improves downstream task performance by a maximum of 11% and an average of 3%, with significantly accelerated training speed by a maximum of 1034% and an average of 362%, the convergence rate is also improved by approximately 2 times. Moreover, positive optimization is observed even with low-quality predicted structures. The source code for SES-Adapter is available at https://github.com/tyang816/SES-Adapter.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2404.09738.pdf' target='_blank'>https://arxiv.org/pdf/2404.09738.pdf</a></span>   <span><a href='https://github.com/Kewei2023/AMPCliff-generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kewei Li, Yuqian Wu, Yinheng Li, Yutong Guo, Yan Wang, Yiyang Liang, Yusi Fan, Lan Huang, Ruochi Zhang, Fengfeng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09738">AMPCliff: quantitative definition and benchmarking of activity cliffs in antimicrobial peptides</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the mechanism of action of drug molecules in the human body is difficult to reproduce in the in vitro environment, it becomes difficult to reveal the causes of the activity cliff phenomenon of drug molecules. We found out the AC of small molecules has been extensively investigated but limited knowledge is accumulated about the AC phenomenon in peptides with canonical amino acids. Understanding the mechanism of AC in canonical amino acids might help understand the one in drug molecules. This study introduces a quantitative definition and benchmarking framework AMPCliff for the AC phenomenon in antimicrobial peptides (AMPs) composed by canonical amino acids. A comprehensive analysis of the existing AMP dataset reveals a significant prevalence of AC within AMPs. AMPCliff quantifies the activities of AMPs by the MIC, and defines 0.9 as the minimum threshold for the normalized BLOSUM62 similarity score between a pair of aligned peptides with at least two-fold MIC changes. This study establishes a benchmark dataset of paired AMPs in Staphylococcus aureus from the publicly available AMP dataset GRAMPA, and conducts a rigorous procedure to evaluate various AMP AC prediction models, including nine machine learning, four deep learning algorithms, four masked language models, and four generative language models. Our analysis reveals that these models are capable of detecting AMP AC events and the pre-trained protein language model ESM2 demonstrates superior performance across the evaluations. The predictive performance of AMP activity cliffs remains to be further improved, considering that ESM2 with 33 layers only achieves the Spearman correlation coefficient 0.4669 for the regression task of the MIC values on the benchmark dataset. Source code and additional resources are available at https://www.healthinformaticslab.org/supp/ or https://github.com/Kewei2023/AMPCliff-generation.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2404.04299.pdf' target='_blank'>https://arxiv.org/pdf/2404.04299.pdf</a></span>   <span><a href='https://github.com/anath2110/GENEVIC.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anindita Nath, Savannah Mwesigwa, Yulin Dai, Xiaoqian Jiang, Zhongming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04299">GENEVIC: GENetic data Exploration and Visualization via Intelligent interactive Console</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Summary: The vast generation of genetic data poses a significant challenge in efficiently uncovering valuable knowledge. Introducing GENEVIC, an AI-driven chat framework that tackles this challenge by bridging the gap between genetic data generation and biomedical knowledge discovery. Leveraging generative AI, notably ChatGPT, it serves as a biologist's 'copilot'. It automates the analysis, retrieval, and visualization of customized domain-specific genetic information, and integrates functionalities to generate protein interaction networks, enrich gene sets, and search scientific literature from PubMed, Google Scholar, and arXiv, making it a comprehensive tool for biomedical research. In its pilot phase, GENEVIC is assessed using a curated database that ranks genetic variants associated with Alzheimer's disease, schizophrenia, and cognition, based on their effect weights from the Polygenic Score Catalog, thus enabling researchers to prioritize genetic variants in complex diseases. GENEVIC's operation is user-friendly, accessible without any specialized training, secured by Azure OpenAI's HIPAA-compliant infrastructure, and evaluated for its efficacy through real-time query testing. As a prototype, GENEVIC is set to advance genetic research, enabling informed biomedical decisions.
  Availability and implementation: GENEVIC is publicly accessible at https://genevic-anath2024.streamlit.app. The underlying code is open-source and available via GitHub at https://github.com/anath2110/GENEVIC.git.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2403.14736.pdf' target='_blank'>https://arxiv.org/pdf/2403.14736.pdf</a></span>   <span><a href='https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14736">NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation methods and the co-embedding residual learning framework can improve the performance of GIN (Xu et al., 2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41% and 11.33% respectively. Our code is available at https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2403.12995.pdf' target='_blank'>https://arxiv.org/pdf/2403.12995.pdf</a></span>   <span><a href='https://github.com/zhengkangjie/ESM-AA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12995">ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ESM-AA (ESM All-Atom), a novel approach that enables atom-scale and residue-scale unified molecular modeling. ESM-AA achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ESM-AA surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ESM-AA not only gains molecular knowledge but also retains its understanding of proteins. The source codes of ESM-AA are publicly released at https://github.com/zhengkangjie/ESM-AA.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2403.07920.pdf' target='_blank'>https://arxiv.org/pdf/2403.07920.pdf</a></span>   <span><a href='https://protllm.github.io/project/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07920">ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2403.05602.pdf' target='_blank'>https://arxiv.org/pdf/2403.05602.pdf</a></span>   <span><a href='https://github.com/BNLNLP/PPI-Relation-Extraction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby, Shinjae Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05602">Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Because protein-protein interactions (PPIs) are crucial to understand living systems, harvesting these data is essential to probe disease development and discern gene/protein functions and biological processes. Some curated datasets contain PPI data derived from the literature and other sources (e.g., IntAct, BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their maintenance is a labor-intensive process. On the other hand, machine learning methods to automate PPI knowledge extraction from the scientific literature have been limited by a shortage of appropriate annotated data. This work presents a unified, multi-source PPI corpora with vetted interaction definitions augmented by binary interaction type labels and a Transformer-based deep learning method that exploits entities' relational context information for relation representation to improve relation classification performance. The model's performance is evaluated on four widely studied biomedical relation extraction datasets, as well as this work's target PPI datasets, to observe the effectiveness of the representation to relation extraction tasks in various data. Results show the model outperforms prior state-of-the-art models. The code and data are available at: https://github.com/BNLNLP/PPI-Relation-Extraction
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2403.03726.pdf' target='_blank'>https://arxiv.org/pdf/2403.03726.pdf</a></span>   <span><a href='https://github.com/MeshchaninovViacheslav/DiMA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MeshchaninovViacheslav/DiMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03726">Diffusion on language model encodings for protein sequence generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We extensively evaluate existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios. Code is released at \href{https://github.com/MeshchaninovViacheslav/DiMA}{GitHub}.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2402.19009.pdf' target='_blank'>https://arxiv.org/pdf/2402.19009.pdf</a></span>   <span><a href='https://github.com/guangyliu/EDDPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen Li, Shuguang Cui, Julian McAuley, Zichao Yang, Eric P. Xing, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19009">Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like variational autoencoders (VAEs), generative adversarial networks (GANs), autoregressive models, and (latent) diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce Generalized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs) which integrate the core capabilities for broad applicability and enhanced performance. EDDPMs generalize the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, EDDPMs are compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), EDDPMs naturally apply to different data types. Extensive experiments on text, proteins, and images demonstrate the flexibility to handle diverse data and tasks and the strong improvement over various existing models.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2402.18813.pdf' target='_blank'>https://arxiv.org/pdf/2402.18813.pdf</a></span>   <span><a href='https://github.com/zqgao22/PromptMSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18813">Protein Multimer Structure Prediction via Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \textbf{\textsc{PromptMSP}}, a pre-training and \textbf{Prompt} tuning framework for \textbf{M}ultimer \textbf{S}tructure \textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \url{https://github.com/zqgao22/PromptMSP}.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2402.18583.pdf' target='_blank'>https://arxiv.org/pdf/2402.18583.pdf</a></span>   <span><a href='https://github.com/YangLing0818/BindDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilin Huang, Ling Yang, Zaixi Zhang, Xiangxin Zhou, Yu Bao, Xiawu Zheng, Yuwei Yang, Yu Wang, Wenming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18583">Binding-Adaptive Diffusion Models for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) aims to generate 3D ligand molecules that bind to specific protein targets. Existing 3D deep generative models including diffusion models have shown great promise for SBDD. However, it is complex to capture the essential protein-ligand interactions exactly in 3D space for molecular generation. To address this problem, we propose a novel framework, namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively extract subcomplex, the essential part of binding sites responsible for protein-ligand interactions. Then the selected protein-ligand subcomplex is processed with SE(3)-equivariant neural networks, and transmitted back to each atom of the complex for augmenting the target-aware 3D molecule diffusion generation with binding interaction information. We iterate this hierarchical complex-subcomplex process with cross-hierarchy interaction node for adequately fusing global binding context between the complex and its corresponding subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can generate molecules with more realistic 3D structures and higher binding affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while maintaining proper molecular properties. Our code is available at https://github.com/YangLing0818/BindDM
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2402.18567.pdf' target='_blank'>https://arxiv.org/pdf/2402.18567.pdf</a></span>   <span><a href='https://github.com/bytedance/dplm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18567">Diffusion Language Models Are Versatile Protein Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance. Code is released at \url{https://github.com/bytedance/dplm}.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2402.18028.pdf' target='_blank'>https://arxiv.org/pdf/2402.18028.pdf</a></span>   <span><a href='https://github.com/openmedlab' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/openmedlab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, Liang Hong, Qicheng Lao, Tong Ruan, Yukun Zhou, Yixue Li, Jie Zhao, Kang Li, Xin Sun, Lifeng Zhu, Shaoting Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18028">OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2402.17156.pdf' target='_blank'>https://arxiv.org/pdf/2402.17156.pdf</a></span>   <span><a href='https://github.com/Linzy19/TaxDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Zongying, Li Hao, Lv Liuzhenghao, Lin Bin, Zhang Junwu, Chen Calvin Yu-Chian, Yuan Li, Tian Yonghong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17156">TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation benchmarks in both taxonomic-guided controllable generation and unconditional generation. Remarkably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the diffusion model. The code for generating proteins and training new versions of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2402.16445.pdf' target='_blank'>https://arxiv.org/pdf/2402.16445.pdf</a></span>   <span><a href='https://github.com/PKU-YuanGroup/ProLLaMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16445">ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Protein Language Models (PLMs) have transformed protein engineering, yet unlike their counterparts in Natural Language Processing (NLP), current PLMs exhibit a fundamental limitation: they excel in either Protein Language Understanding (PLU) or Protein Language Generation (PLG), but rarely both. This fragmentation hinders progress in protein engineering. To bridge this gap, we introduce ProLLaMA, a multitask protein language model enhanced by the Evolutionary Protein Generation Framework (EPGF). We construct a comprehensive instruction dataset containing approximately 13 million samples with over 11,000 superfamily annotations to facilitate better modeling of sequence-function landscapes. We leverage a two-stage training approach to develop ProLLaMA, a multitask LLM with protein domain expertise. Our EPGF addresses the mismatch between statistic language modeling and biological constraints through three innovations: a multi-dimensional interpretable scorer, hierarchical efficient decoding, and a probabilistic-biophysical joint selection mechanism. Extensive experiments demonstrate that ProLLaMA excels in both unconditional and controllable protein generation tasks, achieving superior structural quality metrics compared to existing PLMs. Additionally, ProLLaMA demonstrates strong understanding capabilities with a 67.1% exact match rate in superfamily prediction. EPGF significantly enhances the biological viability of generated sequences, as evidenced by improved biophysical scores (+4.3%) and structural metrics (+14.5%). The project is available at https://github.com/PKU-YuanGroup/ProLLaMA.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2402.13418.pdf' target='_blank'>https://arxiv.org/pdf/2402.13418.pdf</a></span>   <span><a href='https://github.com/zhiqiangzhongddu/EvolMPNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Zhong, Davide Mottin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13418">Efficiently Predicting Mutational Effect on Homologous Proteins by Evolution Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, an efficient model to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way, EvolMPNN can efficiently utilise a novel message-passing method to capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than state-of-the-art methods and attains 36X inference speedup in comparison with large pre-trained models. Code and models are available at https://github.com/zhiqiangzhongddu/EvolMPNN.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2402.11363.pdf' target='_blank'>https://arxiv.org/pdf/2402.11363.pdf</a></span>   <span><a href='https://github.com/Biocomputing-Research-Group/DiaTrans' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiva Ebrahimi, Xuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11363">Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2402.08703.pdf' target='_blank'>https://arxiv.org/pdf/2402.08703.pdf</a></span>   <span><a href='https://github.com/gersteinlab/GenAI4Drug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li, Tianxiao Li, Mark Gerstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08703">A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2402.06532.pdf' target='_blank'>https://arxiv.org/pdf/2402.06532.pdf</a></span>   <span><a href='https://github.com/michael-s-yao/gabo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael S. Yao, Yimeng Zeng, Hamsa Bastani, Jacob Gardner, James C. Gee, Osbert Bastani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06532">Generative Adversarial Model-Based Optimization via Source Critic Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline model-based optimization seeks to optimize against a learned surrogate model without querying the true oracle objective function during optimization. Such tasks are commonly encountered in protein design, robotics, and clinical medicine where evaluating the oracle function is prohibitively expensive. However, inaccurate surrogate model predictions are frequently encountered along offline optimization trajectories. To address this limitation, we propose generative adversarial model-based optimization using adaptive source critic regularization (aSCR) -- a task- and optimizer- agnostic framework for constraining the optimization trajectory to regions of the design space where the surrogate function is reliable. We propose a computationally tractable algorithm to dynamically adjust the strength of this constraint, and show how leveraging aSCR with standard Bayesian optimization outperforms existing methods on a suite of offline generative design tasks. Our code is available at https://github.com/michael-s-yao/gabo
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2402.05856.pdf' target='_blank'>https://arxiv.org/pdf/2402.05856.pdf</a></span>   <span><a href='https://github.com/DeepGraphLearning/esm-s' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, AurÃ©lie Lozano, Payel Das, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05856">Structure-Informed Protein Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to distill structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at https://github.com/DeepGraphLearning/esm-s.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2402.04845.pdf' target='_blank'>https://arxiv.org/pdf/2402.04845.pdf</a></span>   <span><a href='https://github.com/bjing2016/alphaflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Jing, Bonnie Berger, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04845">AlphaFold Meets Flow Matching for Generating Protein Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2401.15721.pdf' target='_blank'>https://arxiv.org/pdf/2401.15721.pdf</a></span>   <span><a href='https://github.com/bonaventuredossou/ece526_course_project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bonaventure F. P. Dossou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15721">A Study of Acquisition Functions for Medical Imaging Deep Active Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where obtaining labeled data (or annotation budget is very limited). We compare several selection criteria (BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the effect of acquired pool size on the model's performance. Our results suggest that uncertainty is useful to the Melanoma detection task, and confirms the hypotheses of the author of the paper of interest, that \textit{bald} performs on average better than other acquisition functions. Our extended analyses however revealed that all acquisition functions perform badly on the positive (cancerous) samples, suggesting exploitation of class unbalance, which could be crucial in real-world settings. We finish by suggesting future work directions that would be useful to improve this current work. The code of our implementation is open-sourced at \url{https://github.com/bonaventuredossou/ece526_course_project}
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2401.15478.pdf' target='_blank'>https://arxiv.org/pdf/2401.15478.pdf</a></span>   <span><a href='https://github.com/mcneela/Mixed-Curvature-Pathways' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/mcneela/Mixed-Curvature-GCN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel McNeela, Frederic Sala, Anthony Gitter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15478">Product Manifold Representations for Learning on Biological Pathways</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models that embed graphs in non-Euclidean spaces have shown substantial benefits in a variety of contexts, but their application has not been studied extensively in the biological domain, particularly with respect to biological pathway graphs. Such graphs exhibit a variety of complex network structures, presenting challenges to existing embedding approaches. Learning high-quality embeddings for biological pathway graphs is important for researchers looking to understand the underpinnings of disease and train high-quality predictive models on these networks. In this work, we investigate the effects of embedding pathway graphs in non-Euclidean mixed-curvature spaces and compare against traditional Euclidean graph representation learning models. We then train a supervised model using the learned node embeddings to predict missing protein-protein interactions in pathway graphs. We find large reductions in distortion and boosts on in-distribution edge prediction performance as a result of using mixed-curvature embeddings and their corresponding graph neural network models. However, we find that mixed-curvature representations underperform existing baselines on out-of-distribution edge prediction performance suggesting that these representations may overfit to the training graph topology. We provide our Mixed-Curvature Product Graph Convolutional Network code at https://github.com/mcneela/Mixed-Curvature-GCN and our pathway analysis code at https://github.com/mcneela/Mixed-Curvature-Pathways.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2401.14819.pdf' target='_blank'>https://arxiv.org/pdf/2401.14819.pdf</a></span>   <span><a href='https://github.com/BorgwardtLab/PST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14819">Endowing Protein Language Models with Structural Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models. Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures. Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction. Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at https://github.com/BorgwardtLab/PST.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2401.11360.pdf' target='_blank'>https://arxiv.org/pdf/2401.11360.pdf</a></span>   <span><a href='https://github.com/zhangruochi/PepHarmony' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochi Zhang, Haoran Wu, Chang Liu, Huaping Li, Yuqian Wu, Kewei Li, Yifan Wang, Yifan Deng, Jiahui Chen, Fengfeng Zhou, Xin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11360">PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or http://www.healthinformaticslab.org/supp/.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2401.10144.pdf' target='_blank'>https://arxiv.org/pdf/2401.10144.pdf</a></span>   <span><a href='https://github.com/xmed-lab/HCGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Lin, Liang Pan, Yi Li, Ziwei Liu, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10144">Exploiting Hierarchical Interactions for Protein Surface Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for protein surface analysis by bridging chemical and geometric features with hierarchical interactions. Extensive experiments demonstrate that our method outperforms the prior state-of-the-art method by 2.3% in site prediction task and 3.2% in interaction matching task, respectively. Our code is available at https://github.com/xmed-lab/HCGNet.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2401.09176.pdf' target='_blank'>https://arxiv.org/pdf/2401.09176.pdf</a></span>   <span><a href='https://github.com/idrugLab/ADCNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liye Chen, Biaoshun Li, Yihao Chen, Mujie Lin, Shipeng Zhang, Chenxin Li, Yu Pang, Ling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09176">ADCNet: a unified framework for predicting the activity of antibody-drug conjugates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antibody-drug conjugate (ADC) has revolutionized the field of cancer treatment in the era of precision medicine due to their ability to precisely target cancer cells and release highly effective drug. Nevertheless, the realization of rational design of ADC is very difficult because the relationship between their structures and activities is difficult to understand. In the present study, we introduce a unified deep learning framework called ADCNet to help design potential ADCs. The ADCNet highly integrates the protein representation learning language model ESM-2 and small-molecule representation learning language model FG-BERT models to achieve activity prediction through learning meaningful features from antigen and antibody protein sequences of ADC, SMILES strings of linker and payload, and drug-antibody ratio (DAR) value. Based on a carefully designed and manually tailored ADC data set, extensive evaluation results reveal that ADCNet performs best on the test set compared to baseline machine learning models across all evaluation metrics. For example, it achieves an average prediction accuracy of 87.12%, a balanced accuracy of 0.8689, and an area under receiver operating characteristic curve of 0.9293 on the test set. In addition, cross-validation, ablation experiments, and external independent testing results further prove the stability, advancement, and robustness of the ADCNet architecture. For the convenience of the community, we develop the first online platform (https://ADCNet.idruglab.cn) for the prediction of ADCs activity based on the optimal ADCNet model, and the source code is publicly available at https://github.com/idrugLab/ADCNet.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2401.06155.pdf' target='_blank'>https://arxiv.org/pdf/2401.06155.pdf</a></span>   <span><a href='https://github.com/HXYfighter/MolRL-MGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06155">De novo Drug Design using Reinforcement Learning with Multiple GPT Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2401.06151.pdf' target='_blank'>https://arxiv.org/pdf/2401.06151.pdf</a></span>   <span><a href='https://github.com/Profluent-Internships/MMDiff' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Profluent-Internships/MMDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Morehead, Jeffrey Ruffolo, Aadyot Bhatnagar, Ali Madani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06151">Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA molecules while being modestly capable of joint modeling DNA and RNA molecules in interaction with multi-chain protein complexes. Source code: https://github.com/Profluent-Internships/MMDiff.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2401.04082.pdf' target='_blank'>https://arxiv.org/pdf/2401.04082.pdf</a></span>   <span><a href='https://github.com/microsoft/protein-frame-flow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Yim, Andrew Campbell, Emile Mathieu, Andrew Y. K. Foong, Michael Gastegger, JosÃ© JimÃ©nez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Frank NoÃ©, Regina Barzilay, Tommi S. Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04082">Improved motif-scaffolding with SE(3) flow matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein design often begins with the knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a range of motifs. However, generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow without additional training. On a benchmark of 24 biologically meaningful motifs, we show our method achieves 2.5 times more designable and unique motif-scaffolds compared to state-of-the-art. Code: https://github.com/microsoft/protein-frame-flow
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2401.00529.pdf' target='_blank'>https://arxiv.org/pdf/2401.00529.pdf</a></span>   <span><a href='https://github.com/alibaba/graph-gpt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifang Zhao, Weidong Ren, Tianyu Li, Hong Liu, Xingsheng He, Xiaoxiao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00529">GraphGPT: Generative Pre-trained Graph Eulerian Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduceGraphGPT, a novel self-supervised generative pre-trained model for graph learning based on the Graph Eulerian Transformer (GET). First, we propose GET, which combines a standard transformer encoder or decoder architecture with an innovative graph-to-sequence transformation method. This method converts graphs or sampled subgraphs into sequences of tokens representing nodes, edges, and attributes in a reversible manner using Eulerian paths. We pre-train GET using either of the two self-supervised tasks: next-token prediction (NTP) and scheduled masked-token prediction (SMTP). The pre-trained model is then fine-tuned for downstream tasks such as graph-, edge-, and node-level prediction. Despite its simplicity, GraphGPT achieves performance comparable to or surpassing state-of-the-art methods on multiple large-scale Open Graph Benchmark (OGB) datasets. It demonstrates exceptional results on the molecular property prediction dataset PCQM4Mv2 and the protein-protein interaction dataset ogbl-ppa. Notably, generative pre-training enables scaling GraphGPT to 2 billion parameters while maintaining performance gains - a breakthrough that overcomes the scalability limitations of traditional Graph Neural Networks (GNNs) and prior graph transformers (GTs). To advance research in graph foundation models and facilitate scientific discovery in chemistry, materials science, and related fields, we will release the source code (https://github.com/alibaba/graph-gpt) and pre-trained checkpoints.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2507.08920.pdf' target='_blank'>https://arxiv.org/pdf/2507.08920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changze Lv, Jiang Zhou, Siyu Long, Lihao Wang, Jiangtao Feng, Dongyu Xue, Yu Pei, Hao Wang, Zherui Zhang, Yuchen Cai, Zhiqiang Gao, Ziyuan Ma, Jiakai Hu, Chaochen Gao, Jingjing Gong, Yuxuan Song, Shuyi Zhang, Xiaoqing Zheng, Deyi Xiong, Lei Bai, Wanli Ouyang, Ya-Qin Zhang, Wei-Ying Ma, Bowen Zhou, Hao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08920">AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2501.06485.pdf' target='_blank'>https://arxiv.org/pdf/2501.06485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>En Xu, Can Rong, Jingtao Ding, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06485">A Diffusive Data Augmentation Framework for Reconstruction of Complex Network Evolutionary History</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolutionary processes of complex systems contain critical information regarding their functional characteristics. The generation time of edges provides insights into the historical evolution of various networked complex systems, such as protein-protein interaction networks, ecosystems, and social networks. Recovering these evolutionary processes holds significant scientific value, including aiding in the interpretation of the evolution of protein-protein interaction networks. However, existing methods are capable of predicting the generation times of remaining edges given a partial temporal network but often perform poorly in cross-network prediction tasks. These methods frequently fail in edge generation time recovery tasks for static networks that lack timestamps. In this work, we adopt a comparative paradigm-based framework that fuses multiple networks for training, enabling cross-network learning of the relationship between network structure and edge generation times. Compared to separate training, this approach yields an average accuracy improvement of 16.98%. Furthermore, given the difficulty in collecting temporal networks, we propose a novel diffusion-model-based generation method to produce a large number of temporal networks. By combining real temporal networks with generated ones for training, we achieve an additional average accuracy improvement of 5.46% through joint training.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2503.03989.pdf' target='_blank'>https://arxiv.org/pdf/2503.03989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangxin Zhou, Yi Xiao, Haowei Lin, Xinheng He, Jiaqi Guan, Yang Wang, Qiang Liu, Feng Zhou, Liang Wang, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03989">Integrating Protein Dynamics into Structure-Based Drug Design via Full-Atom Stochastic Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dynamic nature of proteins, influenced by ligand interactions, is essential for comprehending protein function and progressing drug discovery. Traditional structure-based drug design (SBDD) approaches typically target binding sites with rigid structures, limiting their practical application in drug development. While molecular dynamics simulation can theoretically capture all the biologically relevant conformations, the transition rate is dictated by the intrinsic energy barrier between them, making the sampling process computationally expensive. To overcome the aforementioned challenges, we propose to use generative modeling for SBDD considering conformational changes of protein pockets. We curate a dataset of apo and multiple holo states of protein-ligand complexes, simulated by molecular dynamics, and propose a full-atom flow model (and a stochastic version), named DynamicFlow, that learns to transform apo pockets and noisy ligands into holo pockets and corresponding 3D ligand molecules. Our method uncovers promising ligand molecules and corresponding holo conformations of pockets. Additionally, the resultant holo-like states provide superior inputs for traditional SBDD approaches, playing a significant role in practical drug discovery.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2401.06199.pdf' target='_blank'>https://arxiv.org/pdf/2401.06199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Chen, Xingyi Cheng, Pan Li, Yangli-ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06199">xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2410.09543.pdf' target='_blank'>https://arxiv.org/pdf/2410.09543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoran Jiao, Weian Mao, Wengong Jin, Peiyuan Yang, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09543">Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the change in binding free energy ($ÎÎG$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental $ÎÎG$ data, existing methods focus on pre-training, while neglecting the importance of alignment. In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to $ÎÎG$ prediction. We begin by analyzing the thermodynamic definition of $ÎÎG$ and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes' theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for $ÎÎG$ estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the $ÎÎG$ thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised), significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we demonstrate the capability of our method on binding energy prediction, protein-protein docking and antibody optimization tasks.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2404.13506.pdf' target='_blank'>https://arxiv.org/pdf/2404.13506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charith Chandra Sai Balne, Sreyoshi Bhaduri, Tamoghna Roy, Vinija Jain, Aman Chadha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13506">Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of deep learning has marked significant progress in fields such as computer vision, natural language processing, and medical imaging, primarily through the adaptation of pre-trained models for specific tasks. Traditional fine-tuning methods, involving adjustments to all parameters, face challenges due to high computational and memory demands. This has led to the development of Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update parameters to balance computational efficiency with performance. This review examines PEFT approaches, offering a detailed comparison of various strategies highlighting applications across different domains, including text generation, medical imaging, protein modeling, and speech synthesis. By assessing the effectiveness of PEFT methods in reducing computational load, speeding up training, and lowering memory usage, this paper contributes to making deep learning more accessible and adaptable, facilitating its wider application and encouraging innovation in model optimization. Ultimately, the paper aims to contribute towards insights into PEFT's evolving landscape, guiding researchers and practitioners in overcoming the limitations of conventional fine-tuning approaches.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2510.09020.pdf' target='_blank'>https://arxiv.org/pdf/2510.09020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekai Chen, Xunkai Li, Sirui Zhang, Henan Sun, Jia Li, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09020">MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>De novo ligand design is a fundamental task that seeks to generate protein or molecule candidates that can effectively dock with protein receptors and achieve strong binding affinity entirely from scratch. It holds paramount significance for a wide spectrum of biomedical applications. However, most existing studies are constrained by the \textbf{Pseudo De Novo}, \textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. To address these issues, we propose MagicDock, a forward-looking framework grounded in the progressive pipeline and differentiable surface modeling. (1) We adopt a well-designed gradient inversion framework. To begin with, general docking knowledge of receptors and ligands is incorporated into the backbone model. Subsequently, the docking knowledge is instantiated as reverse gradient flows by binding prediction, which iteratively guide the de novo generation of ligands. (2) We emphasize differentiable surface modeling in the docking process, leveraging learnable 3D point-cloud representations to precisely capture binding details, thereby ensuring that the generated ligands preserve docking validity through direct and interpretable spatial fingerprints. (3) We introduce customized designs for different ligand types and integrate them into a unified gradient inversion framework with flexible triggers, thereby ensuring broad applicability. Moreover, we provide rigorous theoretical guarantees for each component of MagicDock. Extensive experiments across 9 scenarios demonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\% over SOTA baselines specialized for protein or molecule ligand design, respectively.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2406.16268.pdf' target='_blank'>https://arxiv.org/pdf/2406.16268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lantian Xu, Rong-Hua Li, Dong Wen, Qiangqiang Dai, Guoren Wang, Lu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16268">Efficient Antagonistic k-plex Enumeration in Signed Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A signed graph is a graph where each edge receives a sign, positive or negative. The signed graph model has been used in many real applications, such as protein complex discovery and social network analysis. Finding cohesive subgraphs in signed graphs is a fundamental problem. A k-plex is a common model for cohesive subgraphs in which every vertex is adjacent to all but at most k vertices within the subgraph. In this paper, we propose the model of size-constrained antagonistic k-plex in a signed graph. The proposed model guarantees that the resulting subgraph is a k-plex and can be divided into two sub-k-plexes, both of which have positive inner edges and negative outer edges. This paper aims to identify all maximal antagonistic k-plexes in a signed graph. Through rigorous analysis, we show that the problem is NP-Hardness. We propose a novel framework for maximal antagonistic k-plexes utilizing set enumeration. Efficiency is improved through pivot pruning and early termination based on the color bound. Preprocessing techniques based on degree and dichromatic graphs effectively narrow the search space before enumeration. Extensive experiments on real-world datasets demonstrate our algorithm's efficiency, effectiveness, and scalability.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2503.13522.pdf' target='_blank'>https://arxiv.org/pdf/2503.13522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Zhang, Ningyuan Deng, Xinyuan Song, Ziqian Bi, Tianyang Wang, Zheyu Yao, Keyu Chen, Ming Li, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Liu, Li Zhang, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Yizhu Wen, Lawrence KQ Yan, Hongming Tseng, Yan Zhong, Yunze Wang, Ziyuan Qin, Bowen Jing, Junjie Yang, Jun Zhou, Chia Xin Liang, Junhao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13522">Advanced Deep Learning Methods for Protein Structure Prediction and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>After AlphaFold won the Nobel Prize, protein prediction with deep learning once again became a hot topic. We comprehensively explore advanced deep learning methods applied to protein structure prediction and design. It begins by examining recent innovations in prediction architectures, with detailed discussions on improvements such as diffusion based frameworks and novel pairwise attention modules. The text analyses key components including structure generation, evaluation metrics, multiple sequence alignment processing, and network architecture, thereby illustrating the current state of the art in computational protein modelling. Subsequent chapters focus on practical applications, presenting case studies that range from individual protein predictions to complex biomolecular interactions. Strategies for enhancing prediction accuracy and integrating deep learning techniques with experimental validation are thoroughly explored. The later sections review the industry landscape of protein design, highlighting the transformative role of artificial intelligence in biotechnology and discussing emerging market trends and future challenges. Supplementary appendices provide essential resources such as databases and open source tools, making this volume a valuable reference for researchers and students.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2502.03478.pdf' target='_blank'>https://arxiv.org/pdf/2502.03478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Wang, Silin Chen, Yunze Wang, Yichao Zhang, Xinyuan Song, Ziqian Bi, Ming Liu, Qian Niu, Junyu Liu, Pohsun Feng, Xintian Sun, Benji Peng, Charles Zhang, Keyu Chen, Ming Li, Cheng Fei, Lawrence KQ Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03478">From In Silico to In Vitro: A Comprehensive Guide to Validating Bioinformatics Findings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of bioinformatics predictions and experimental validation plays a pivotal role in advancing biological research, from understanding molecular mechanisms to developing therapeutic strategies. Bioinformatics tools and methods offer powerful means for predicting gene functions, protein interactions, and regulatory networks, but these predictions must be validated through experimental approaches to ensure their biological relevance. This review explores the various methods and technologies used for experimental validation, including gene expression analysis, protein-protein interaction verification, and pathway validation. We also discuss the challenges involved in translating computational predictions to experimental settings and highlight the importance of collaboration between bioinformatics and experimental research. Finally, emerging technologies, such as CRISPR gene editing, next-generation sequencing, and artificial intelligence, are shaping the future of bioinformatics validation and driving more accurate and efficient biological discoveries.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2404.01693.pdf' target='_blank'>https://arxiv.org/pdf/2404.01693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Han, Wenbing Huang, Lingxiao Luo, Xinyan Han, Jiaming Shen, Zhiqiang Zhang, Jun Zhou, Ting Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01693">HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks. While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task. However, each of the tasks is of small size, and such a single-task strategy hinders the models' performance and generalization ability. As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem. In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures. In particular, we first construct a standard structure-based multi-task benchmark called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public datasets. Then, we develop a novel graph neural network for multi-task learning, dubbed Heterogeneous Multichannel Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture heterogeneous relationships between different atoms. Besides, HeMeNet can achieve task-specific learning via the task-aware readout mechanism. Extensive evaluations on our benchmark verify the effectiveness of multi-task learning, and our model generally surpasses state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2412.19198.pdf' target='_blank'>https://arxiv.org/pdf/2412.19198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19198">Multi-Attribute Constraint Satisfaction via Language Model Rewriting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obeying precise constraints on top of multiple external attributes is a common computational problem underlying seemingly different domains, from controlled text generation to protein engineering. Existing language model (LM) controllability methods for multi-attribute constraint satisfaction often rely on specialized architectures or gradient-based classifiers, limiting their flexibility to work with arbitrary black-box evaluators and pretrained models. Current general-purpose large language models, while capable, cannot achieve fine-grained multi-attribute control over external attributes. Thus, we create Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of finetuning language models on any sequential domain to satisfy user-specified constraints on multiple external real-value attributes. Our method trains LMs as editors by sampling diverse multi-attribute edit pairs from an initial set of paraphrased outputs. During inference, LM iteratively improves upon its previous solution to satisfy constraints for all attributes by leveraging our designed constraint satisfaction reward. We additionally experiment with reward-weighted behavior cloning to further improve the constraint satisfaction rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text Style Transfer, where the goal is to simultaneously modify the sentiment and complexity of reviews, and (2) Protein Design, focusing on modulating fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical results show that MACS achieves the highest threshold satisfaction in both FineCS tasks, outperforming strong domain-specific baselines. Our work opens new avenues for generalized and real-value multi-attribute control, with implications for diverse applications spanning NLP and bioinformatics.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2505.23444.pdf' target='_blank'>https://arxiv.org/pdf/2505.23444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runmin Jiang, Genpei Zhang, Yuntian Yang, Siqi Wu, Minhao Wu, Wanyue Feng, Yizhou Zhao, Xi Xiao, Xiao Wang, Tianyang Wang, Xingjian Li, Muyuan Chen, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23444">CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-particle cryo-electron microscopy (cryo-EM) has become a cornerstone of structural biology, enabling near-atomic resolution analysis of macromolecules through advanced computational methods. However, the development of cryo-EM processing tools is constrained by the scarcity of high-quality annotated datasets. Synthetic data generation offers a promising alternative, but existing approaches lack thorough biophysical modeling of heterogeneity and fail to reproduce the complex noise observed in real imaging. To address these limitations, we present CryoCCD, a synthesis framework that unifies versatile biophysical modeling with the first conditional cycle-consistent diffusion model tailored for cryo-EM. The biophysical engine provides multi-functional generation capabilities to capture authentic biological organization, and the diffusion model is enhanced with cycle consistency and mask-guided contrastive learning to ensure realistic noise while preserving structural fidelity. Extensive experiments demonstrate that CryoCCD generates structurally faithful micrographs, enhances particle picking and pose estimation, as well as achieves superior performance over state-of-the-art baselines, while also generalizing effectively to held-out protein families.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2512.12272.pdf' target='_blank'>https://arxiv.org/pdf/2512.12272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Chen, Shang Qu, Zhiqiang Gao, Yuejin Yang, Xiang Zhang, Sheng Xu, Xinjie Mao, Liujia Qian, Jiaqi Wei, Zijie Qiu, Chenyu You, Lei Bai, Ning Ding, Tiannan Guo, Bowen Zhou, Siqi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12272">Accurate de novo sequencing of the modified proteome with OmniNovo</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2506.00925.pdf' target='_blank'>https://arxiv.org/pdf/2506.00925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengdi Liu, Xiaoxue Cheng, Zhangyang Gao, Hong Chang, Cheng Tan, Shiguang Shan, Xilin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00925">ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein sequences that fold into a target 3D structure, known as protein inverse folding, is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by expanding the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2502.01383.pdf' target='_blank'>https://arxiv.org/pdf/2502.01383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergei Kholkin, Ivan Butakov, Evgeny Burnaev, Nikita Gushchin, Alexander Korotin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01383">InfoBridge: Mutual Information estimation via Bridge Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion bridge models have recently become a powerful tool in the field of generative modeling. In this work, we leverage their power to address another important problem in machine learning and information theory, the estimation of the mutual information (MI) between two random variables. We show that by using the theory of diffusion bridges, one can construct an unbiased estimator for data posing difficulties for conventional MI estimators. We showcase the performance of our estimator on two standard MI estimation benchmarks, i.e., low-dimensional and image-based, and on real-world data, i.e., protein language model embeddings.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2509.11782.pdf' target='_blank'>https://arxiv.org/pdf/2509.11782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bozhen Hu, Cheng Tan, Siyuan Li, Jiangbin Zheng, Sizhe Qiu, Jun Xia, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11782">Multimodal Regression for Enzyme Turnover Rates Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The enzyme turnover rate is a fundamental parameter in enzyme kinetics, reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates remain scarce across most organisms due to the high cost and complexity of experimental measurements. To address this gap, we propose a multimodal framework for predicting the enzyme turnover rate by integrating enzyme sequences, substrate structures, and environmental factors. Our model combines a pre-trained language model and a convolutional neural network to extract features from protein sequences, while a graph neural network captures informative representations from substrate molecules. An attention mechanism is incorporated to enhance interactions between enzyme and substrate representations. Furthermore, we leverage symbolic regression via Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that govern the enzyme turnover rate, enabling interpretable and accurate predictions. Extensive experiments demonstrate that our framework outperforms both traditional and state-of-the-art deep learning approaches. This work provides a robust tool for studying enzyme kinetics and holds promise for applications in enzyme engineering, biotechnology, and industrial biocatalysis.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2506.08365.pdf' target='_blank'>https://arxiv.org/pdf/2506.08365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08365">AlphaFold Database Debiasing for Robust Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The AlphaFold Protein Structure Database (AFDB) offers unparalleled structural coverage at near-experimental accuracy, positioning it as a valuable resource for data-driven protein design. However, its direct use in training deep models that are sensitive to fine-grained atomic geometry, such as inverse folding, exposes a critical limitation. Comparative analysis of structural feature distributions reveals that AFDB structures exhibit distinct statistical regularities, reflecting a systematic geometric bias that deviates from the conformational diversity found in experimentally determined structures from the Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized, PDB structures capture the intrinsic variability and physical realism essential for generalization in downstream tasks. To address this discrepancy, we introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct native-like conformations from intentionally corrupted backbone geometries. By training the model to recover plausible structural states, DeSAE implicitly captures a more robust and natural structural manifold. At inference, applying DeSAE to AFDB structures produces debiased structures that significantly improve inverse folding performance across multiple benchmarks. This work highlights the critical impact of subtle systematic biases in predicted structures and presents a principled framework for debiasing, significantly boosting the performance of structure-based learning tasks like inverse folding.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2410.15010.pdf' target='_blank'>https://arxiv.org/pdf/2410.15010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhe Liu, Jun Xia, Lecheng Zhang, Yuchen Liu, Yue Liu, Wenjie Du, Zhangyang Gao, Bozhen Hu, Cheng Tan, Hongxin Xiang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15010">FlexMol: A Flexible Toolkit for Benchmarking Molecular Relational Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular relational learning (MRL) is crucial for understanding the interaction behaviors between molecular pairs, a critical aspect of drug discovery and development. However, the large feasible model space of MRL poses significant challenges to benchmarking, and existing MRL frameworks face limitations in flexibility and scope. To address these challenges, avoid repetitive coding efforts, and ensure fair comparison of models, we introduce FlexMol, a comprehensive toolkit designed to facilitate the construction and evaluation of diverse model architectures across various datasets and performance metrics. FlexMol offers a robust suite of preset model components, including 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers. With its easy-to-use API and flexibility, FlexMol supports the dynamic construction of over 70, 000 distinct combinations of model architectures. Additionally, we provide detailed benchmark results and code examples to demonstrate FlexMol's effectiveness in simplifying and standardizing MRL model development and comparison.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2407.00050.pdf' target='_blank'>https://arxiv.org/pdf/2407.00050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Gao, Cheng Tan, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00050">FoldToken2: Learning compact, invariant and generative protein structure language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20\% in TMScore and 81\% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2405.18968.pdf' target='_blank'>https://arxiv.org/pdf/2405.18968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Gao, Jue Wang, Cheng Tan, Lirong Wu, Yufei Huang, Siyuan Li, Zhirui Ye, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18968">UniIF: Unified Molecule Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecule inverse folding has been a long-standing challenge in chemistry and biology, with the potential to revolutionize drug discovery and material science. Despite specified models have been proposed for different small- or macro-molecules, few have attempted to unify the learning process, resulting in redundant efforts. Complementary to recent advancements in molecular structure prediction, such as RoseTTAFold All-Atom and AlphaFold3, we propose the unified model UniIF for the inverse folding of all molecules. We do such unification in two levels: 1) Data-Level: We propose a unified block graph data form for all molecules, including the local frame building and geometric feature initialization. 2) Model-Level: We introduce a geometric block attention network, comprising a geometric interaction, interactive attention and virtual long-term dependency modules, to capture the 3D interactions of all molecules. Through comprehensive evaluations across various tasks such as protein design, RNA design, and material design, we demonstrate that our proposed method surpasses state-of-the-art methods on all tasks. UniIF offers a versatile and effective solution for general molecule inverse folding.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2403.09673.pdf' target='_blank'>https://arxiv.org/pdf/2403.09673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09673">FoldToken: Learning Protein Language via Vector Quantization and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\textbf{SoftCVQ}).
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2402.09416.pdf' target='_blank'>https://arxiv.org/pdf/2402.09416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bozhen Hu, Zelin Zang, Cheng Tan, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09416">Deep Manifold Transformation for Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning is critical in various tasks in biology, such as drug design and protein structure or function prediction, which has primarily benefited from protein language models and graph neural networks. These models can capture intrinsic patterns from protein sequences and structures through masking and task-related losses. However, the learned protein representations are usually not well optimized, leading to performance degradation due to limited data, difficulty adapting to new tasks, etc. To address this, we propose a new \underline{d}eep \underline{m}anifold \underline{t}ransformation approach for universal \underline{p}rotein \underline{r}epresentation \underline{l}earning (DMTPRL). It employs manifold learning strategies to improve the quality and adaptability of the learned embeddings. Specifically, we apply a novel manifold learning loss during training based on the graph inter-node similarity. Our proposed DMTPRL method outperforms state-of-the-art baselines on diverse downstream tasks across popular datasets. This validates our approach for learning universal and robust protein representations. We promise to release the code after acceptance.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2402.08198.pdf' target='_blank'>https://arxiv.org/pdf/2402.08198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lirong Wu, Yufei Huang, Cheng Tan, Zhangyang Gao, Bozhen Hu, Haitao Lin, Zicheng Liu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08198">PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow contrasting to be performed at different scales, from the amino acid level to the sequence level. Finally, in order to more fairly evaluate the model generalizability, we split the test data into four settings based on whether compounds and proteins have been observed during the training stage. Extensive experiments have shown that PSC-CPI generalizes well in all four settings, particularly in the more challenging ``Unseen-Both" setting, where neither compounds nor proteins have been observed during training. Furthermore, even when encountering a situation of modality missing, i.e., inference with only single-modality protein data, PSC-CPI still exhibits comparable or even better performance than previous approaches.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2508.10696.pdf' target='_blank'>https://arxiv.org/pdf/2508.10696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Jiang, Shuzhou Sun, Biqing Qi, Yuchen Fu, Xiaohua Xu, Yuqiang Li, Dongzhan Zhou, Tianfan Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10696">Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2510.27281.pdf' target='_blank'>https://arxiv.org/pdf/2510.27281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghui Li, Yuanhang Wang, Peijin Guo, Wei Wan, Shengshan Hu, Shengqing Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27281">HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing experimental costs and accelerating early screening in computational drug discovery. While sequence-based deep learning methods avoid reliance on costly 3D structures, they still overlook simultaneous modeling of global sequence semantic features and local topological structural features within drugs and proteins, and represent drugs as flat sequences without atomic-level, substructural-level, and molecular-level multi-scale features. We propose HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract both global sequence semantic and local topological features from drug and protein sequences, and models drugs multi-scale to learn atomic, substructural, and molecular representations fused via a multi-scale bilinear attention module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms state-of-the-art baselines, with ablations confirming the importance of global-local extraction and multi-scale fusion.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2505.00364.pdf' target='_blank'>https://arxiv.org/pdf/2505.00364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Yuwen Wang, Kaixuan Chen, Tongya Zheng, Yihe Zhou, Zhenbang Xiao, Ji Cao, Mingli Song, Shunyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00364">From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying reasoning behind model predictions, attributing their decisions to specific subgraphs that are informative. However, existing subgraph-based interpretable methods suffer from an overemphasis on local structure, potentially overlooking long-range dependencies within the entire graphs. Although recent efforts that rely on graph coarsening have proven beneficial for global interpretability, they inevitably reduce the graphs to a fixed granularity. Such an inflexible way can only capture graph connectivity at a specific level, whereas real-world graph tasks often exhibit relationships at varying granularities (e.g., relevant interactions in proteins span from functional groups, to amino acids, and up to protein domains). In this paper, we introduce a novel Tree-like Interpretable Framework (TIF) for graph classification, where plain GNNs are transformed into hierarchical trees, with each level featuring coarsened graphs of different granularity as tree nodes. Specifically, TIF iteratively adopts a graph coarsening module to compress original graphs (i.e., root nodes of trees) into increasingly coarser ones (i.e., child nodes of trees), while preserving diversity among tree nodes within different branches through a dedicated graph perturbation module. Finally, we propose an adaptive routing module to identify the most informative root-to-leaf paths, providing not only the final prediction but also the multi-granular interpretability for the decision-making process. Extensive experiments on the graph classification benchmarks with both synthetic and real-world datasets demonstrate the superiority of TIF in interpretability, while also delivering a competitive prediction performance akin to the state-of-the-art counterparts.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2503.16278.pdf' target='_blank'>https://arxiv.org/pdf/2503.16278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuqi Lu, Haowei Lin, Lin Yao, Zhifeng Gao, Xiaohong Ji, Yitao Liang, Weinan E, Linfeng Zhang, Guolin Ke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16278">Unified Cross-Scale 3D Generation and Understanding via Autoregressive Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D structure modeling is essential across scales, enabling applications from fluid simulation and 3D reconstruction to protein folding and molecular docking. Yet, despite shared 3D spatial patterns, current approaches remain fragmented, with models narrowly specialized for specific domains and unable to generalize across tasks or scales. We propose Uni-3DAR, a unified autoregressive framework for cross-scale 3D generation and understanding. At its core is a coarse-to-fine tokenizer based on octree data structures, which compresses diverse 3D structures into compact 1D token sequences. We further propose a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. To address the challenge of dynamically varying token positions introduced by compression, we introduce a masked next-token prediction strategy that ensures accurate positional modeling, significantly boosting model performance. Extensive experiments across multiple 3D generation and understanding tasks, including small molecules, proteins, polymers, crystals, and macroscopic 3D objects, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\% relative improvement while delivering inference speeds up to 21.8x faster.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2502.07299.pdf' target='_blank'>https://arxiv.org/pdf/2502.07299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Liu, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07299">Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2405.10348.pdf' target='_blank'>https://arxiv.org/pdf/2405.10348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lirong Wu, Yijun Tian, Haitao Lin, Yufei Huang, Siyuan Li, Nitesh V Chawla, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10348">Learning to Predict Mutation Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein bindings play a key role in a variety of fundamental biological processes, and thus predicting the effects of amino acid mutations on protein-protein binding is crucial. To tackle the scarcity of annotated mutation data, pre-training with massive unlabeled data has emerged as a promising solution. However, this process faces a series of challenges: (1) complex higher-order dependencies among multiple (more than paired) structural scales have not yet been fully captured; (2) it is rarely explored how mutations alter the local conformation of the surrounding microenvironment; (3) pre-training is costly, both in data size and computational burden. In this paper, we first construct a hierarchical prompt codebook to record common microenvironmental patterns at different structural scales independently. Then, we develop a novel codebook pre-training task, namely masked microenvironment modeling, to model the joint distribution of each mutation with their residue types, angular statistics, and local conformational changes in the microenvironment. With the constructed prompt codebook, we encode the microenvironment around each mutation into multiple hierarchical prompts and combine them to flexibly provide information to wild-type and mutated protein complexes about their microenvironmental differences. Such a hierarchical prompt learning framework has demonstrated superior performance and training efficiency over state-of-the-art pre-training-based methods in mutation effect prediction and a case study of optimizing human antibodies against SARS-CoV-2.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2402.16901.pdf' target='_blank'>https://arxiv.org/pdf/2402.16901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ChenRui Duan, Zelin Zang, Yongjie Xu, Hang He, Zihan Liu, Siyuan Li, Zijia Song, Ju-Sheng Zheng, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16901">FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer, which limits the capture of structurally and functionally relevant gene contexts. Moreover, these approaches struggle with encoding biologically meaningful genes and fail to address the One-to-Many and Many-to-One relationships inherent in metagenomic data. To overcome these challenges, we introduce FGBERT, a novel metagenomic pre-trained model that employs a protein-based gene representation as a context-aware and structure-relevant tokenizer. FGBERT incorporates Masked Gene Modeling (MGM) to enhance the understanding of inter-gene contextual relationships and Triplet Enhanced Metagenomic Contrastive Learning (TMC) to elucidate gene sequence-function relationships. Pre-trained on over 100 million metagenomic sequences, FGBERT demonstrates superior performance on metagenomic datasets at four levels, spanning gene, functional, bacterial, and environmental levels and ranging from 1k to 213k input sequences. Case studies of ATP Synthase and Gene Operons highlight FGBERT's capability for functional recognition and its biological relevance in metagenomic research.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2402.14391.pdf' target='_blank'>https://arxiv.org/pdf/2402.14391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh V Chawla, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14391">MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the "vocabulary" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment "vocabulary" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2511.07006.pdf' target='_blank'>https://arxiv.org/pdf/2511.07006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowei He, Bowen Gao, Yankai Chen, Yanyan Lan, Chen Ma, Philip S. Yu, Ya-Qin Zhang, Wei-Ying Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07006">S$^2$Drug: Bridging Protein Sequence and 3D Structure in Contrastive Representation Learning for Virtual Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual screening (VS) is an essential task in drug discovery, focusing on the identification of small-molecule ligands that bind to specific protein pockets. Existing deep learning methods, from early regression models to recent contrastive learning approaches, primarily rely on structural data while overlooking protein sequences, which are more accessible and can enhance generalizability. However, directly integrating protein sequences poses challenges due to the redundancy and noise in large-scale protein-ligand datasets. To address these limitations, we propose \textbf{S$^2$Drug}, a two-stage framework that explicitly incorporates protein \textbf{S}equence information and 3D \textbf{S}tructure context in protein-ligand contrastive representation learning. In the first stage, we perform protein sequence pretraining on ChemBL using an ESM2-based backbone, combined with a tailored data sampling strategy to reduce redundancy and noise on both protein and ligand sides. In the second stage, we fine-tune on PDBBind by fusing sequence and structure information through a residue-level gating module, while introducing an auxiliary binding site prediction task. This auxiliary task guides the model to accurately localize binding residues within the protein sequence and capture their 3D spatial arrangement, thereby refining protein-ligand matching. Across multiple benchmarks, S$^2$Drug consistently improves virtual screening performance and achieves strong results on binding site prediction, demonstrating the value of bridging sequence and structure in contrastive learning.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2506.06294.pdf' target='_blank'>https://arxiv.org/pdf/2506.06294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunqing Liu, Wenqi Fan, Xiaoyong Wei, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06294">GLProtein: Global-and-Local Structure Aware Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2501.10282.pdf' target='_blank'>https://arxiv.org/pdf/2501.10282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqi Fan, Yi Zhou, Shijie Wang, Yuyao Yan, Hui Liu, Qian Zhao, Le Song, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10282">Computational Protein Science in the Era of Large Language Models (LLMs)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Considering the significance of proteins, computational protein science has always been a critical scientific field, dedicated to revealing knowledge and developing applications within the protein sequence-structure-function paradigm. In the last few decades, Artificial Intelligence (AI) has made significant impacts in computational protein science, leading to notable successes in specific protein modeling tasks. However, those previous AI models still meet limitations, such as the difficulty in comprehending the semantics of protein sequences, and the inability to generalize across a wide range of protein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to their unprecedented language processing & generalization capability. They can promote comprehensive progress in fields rather than solving individual tasks. As a result, researchers have actively introduced LLM techniques in computational protein science, developing protein Language Models (pLMs) that skillfully grasp the foundational knowledge of proteins and can be effectively generalized to solve a diversity of sequence-structure-function reasoning problems. While witnessing prosperous developments, it's necessary to present a systematic overview of computational protein science empowered by LLM techniques. First, we summarize existing pLMs into categories based on their mastered protein knowledge, i.e., underlying sequence patterns, explicit structural and functional information, and external scientific languages. Second, we introduce the utilization and adaptation of pLMs, highlighting their remarkable achievements in promoting protein structure prediction, protein function prediction, and protein design studies. Then, we describe the practical application of pLMs in antibody design, enzyme design, and drug discovery. Finally, we specifically discuss the promising future directions in this fast-growing field.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2507.04832.pdf' target='_blank'>https://arxiv.org/pdf/2507.04832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Han, Austin Wang, Minkai Xu, Wenda Chu, Meihua Dang, Yisong Yue, Stefano Ermon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04832">Discrete Diffusion Trajectory Alignment via Stepwise Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discrete diffusion models have demonstrated great promise in modeling various sequence data, ranging from human language to biological sequences. Inspired by the success of RL in language models, there is growing interest in further improving the models by alignment with a certain reward. In this work, we propose a novel preference optimization method for masked discrete diffusion models through a principled diffusion trajectory alignment. Instead of applying the reward on the final output and backpropagating the gradient to the entire discrete denoising process, we decompose the problem into a set of stepwise alignment objectives. This framework enables efficient diffusion optimization, is compatible with arbitrary reward functions, and importantly, guarantees an equivalent optimal solution under additive factorization of the trajectory reward. Experiments across multiple domains including DNA sequence design, protein inverse folding, and language modeling consistently demonstrate the superiority of our approach. Notably, it achieves an up to 12\% improvement over the most competitive RL-based baseline in terms of predicted activity on DNA sequence design, and further improves the GSM8K score from 78.6 to 80.7 on LLaDA-8B-Instruct for language modeling.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2410.13027.pdf' target='_blank'>https://arxiv.org/pdf/2410.13027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, Stefano Ermon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13027">Geometric Trajectory Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2402.11459.pdf' target='_blank'>https://arxiv.org/pdf/2402.11459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan. Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11459">Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2504.10983.pdf' target='_blank'>https://arxiv.org/pdf/2504.10983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitai Kong, Yiheng Zhu, Yinlong Xu, Hanjing Zhou, Mingzhe Yin, Jialu Wu, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10983">ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of protein sequences with desired functionalities is a fundamental task in protein engineering. Deep generative methods, such as autoregressive models and diffusion models, have greatly accelerated the discovery of novel protein sequences. However, these methods mainly focus on local or shallow residual semantics and suffer from low inference efficiency, large modeling space and high training cost. To address these challenges, we introduce ProtFlow, a fast flow matching-based protein sequence design framework that operates on embeddings derived from semantically meaningful latent space of protein language models. By compressing and smoothing the latent space, ProtFlow enhances performance while training on limited computational resources. Leveraging reflow techniques, ProtFlow enables high-quality single-step sequence generation. Additionally, we develop a joint design pipeline for the design scene of multichain proteins. We evaluate ProtFlow across diverse protein design tasks, including general peptides and long-chain proteins, antimicrobial peptides, and antibodies. Experimental results demonstrate that ProtFlow outperforms task-specific methods in these applications, underscoring its potential and broad applicability in computational protein sequence design and analysis.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2412.20014.pdf' target='_blank'>https://arxiv.org/pdf/2412.20014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanjing Zhou, Mingze Yin, Wei Wu, Mingyang Li, Kun Fu, Jintai Chen, Jian Wu, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20014">ProtCLIP: Function-Informed Protein Multi-Modal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modality pre-training paradigm that aligns protein sequences and biological descriptions has learned general protein representations and achieved promising performance in various downstream applications. However, these works were still unable to replicate the extraordinary success of language-supervised visual foundation models due to the ineffective usage of aligned protein-text paired data and the lack of an effective function-informed pre-training paradigm. To address these issues, this paper curates a large-scale protein-text paired dataset called ProtAnno with a property-driven sampling strategy, and introduces a novel function-informed protein pre-training paradigm. Specifically, the sampling strategy determines selecting probability based on the sample confidence and property coverage, balancing the data quality and data quantity in face of large-scale noisy data. Furthermore, motivated by significance of the protein specific functional mechanism, the proposed paradigm explicitly model protein static and dynamic functional segments by two segment-wise pre-training objectives, injecting fine-grained information in a function-informed manner. Leveraging all these innovations, we develop ProtCLIP, a multi-modality foundation model that comprehensively represents function-aware protein embeddings. On 22 different protein benchmarks within 5 types, including protein functionality classification, mutation effect prediction, cross-modal transformation, semantic similarity inference and protein-protein interaction prediction, our ProtCLIP consistently achieves SOTA performance, with remarkable improvements of 75% on average in five cross-modal transformation benchmarks, 59.9% in GO-CC and 39.7% in GO-BP protein function prediction. The experimental results verify the extraordinary potential of ProtCLIP serving as the protein multi-modality foundation model.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2407.19296.pdf' target='_blank'>https://arxiv.org/pdf/2407.19296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingze Yin, Hanjing Zhou, Yiheng Zhu, Miao Lin, Yixuan Wu, Jialu Wu, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou, Jintai Chen, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19296">Multi-Modal CLIP-Informed Protein Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins govern most biological functions essential for life, but achieving controllable protein discovery and optimization remains challenging. Recently, machine learning-assisted protein editing (MLPE) has shown promise in accelerating optimization cycles and reducing experimental workloads. However, current methods struggle with the vast combinatorial space of potential protein edits and cannot explicitly conduct protein editing using biotext instructions, limiting their interactivity with human feedback. To fill these gaps, we propose a novel method called ProtET for efficient CLIP-informed protein editing through multi-modality learning. Our approach comprises two stages: in the pretraining stage, contrastive learning aligns protein-biotext representations encoded by two large language models (LLMs), respectively. Subsequently, during the protein editing stage, the fused features from editing instruction texts and original protein sequences serve as the final editing condition for generating target protein sequences. Comprehensive experiments demonstrated the superiority of ProtET in editing proteins to enhance human-expected functionality across multiple attribute domains, including enzyme catalytic activity, protein stability and antibody specific binding ability. And ProtET improves the state-of-the-art results by a large margin, leading to significant stability improvements of 16.67% and 16.90%. This capability positions ProtET to advance real-world artificial protein editing, potentially addressing unmet academic, industrial, and clinical needs.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2404.00254.pdf' target='_blank'>https://arxiv.org/pdf/2404.00254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Quan, Wenguan Wang, Fan Ma, Hehe Fan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00254">Clustering for Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2512.20954.pdf' target='_blank'>https://arxiv.org/pdf/2512.20954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20954">Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2506.13485.pdf' target='_blank'>https://arxiv.org/pdf/2506.13485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Jiaqi Wei, Zijie Qiu, Sheng Xu, Nanqing Dong, Zhiqiang Gao, Siqi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13485">Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptide sequencing-the process of identifying amino acid sequences from mass spectrometry data-is a fundamental task in proteomics. Non-Autoregressive Transformers (NATs) have proven highly effective for this task, outperforming traditional methods. Unlike autoregressive models, which generate tokens sequentially, NATs predict all positions simultaneously, leveraging bidirectional context through unmasked self-attention. However, existing NAT approaches often rely on Connectionist Temporal Classification (CTC) loss, which presents significant optimization challenges due to CTC's complexity and increases the risk of training failures. To address these issues, we propose an improved non-autoregressive peptide sequencing model that incorporates a structured protein sequence curriculum learning strategy. This approach adjusts protein's learning difficulty based on the model's estimated protein generational capabilities through a sampling process, progressively learning peptide generation from simple to complex sequences. Additionally, we introduce a self-refining inference-time module that iteratively enhances predictions using learned NAT token embeddings, improving sequence accuracy at a fine-grained level. Our curriculum learning strategy reduces NAT training failures frequency by more than 90% based on sampled training over various data distributions. Evaluations on nine benchmark species demonstrate that our approach outperforms all previous methods across multiple metrics and species.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2408.15299.pdf' target='_blank'>https://arxiv.org/pdf/2408.15299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqing Shen, Zan Chen, Michail Mamalakis, Yungeng Liu, Tianbin Li, Yanzhou Su, Junjun He, Pietro LiÃ², Yu Guang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15299">TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The structural similarities between protein sequences and natural languages have led to parallel advancements in deep learning across both domains. While large language models (LLMs) have achieved much progress in the domain of natural language processing, their potential in protein engineering remains largely unexplored. Previous approaches have equipped LLMs with protein understanding capabilities by incorporating external protein encoders, but this fails to fully leverage the inherent similarities between protein sequences and natural languages, resulting in sub-optimal performance and increased model complexity. To address this gap, we present TourSynbio-7B, the first multi-modal large model specifically designed for protein engineering tasks without external protein encoders. TourSynbio-7B demonstrates that LLMs can inherently learn to understand proteins as language. The model is post-trained and instruction fine-tuned on InternLM2-7B using ProteinLMDataset, a dataset comprising 17.46 billion tokens of text and protein sequence for self-supervised pretraining and 893K instructions for supervised fine-tuning. TourSynbio-7B outperforms GPT-4 on the ProteinLMBench, a benchmark of 944 manually verified multiple-choice questions, with 62.18% accuracy. Leveraging TourSynbio-7B's enhanced protein sequence understanding capability, we introduce TourSynbio-Agent, an innovative framework capable of performing various protein engineering tasks, including mutation analysis, inverse folding, protein folding, and visualization. TourSynbio-Agent integrates previously disconnected deep learning models in the protein engineering domain, offering a unified conversational user interface for improved usability. Finally, we demonstrate the efficacy of TourSynbio-7B and TourSynbio-Agent through two wet lab case studies on vanilla key enzyme modification and steroid compound catalysis.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2406.05540.pdf' target='_blank'>https://arxiv.org/pdf/2406.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, Yu Guang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05540">A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The parallels between protein sequences and natural language in their sequential structures have inspired the application of large language models (LLMs) to protein understanding. Despite the success of LLMs in NLP, their effectiveness in comprehending protein sequences remains an open question, largely due to the absence of datasets linking protein sequences to descriptive text. Researchers have then attempted to adapt LLMs for protein understanding by integrating a protein sequence encoder with a pre-trained LLM. However, this adaptation raises a fundamental question: "Can LLMs, originally designed for NLP, effectively comprehend protein sequences as a form of language?" Current datasets fall short in addressing this question due to the lack of a direct correlation between protein sequences and corresponding text descriptions, limiting the ability to train and evaluate LLMs for protein understanding effectively. To bridge this gap, we introduce ProteinLMDataset, a dataset specifically designed for further self-supervised pretraining and supervised fine-tuning (SFT) of LLMs to enhance their capability for protein sequence comprehension. Specifically, ProteinLMDataset includes 17.46 billion tokens for pretraining and 893,000 instructions for SFT. Additionally, we present ProteinLMBench, the first benchmark dataset consisting of 944 manually verified multiple-choice questions for assessing the protein understanding capabilities of LLMs. ProteinLMBench incorporates protein-related details and sequences in multiple languages, establishing a new standard for evaluating LLMs' abilities in protein comprehension. The large language model InternLM2-7B, pretrained and fine-tuned on the ProteinLMDataset, outperforms GPT-4 on ProteinLMBench, achieving the highest accuracy score.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2402.09649.pdf' target='_blank'>https://arxiv.org/pdf/2402.09649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09649">ProtChatGPT: Towards Understanding Proteins with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2510.03095.pdf' target='_blank'>https://arxiv.org/pdf/2510.03095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyang Xie, Haoran Zhang, Zhendong Wang, Wesley Tansey, Mingyuan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03095">Distilled Protein Backbone Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.05309.pdf' target='_blank'>https://arxiv.org/pdf/2509.05309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Liu, Haodi Lei, Yi Liu, Yang Liu, Wei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05309">ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic interpretability of large language models. Recent works apply SAE to protein language models (PLMs), aiming to extract and analyze biologically meaningful features from their latent spaces. However, SAE suffers from semantic entanglement, where individual neurons often mix multiple nonlinear concepts, making it difficult to reliably interpret or manipulate model behaviors. In this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike existing SAE which requires annotation datasets to filter and interpret activations, we guide semantic disentanglement during training using both annotation datasets and domain knowledge to mitigate the effects of entangled attributes. We design interpretability experiments showing that ProtSAE learns more biologically relevant and interpretable hidden features compared to previous methods. Performance analyses further demonstrate that ProtSAE maintains high reconstruction fidelity while achieving better results in interpretable probing. We also show the potential of ProtSAE in steering PLMs for downstream generation tasks.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2505.19014.pdf' target='_blank'>https://arxiv.org/pdf/2505.19014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Lin, Odin Zhang, Jia Xu, Yunfan Liu, Zheng Cheng, Lirong Wu, Yufei Huang, Zhifeng Gao, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19014">Tokenizing Electron Cloud in Protein-Ligand Interaction Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulation and signal transduction. Most deep-learning-based prediction approaches focus on structures of atoms or fragments. However, quantum chemical properties, such as electronic structures, are the key to unveiling interaction patterns but remain largely underexplored. To bridge this gap, we propose ECBind, a method for tokenizing electron cloud signals into quantized embeddings, enabling their integration into downstream tasks such as binding affinity prediction. By incorporating electron densities, ECBind helps uncover binding modes that cannot be fully represented by atom-level models. Specifically, to remove the redundancy inherent in electron cloud signals, a structure-aware transformer and hierarchical codebooks encode 3D binding sites enriched with electron structures into tokens. These tokenized codes are then used for specific tasks with labels. To extend its applicability to a wider range of scenarios, we utilize knowledge distillation to develop an electron-cloud-agnostic prediction model. Experimentally, ECBind demonstrates state-of-the-art performance across multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure Pearson and Spearman correlation coefficients, respectively.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2502.06913.pdf' target='_blank'>https://arxiv.org/pdf/2502.06913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Guojiang Zhao, Zhifeng Gao, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06913">A Simple yet Effective DDG Predictor is An Unsupervised Antibody Optimizer and Explainer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on the fitness landscape is beneficial. There have been numerous priors used to constrain protein evolution to regions of landscapes with high-fitness variants, among which the change in binding free energy (DDG) of protein complexes upon mutations is one of the most commonly used priors. However, the huge mutation space poses two challenges: (1) how to improve the efficiency of DDG prediction for fast mutation screening; and (2) how to explain mutation preferences and efficiently explore accessible evolutionary regions. To address these challenges, we propose a lightweight DDG predictor (Light-DDG), which adopts a structure-aware Transformer as the backbone and enhances it by knowledge distilled from existing powerful but computationally heavy DDG predictors. Additionally, we augmented, annotated, and released a large-scale dataset containing millions of mutation data for pre-training Light-DDG. We find that such a simple yet effective Light-DDG can serve as a good unsupervised antibody optimizer and explainer. For the target antibody, we propose a novel Mutation Explainer to learn mutation preferences, which accounts for the marginal benefit of each mutation per residue. To further explore accessible evolutionary regions, we conduct preference-guided antibody optimization and evaluate antibody candidates quickly using Light-DDG to identify desirable mutations.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2406.11906.pdf' target='_blank'>https://arxiv.org/pdf/2406.11906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Zhou, Shaorong Chen, Jun Xia, Sizhe Liu, Tianze Ling, Wenjie Du, Yue Liu, Jianwei Yin, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11906">NovoBench: Benchmarking Deep Learning-based De Novo Peptide Sequencing Methods in Proteomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Many deep learning methods have been developed for \emph{de novo} peptide sequencing task, i.e., predicting the peptide sequence for the observed mass spectrum. However, two key challenges seriously hinder the further advancement of this important task. Firstly, since there is no consensus for the evaluation datasets, the empirical results in different research papers are often not comparable, leading to unfair comparison. Secondly, the current methods are usually limited to amino acid-level or peptide-level precision and recall metrics. In this work, we present the first unified benchmark NovoBench for \emph{de novo} peptide sequencing, which comprises diverse mass spectrum data, integrated models, and comprehensive evaluation metrics. Recent impressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo and $Ï$-HelixNovo are integrated into our framework. In addition to amino acid-level and peptide-level precision and recall, we evaluate the models' performance in terms of identifying post-tranlational modifications (PTMs), efficiency and robustness to peptide length, noise peaks and missing fragment ratio, which are important influencing factors while seldom be considered. Leveraging this benchmark, we conduct a large-scale study of current methods, report many insightful findings that open up new possibilities for future development.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2405.06642.pdf' target='_blank'>https://arxiv.org/pdf/2405.06642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Lin, Odin Zhang, Huifeng Zhao, Dejun Jiang, Lirong Wu, Zicheng Liu, Yufei Huang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06642">PPFlow: Target-aware Peptide Design with Torsional Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Therapeutic peptides have proven to have great pharmaceutical value and potential in recent decades. However, methods of AI-assisted peptide drug discovery are not fully explored. To fill the gap, we propose a target-aware peptide design method called \textsc{PPFlow}, based on conditional flow matching on torus manifolds, to model the internal geometries of torsion angles for the peptide structure design. Besides, we establish a protein-peptide binding dataset named PPBench2024 to fill the void of massive data for the task of structure-based peptide drug design and to allow the training of deep learning methods. Extensive experiments show that PPFlow reaches state-of-the-art performance in tasks of peptide drug generation and optimization in comparison with baseline models, and can be generalized to other tasks including docking and side-chain packing.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2405.01616.pdf' target='_blank'>https://arxiv.org/pdf/2405.01616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Korablyov, Cheng-Hao Liu, Moksh Jain, Almer M. van der Sloot, Eric Jolicoeur, Edward Ruediger, Andrei Cristian Nica, Emmanuel Bengio, Kostiantyn Lapchevskyi, Daniel St-Cyr, Doris Alexandra Schuetz, Victor Ion Butoi, Jarrid Rector-Brooks, Simon Blackburn, Leo Feng, Hadi Nekoei, SaiKrishna Gottipati, Priyesh Vijayan, Prateek Gupta, Ladislav RampÃ¡Å¡ek, Sasikanth Avancha, Pierre-Luc Bacon, William L. Hamilton, Brooks Paige, Sanchit Misra, Stanislaw Kamil Jastrzebski, Bharat Kaul, Doina Precup, JosÃ© Miguel HernÃ¡ndez-Lobato, Marwin Segler, Michael Bronstein, Anne Marinier, Mike Tyers, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01616">Generative Active Learning for the Search of Small-molecule Protein Binders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in machine learning for scientific discovery in recent years, truly de novo design of small molecules which exhibit a property of interest remains a significant challenge. We introduce LambdaZero, a generative active learning approach to search for synthesizable molecules. Powered by deep reinforcement learning, LambdaZero learns to search over the vast space of molecules to discover candidates with a desired property. We apply LambdaZero with molecular docking to design novel small molecules that inhibit the enzyme soluble Epoxide Hydrolase 2 (sEH), while enforcing constraints on synthesizability and drug-likeliness. LambdaZero provides an exponential speedup in terms of the number of calls to the expensive molecular docking oracle, and LambdaZero de novo designed molecules reach docking scores that would otherwise require the virtual screening of a hundred billion molecules. Importantly, LambdaZero discovers novel scaffolds of synthesizable, drug-like inhibitors for sEH. In in vitro experimental validation, a series of ligands from a generated quinazoline-based scaffold were synthesized, and the lead inhibitor N-(4,6-di(pyrrolidin-1-yl)quinazolin-2-yl)-N-methylbenzamide (UM0152893) displayed sub-micromolar enzyme inhibition of sEH.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2404.10094.pdf' target='_blank'>https://arxiv.org/pdf/2404.10094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MichaÅ Koziarski, Mohammed Abukalam, Vedant Shah, Louis Vaillancourt, Doris Alexandra Schuetz, Moksh Jain, Almer van der Sloot, Mathieu Bourgey, Anne Marinier, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10094">Towards DNA-Encoded Library Generation with GFlowNets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA-encoded libraries (DELs) are a powerful approach for rapidly screening large numbers of diverse compounds. One of the key challenges in using DELs is library design, which involves choosing the building blocks that will be combinatorially combined to produce the final library. In this paper we consider the task of protein-protein interaction (PPI) biased DEL design. To this end, we evaluate several machine learning algorithms on the PPI modulation task and use them as a reward for the proposed GFlowNet-based generative approach. We additionally investigate the possibility of using structural information about building blocks to design a hierarchical action space for the GFlowNet. The observed results indicate that GFlowNets are a promising approach for generating diverse combinatorial library candidates.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2403.07013.pdf' target='_blank'>https://arxiv.org/pdf/2403.07013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Ling, Wenjie Du, Sizhe Liu, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07013">AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional mutual information (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extensive experiments demonstrate AdaNovo's state-of-the-art performance on a 9-species benchmark, where the peptides in the training set are almost completely disjoint from the peptides of the test sets. Moreover, AdaNovo excels in identifying amino acids with PTMs and exhibits robustness against data noise. The supplementary materials contain the official code.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2403.00875.pdf' target='_blank'>https://arxiv.org/pdf/2403.00875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Sun, Lirong Wu, Haitao Lin, Yufei Huang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00875">Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmentation is an effective alternative to utilize the small amount of labeled protein data. However, most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied data augmentation for proteins. This paper extends data augmentation techniques previously used for images and texts to proteins and then benchmarks these techniques on a variety of protein-related tasks, providing the first comprehensive evaluation of protein augmentation. Furthermore, we propose two novel semantic-level protein augmentation methods, namely Integrated Gradients Substitution and Back Translation Substitution, which enable protein semantic-aware augmentation through saliency detection and biological knowledge. Finally, we integrate extended and proposed augmentations into an augmentation pool and propose a simple but effective framework, namely Automated Protein Augmentation (APA), which can adaptively select the most suitable augmentation combinations for different tasks. Extensive experiments have shown that APA enhances the performance of five protein related tasks by an average of 10.55% across three architectures compared to vanilla implementations without augmentation, highlighting its potential to make a great impact on the field.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2401.06173.pdf' target='_blank'>https://arxiv.org/pdf/2401.06173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Qiu, Hui Yuan, Jinghong Zhang, Wentao Chen, Huazheng Wang, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06173">Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the combination of local search and bandit learning method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pre-trained embeddings, and bandit techniques. We test various instances of the algorithm across benchmark protein datasets using simulated screens. Experiment results demonstrate that the algorithm is both sample-efficient and able to find top designs using reasonably small mutation counts.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2401.02713.pdf' target='_blank'>https://arxiv.org/pdf/2401.02713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Wang, Zelin Zang, Jiangbin Zheng, Jun Xia, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02713">Graph-level Protein Representation Learning by Structure Knowledge Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on learning representation on the whole graph level in an unsupervised manner. Learning graph-level representation plays an important role in a variety of real-world issues such as molecule property prediction, protein structure feature extraction, and social network analysis. The mainstream method is utilizing contrastive learning to facilitate graph feature extraction, known as Graph Contrastive Learning (GCL). GCL, although effective, suffers from some complications in contrastive learning, such as the effect of false negative pairs. Moreover, augmentation strategies in GCL are weakly adaptive to diverse graph datasets. Motivated by these problems, we propose a novel framework called Structure Knowledge Refinement (SKR) which uses data structure to determine the probability of whether a pair is positive or negative. Meanwhile, we propose an augmentation strategy that naturally preserves the semantic meaning of the original data and is compatible with our SKR framework. Furthermore, we illustrate the effectiveness of our SKR framework through intuition and experiments. The experimental results on the tasks of graph-level classification demonstrate that our SKR framework is superior to most state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2405.08197.pdf' target='_blank'>https://arxiv.org/pdf/2405.08197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Wang, Yu Mao, Yufei Cui, Nan Guan, Chun Jason Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08197">IHC Matters: Incorporating IHC analysis to H&E Whole Slide Image Analysis for Improved Cancer Grading via Two-stage Multimodal Bilinear Pooling Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Immunohistochemistry (IHC) plays a crucial role in pathology as it detects the over-expression of protein in tissue samples. However, there are still fewer machine learning model studies on IHC's impact on accurate cancer grading. We discovered that IHC and H\&E possess distinct advantages and disadvantages while possessing certain complementary qualities. Building on this observation, we developed a two-stage multi-modal bilinear model with a feature pooling module. This model aims to maximize the potential of both IHC and HE's feature representation, resulting in improved performance compared to their individual use. Our experiments demonstrate that incorporating IHC data into machine learning models, alongside H\&E stained images, leads to superior predictive results for cancer grading. The proposed framework achieves an impressive ACC higher of 0.953 on the public dataset BCI.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2405.02299.pdf' target='_blank'>https://arxiv.org/pdf/2405.02299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Gao, Tao Feng, Jiaxuan You, Chenyi Zi, Yan Zhou, Chen Zhang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02299">Deep Reinforcement Learning for Modelling Protein Complexes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AlphaFold can be used for both single-chain and multi-chain protein structure prediction, while the latter becomes extremely challenging as the number of chains increases. In this work, by taking each chain as a node and assembly actions as edges, we show that an acyclic undirected connected graph can be used to predict the structure of multi-chain protein complexes (a.k.a., protein complex modelling, PCM). However, there are still two challenges: 1) The huge combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for the PCM problem can easily lead to high computational cost. 2) The scales of protein complexes exhibit distribution shift due to variance in chain numbers, which calls for the generalization in modelling complexes of various scales. To address these challenges, we propose GAPN, a Generative Adversarial Policy Network powered by domain-specific rewards and adversarial loss through policy gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently search through the immense assembly space and optimize the direct docking reward through policy gradient. Importantly, we design an adversarial reward function to enhance the receptive field of our model. In this way, GAPN will simultaneously focus on a specific batch of complexes and the global assembly rules learned from complexes with varied chain numbers. Empirically, we have achieved both significant accuracy (measured by RMSD and TM-Score) and efficiency improvements compared to leading PCM softwares.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2509.18153.pdf' target='_blank'>https://arxiv.org/pdf/2509.18153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqun Cao, Marcelo D. T. Torres, Jingjie Zhang, Zijun Gao, Fang Wu, Chunbin Gu, Jure Leskovec, Yejin Choi, Cesar de la Fuente-Nunez, Guangyong Chen, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18153">A deep reinforcement learning platform for antibiotic discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths annually by 2050, underscoring the urgent need for new antibiotics. Here we present ApexAmphion, a deep-learning framework for de novo design of antibiotics that couples a 6.4-billion-parameter protein language model with reinforcement learning. The model is first fine-tuned on curated peptide data to capture antimicrobial sequence regularities, then optimised with proximal policy optimization against a composite reward that combines predictions from a learned minimum inhibitory concentration (MIC) classifier with differentiable physicochemical objectives. In vitro evaluation of 100 designed peptides showed low MIC values (nanomolar range in some cases) for all candidates (100% hit rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial activity against at least two clinically relevant bacteria. The lead molecules killed bacteria primarily by potently targeting the cytoplasmic membrane. By unifying generation, scoring and multi-objective optimization with deep reinforcement learning in a single pipeline, our approach rapidly produces diverse, potent candidates, offering a scalable route to peptide antibiotics and a platform for iterative steering toward potency and developability within hours.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2502.07527.pdf' target='_blank'>https://arxiv.org/pdf/2502.07527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Ran Bi, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07527">Nature Language Model: Deciphering the Language of Nature for Scientific Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2410.12126.pdf' target='_blank'>https://arxiv.org/pdf/2410.12126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongqi Fu, Liri Fang, Zihao Li, Hanghang Tong, Vetle I. Torvik, Jingrui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12126">What Do LLMs Need to Understand Graphs: A Survey of Parametric Representation of Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphs, as a relational data structure, have been widely used for various application scenarios, like molecule design and recommender systems. Recently, large language models (LLMs) are reorganizing in the AI community for their expected reasoning and inference abilities. Making LLMs understand graph-based relational data has great potential, including but not limited to (1) distillate external knowledge base for eliminating hallucination and breaking the context window limit for LLMs' inference during the retrieval augmentation generation process; (2) taking graph data as the input and directly solve the graph-based research tasks like protein design and drug discovery. However, inputting the entire graph data to LLMs is not practical due to its complex topological structure, data size, and the lack of effective and efficient semantic graph representations. A natural question arises: Is there a kind of graph representation that can be described by natural language for LLM's understanding and is also easy to require to serve as the raw input for LLMs? Based on statistical computation, graph laws pre-define a set of parameters (e.g., degree, time, diameter) and identifie their relationships and values by observing the topological distribution of plenty of real-world graph data. We believe this kind of parametric representation of graphs, graph laws, can be a solution for making LLMs understand graph data as the input. In this survey, we first review the previous study of graph laws from multiple perspectives, i.e., macroscope and microscope of graphs, low-order and high-order graphs, static and dynamic graphs, different observation spaces, and newly proposed graph parameters. After we review various real-world applications benefiting from the guidance of graph laws, we conclude the paper with current challenges and future research directions.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2507.20280.pdf' target='_blank'>https://arxiv.org/pdf/2507.20280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20280">SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2507.10923.pdf' target='_blank'>https://arxiv.org/pdf/2507.10923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Wang, Keyan Ding, Kehua Feng, Zeyuan Wang, Ming Qin, Xiaotong Li, Qiang Zhang, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10923">Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models have emerged as powerful tools for sequence generation, offering substantial advantages in functional optimization and denovo design. However, these models also present significant risks of generating harmful protein sequences, such as those that enhance viral transmissibility or evade immune responses. These concerns underscore critical biosafety and ethical challenges. To address these issues, we propose a Knowledge-guided Preference Optimization (KPO) framework that integrates prior knowledge via a Protein Safety Knowledge Graph. This framework utilizes an efficient graph pruning strategy to identify preferred sequences and employs reinforcement learning to minimize the risk of generating harmful proteins. Experimental results demonstrate that KPO effectively reduces the likelihood of producing hazardous sequences while maintaining high functionality, offering a robust safety assurance framework for applying generative models in biotechnology.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2504.10612.pdf' target='_blank'>https://arxiv.org/pdf/2504.10612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Balcerak, Tamaz Amiranashvili, Antonio Terpin, Suprosanna Shit, Lea Bogensperger, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10612">Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most widely used generative models map noise and data distributions by matching flows or scores. However, they struggle to incorporate partial observations and additional priors--something energy-based models (EBMs) handle elegantly by simply adding corresponding scalar energy terms. We address this issue by proposing Energy Matching, a framework that endows flow-based approaches with the flexibility of EBMs. Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in terms of fidelity, while retaining simulation-free training of transport-based approaches away from the data manifold. Furthermore, we leverage the method's flexibility to introduce an interaction energy that supports diverse mode exploration, which we demonstrate in a controlled protein-generation setting. Our approach focuses on learning a scalar potential energy--without time-conditioning, auxiliary generators, or additional networks--which marks a significant departure from recent EBM methods. We believe that this simplified framework significantly advances EBMs capabilities and paves the way for their wider adoption in generative modeling across diverse domains.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2503.14574.pdf' target='_blank'>https://arxiv.org/pdf/2503.14574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taslim Murad, Sarwan Ali, Murray Patterson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14574">Sequence Analysis Using the Bezier Curve</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of sequences (e.g., protein, DNA, and SMILES string) is essential for disease diagnosis, biomaterial engineering, genetic engineering, and drug discovery domains. Conventional analytical methods focus on transforming sequences into numerical representations for applying machine learning/deep learning-based sequence characterization. However, their efficacy is constrained by the intrinsic nature of deep learning (DL) models, which tend to exhibit suboptimal performance when applied to tabular data. An alternative group of methodologies endeavors to convert biological sequences into image forms by applying the concept of Chaos Game Representation (CGR). However, a noteworthy drawback of these methods lies in their tendency to map individual elements of the sequence onto a relatively small subset of designated pixels within the generated image. The resulting sparse image representation may not adequately encapsulate the comprehensive sequence information, potentially resulting in suboptimal predictions. In this study, we introduce a novel approach to transform sequences into images using the BÃ©zier curve concept for element mapping. Mapping the elements onto a curve enhances the sequence information representation in the respective images, hence yielding better DL-based classification performance. We employed different sequence datasets to validate our system by using different classification tasks, and the results illustrate that our BÃ©zier curve method is able to achieve good performance for all the tasks.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2501.14746.pdf' target='_blank'>https://arxiv.org/pdf/2501.14746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14746">Neuromorphic Spiking Neural Network Based Classification of COVID-19 Spike Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) virus data post-COVID has reached exponentially to an enormous magnitude, opening research doors to analyze its behavior. Various studies are conducted by researchers to gain a deeper understanding of the virus, like genomic surveillance, etc, so that efficient prevention mechanisms can be developed. However, the unstable nature of the virus (rapid mutations, multiple hosts, etc) creates challenges in designing analytical systems for it. Therefore, we propose a neural network-based (NN) mechanism to perform an efficient analysis of the SARS-CoV-2 data, as NN portrays generalized behavior upon training. Moreover, rather than using the full-length genome of the virus, we apply our method to its spike region, as this region is known to have predominant mutations and is used to attach to the host cell membrane. In this paper, we introduce a pipeline that first converts the spike protein sequences into a fixed-length numerical representation and then uses Neuromorphic Spiking Neural Network to classify those sequences. We compare the performance of our method with various baselines using real-world SARS-CoV-2 spike sequence data and show that our method is able to achieve higher predictive accuracy compared to the recent baselines.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2410.12655.pdf' target='_blank'>https://arxiv.org/pdf/2410.12655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarwan Ali, Taslim Murad, Prakash Chourasia, Haris Mansoor, Imdad Ullah Khan, Pin-Yu Chen, Murray Patterson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12655">Position Specific Scoring Is All You Need? Revisiting Protein Sequence Classification Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the structural and functional characteristics of proteins are crucial for developing preventative and curative strategies that impact fields from drug discovery to policy development. An important and popular technique for examining how amino acids make up these characteristics of the protein sequences with position-specific scoring (PSS). While the string kernel is crucial in natural language processing (NLP), it is unclear if string kernels can extract biologically meaningful information from protein sequences, despite the fact that they have been shown to be effective in the general sequence analysis tasks. In this work, we propose a weighted PSS kernel matrix (or W-PSSKM), that combines a PSS representation of protein sequences, which encodes the frequency information of each amino acid in a sequence, with the notion of the string kernel. This results in a novel kernel function that outperforms many other approaches for protein sequence classification. We perform extensive experimentation to evaluate the proposed method. Our findings demonstrate that the W-PSSKM significantly outperforms existing baselines and state-of-the-art methods and achieves up to 45.1\% improvement in classification accuracy.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2410.03769.pdf' target='_blank'>https://arxiv.org/pdf/2410.03769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Li, Jingyu Lu, Chuangxin Chu, Tianyu Zeng, Yujia Zheng, Mei Li, Haotian Huang, Bin Wu, Zuoxian Liu, Kai Ma, Xuejing Yuan, Xingkai Wang, Keyan Ding, Huajun Chen, Qiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03769">SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have a transformative impact on a variety of scientific tasks across disciplines including biology, chemistry, medicine, and physics. However, ensuring the safety alignment of these models in scientific research remains an underexplored area, with existing benchmarks primarily focusing on textual content and overlooking key scientific representations such as molecular, protein, and genomic languages. Moreover, the safety mechanisms of LLMs in scientific tasks are insufficiently studied. To address these limitations, we introduce SciSafeEval, a comprehensive benchmark designed to evaluate the safety alignment of LLMs across a range of scientific tasks. SciSafeEval spans multiple scientific languages-including textual, molecular, protein, and genomic-and covers a wide range of scientific domains. We evaluate LLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a "jailbreak" enhancement feature that challenges LLMs equipped with safety guardrails, rigorously testing their defenses against malicious intention. Our benchmark surpasses existing safety datasets in both scale and scope, providing a robust platform for assessing the safety and performance of LLMs in scientific contexts. This work aims to facilitate the responsible development and deployment of LLMs, promoting alignment with safety and ethical standards in scientific research.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2409.06694.pdf' target='_blank'>https://arxiv.org/pdf/2409.06694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06694">DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer is a complex disease characterized by uncontrolled cell growth. T cell receptors (TCRs), crucial proteins in the immune system, play a key role in recognizing antigens, including those associated with cancer. Recent advancements in sequencing technologies have facilitated comprehensive profiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity and enabling TCR-based immunotherapies. However, analyzing these intricate biomolecules necessitates efficient representations that capture their structural and functional information. T-cell protein sequences pose unique challenges due to their relatively smaller lengths compared to other biomolecules. An image-based representation approach becomes a preferred choice for efficient embeddings, allowing for the preservation of essential details and enabling comprehensive analysis of T-cell protein sequences. In this paper, we propose to generate images from the protein sequences using the idea of Chaos Game Representation (CGR) using the Kaleidoscopic images approach. This Deep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images (called DANCE) provides a unique way to visualize protein sequences by recursively applying chaos game rules around a central seed point. we perform the classification of the T cell receptors (TCRs) protein sequences in terms of their respective target cancer cells, as TCRs are known for their immune response against cancer disease. The TCR sequences are converted into images using the DANCE method. We employ deep-learning vision models to perform the classification to obtain insights into the relationship between the visual patterns observed in the generated kaleidoscopic images and the underlying protein properties. By combining CGR-based image generation with deep learning classification, this study opens novel possibilities in the protein analysis domain.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2409.04922.pdf' target='_blank'>https://arxiv.org/pdf/2409.04922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarwan Ali, Prakash Chourasia, Bipin Koirala, Murray Patterson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04922">Nearest Neighbor CCP-Based Molecular Sequence Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular sequence analysis is crucial for comprehending several biological processes, including protein-protein interactions, functional annotation, and disease classification. The large number of sequences and the inherently complicated nature of protein structures make it challenging to analyze such data. Finding patterns and enhancing subsequent research requires the use of dimensionality reduction and feature selection approaches. Recently, a method called Correlated Clustering and Projection (CCP) has been proposed as an effective method for biological sequencing data. The CCP technique is still costly to compute even though it is effective for sequence visualization. Furthermore, its utility for classifying molecular sequences is still uncertain. To solve these two problems, we present a Nearest Neighbor Correlated Clustering and Projection (CCP-NN)-based technique for efficiently preprocessing molecular sequence data. To group related molecular sequences and produce representative supersequences, CCP makes use of sequence-to-sequence correlations. As opposed to conventional methods, CCP doesn't rely on matrix diagonalization, therefore it can be applied to a range of machine-learning problems. We estimate the density map and compute the correlation using a nearest-neighbor search technique. We performed molecular sequence classification using CCP and CCP-NN representations to assess the efficacy of our proposed approach. Our findings show that CCP-NN considerably improves classification task accuracy as well as significantly outperforms CCP in terms of computational runtime.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2511.09900.pdf' target='_blank'>https://arxiv.org/pdf/2511.09900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaodong Yang, Yang Wang, Jinpeng Li, Pei Guo, Da Han, Guangyong Chen, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09900">Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms focus on designing search strategies, they overlook how to utilize the transformative protein language models, which encode rich evolutionary patterns, to guide search. To bridge this gap, we propose AlphaDE, a novel framework to evolve protein sequences by harnessing the innovative paradigms of large language models. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. An interesting case study further shows that AlphaDE supports condensing the protein sequence space through computational evolution.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2510.17153.pdf' target='_blank'>https://arxiv.org/pdf/2510.17153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjin Choo, Fanchen Bu, Hyunjin Hwang, Young-Gyu Yoon, Kijung Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17153">HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Higher-order interactions (HOIs) in complex systems, such as scientific collaborations, multi-protein complexes, and multi-user communications, are commonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes) represents an HOI among the nodes. Given a hypergraph, hyperedge prediction aims to identify hyperedges that are either missing or likely to form in the future, and it has broad applications, including recommending interest-based social groups, predicting collaborations, and uncovering functional complexes in biological systems. However, the vast search space of hyperedge candidates (i.e., all possible subsets of nodes) poses a significant computational challenge, making naive exhaustive search infeasible. As a result, existing approaches rely on either heuristic sampling to obtain constrained candidate sets or ungrounded assumptions on hypergraph structure to select promising hyperedges. In this work, we propose HyperSearch, a search-based algorithm for hyperedge prediction that efficiently evaluates unconstrained candidate sets, by incorporating two key components: (1) an empirically grounded scoring function derived from observations in real-world hypergraphs and (2) an efficient search mechanism, where we derive and use an anti-monotonic upper bound of the original scoring function (which is not antimonotonic) to prune the search space. This pruning comes with theoretical guarantees, ensuring that discarded candidates are never better than the kept ones w.r.t. the original scoring function. In extensive experiments on 10 real-world hypergraphs across five domains, HyperSearch consistently outperforms state-of-the-art baselines, achieving higher accuracy in predicting new (i.e., not in the training set) hyperedges.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2510.03370.pdf' target='_blank'>https://arxiv.org/pdf/2510.03370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junde Xu, Yapin Shi, Lijun Lang, Taoyong Cui, Zhiming Zhang, Guangyong Chen, Jiezhong Qiu, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03370">InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal protein language models deliver strong performance on mutation-effect prediction, but training such models from scratch demands substantial computational resources. In this paper, we propose a fine-tuning framework called InstructPLM-mu and try to answer a question: \textit{Can multimodal fine-tuning of a pretrained, sequence-only protein language model match the performance of models trained end-to-end? } Surprisingly, our experiments show that fine-tuning ESM2 with structural inputs can reach performance comparable to ESM3. To understand how this is achieved, we systematically compare three different feature-fusion designs and fine-tuning recipes. Our results reveal that both the fusion method and the tuning strategy strongly affect final accuracy, indicating that the fine-tuning process is not trivial. We hope this work offers practical guidance for injecting structure into pretrained protein language models and motivates further research on better fusion mechanisms and fine-tuning protocols.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2507.07048.pdf' target='_blank'>https://arxiv.org/pdf/2507.07048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce Coburn, Jiangpeng He, Megan E. Rollo, Satvinder S. Dhaliwal, Deborah A. Kerr, Fengqing Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07048">Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Multimodal Models (LMMs) are increasingly applied to meal images for nutrition analysis. However, existing work primarily evaluates proprietary models, such as GPT-4. This leaves the broad range of LLMs underexplored. Additionally, the influence of integrating contextual metadata and its interaction with various reasoning modifiers remains largely uncharted. This work investigates how interpreting contextual metadata derived from GPS coordinates (converted to location/venue type), timestamps (transformed into meal/day type), and the food items present can enhance LMM performance in estimating key nutritional values. These values include calories, macronutrients (protein, carbohydrates, fat), and portion sizes. We also introduce ACETADA, a new food-image dataset slated for public release. This open dataset provides nutrition information verified by the dietitian and serves as the foundation for our analysis. Our evaluation across eight LMMs (four open-weight and four closed-weight) first establishes the benefit of contextual metadata integration over straightforward prompting with images alone. We then demonstrate how this incorporation of contextual information enhances the efficacy of reasoning modifiers, such as Chain-of-Thought, Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona. Empirical results show that integrating metadata intelligently, when applied through straightforward prompting strategies, can significantly reduce the Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted nutritional values. This work highlights the potential of context-aware LMMs for improved nutrition analysis.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2506.03028.pdf' target='_blank'>https://arxiv.org/pdf/2506.03028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junde Xu, Zijun Gao, Xinyi Zhou, Jie Hu, Xingyi Cheng, Le Song, Guangyong Chen, Pheng-Ann Heng, Jiezhong Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03028">Protein Inverse Folding From Structure Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inverse folding problem, aiming to design amino acid sequences that fold into desired three-dimensional structures, is pivotal for various biotechnological applications. Here, we introduce a novel approach leveraging Direct Preference Optimization (DPO) to fine-tune an inverse folding model using feedback from a protein folding model. Given a target protein structure, we begin by sampling candidate sequences from the inverse-folding model, then predict the three-dimensional structure of each sequence with the folding model to generate pairwise structural-preference labels. These labels are used to fine-tune the inverse-folding model under the DPO objective. Our results on the CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence recovery of baseline models but also leads to a significant improvement in average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity. Furthermore, iterative application of our DPO-based method on challenging protein structures yields substantial gains, with an average TM-Score increase of 79.5\% with regard to the baseline model. This work establishes a promising direction for enhancing protein sequence design ability from structure feedback by effectively utilizing preference optimization.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2505.21928.pdf' target='_blank'>https://arxiv.org/pdf/2505.21928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianghui Zhu, Xitong Ling, Minxi Ouyang, Xiaoping Liu, Tian Guan, Mingxi Fu, Zhiqiang Cheng, Fanglei Fu, Maomao Zeng, Liming Liu, Song Duan, Qiang Huang, Ying Xiao, Jianming Li, Shanming Lu, Zhenghua Piao, Mingxi Zhu, Yibo Jin, Shan Xu, Qiming He, Yizhi Wang, Junru Cheng, Xuanyu Wang, Luxi Xie, Houqiang Li, Sufang Tian, Yonghong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21928">Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gastrointestinal (GI) diseases represent a clinically significant burden, necessitating precise diagnostic approaches to optimize patient outcomes. Conventional histopathological diagnosis suffers from limited reproducibility and diagnostic variability. To overcome these limitations, we develop Digepath, a specialized foundation model for GI pathology. Our framework introduces a dual-phase iterative optimization strategy combining pretraining with fine-screening, specifically designed to address the detection of sparsely distributed lesion areas in whole-slide images. Digepath is pretrained on over 353 million multi-scale images from 210,043 H&E-stained slides of GI diseases. It attains state-of-the-art performance on 33 out of 34 tasks related to GI pathology, including pathological diagnosis, protein expression status prediction, gene mutation prediction, and prognosis evaluation. We further translate the intelligent screening module for early GI cancer and achieve near-perfect 99.70% sensitivity across nine independent medical institutions. This work not only advances AI-driven precision pathology for GI diseases but also bridge critical gaps in histopathological practice.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2502.16533.pdf' target='_blank'>https://arxiv.org/pdf/2502.16533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16533">A Survey of Graph Transformers: Architectures, Theories and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2502.00831.pdf' target='_blank'>https://arxiv.org/pdf/2502.00831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maike Scherer, Lukas Brand, Louis Wolf, Teena tom Dieck, Maximilian SchÃ¤fer, Sebastian Lotter, Andreas Burkovski, Heinrich Sticht, Robert Schober, Kathrin Castiglione
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00831">Closed-Loop Long-Term Experimental Molecular Communication System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a fluid-based experimental molecular communication (MC) testbed which uses media modulation. Motivated by the natural human cardiovascular system, the testbed operates in a closed-loop tube system. The proposed system is designed to be biocompatible, resource-efficient, and controllable from outside the tube. As signaling molecule, the testbed employs the green fluorescent protein variant "Dreiklang" (GFPD). GFPDs can be reversibly switched via light of different wavelengths between a bright fluorescent state and a less fluorescent state. GFPDs in solution are filled into the testbed prior to the start of information transmission and remain there for an entire experiment. For information transmission, an optical transmitter (TX) and an optical eraser (EX), which are located outside the tube, are used to write and erase the information encoded in the state of the GFPDs, respectively. At the receiver (RX), the state of the GFPDs is read out by fluorescence detection. In our testbed, due to the closed-loop setup, we observe new forms of inter-symbol interferences (ISI), which do not occur in short experiments and open-loop systems. For the testbed, we developed a communication scheme, which includes blind transmission start detection, symbol-by-symbol synchronization, and adaptive threshold detection. We comprehensively analyze our MC experiments using different performance metrics. Moreover, we experimentally demonstrate the error-free transmission of 5370 bit at a data rate of 36 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$ using 8-ary modulation and the error-free binary transmission of around 90000 bit at a data rate of 12 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$. For the latter experiment, data was transmitted for a period of 125 hours. All signals recorded and parts of the evaluation code are publicly available on Zenodo and Github, respectively.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2505.17478.pdf' target='_blank'>https://arxiv.org/pdf/2505.17478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuning Shen, Lihao Wang, Huizhuo Yuan, Yan Wang, Bangji Yang, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17478">Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2409.02588.pdf' target='_blank'>https://arxiv.org/pdf/2409.02588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Quadir, M. Sajid, M. Tanveer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02588">Multiview Random Vector Functional Link Network for Predicting DNA-Binding Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The identification of DNA-binding proteins (DBPs) is a critical task due to their significant impact on various biological activities. Understanding the mechanisms underlying protein-DNA interactions is essential for elucidating various life activities. In recent years, machine learning-based models have been prominently utilized for DBP prediction. In this paper, to predict DBPs, we propose a novel framework termed a multiview random vector functional link (MvRVFL) network, which fuses neural network architecture with multiview learning. The proposed MvRVFL model combines the benefits of late and early fusion, allowing for distinct regularization parameters across different views while leveraging a closed-form solution to determine unknown parameters efficiently. The primal objective function incorporates a coupling term aimed at minimizing a composite of errors stemming from all views. From each of the three protein views of the DBP datasets, we extract five features. These features are then fused together by incorporating a hidden feature during the model training process. The performance of the proposed MvRVFL model on the DBP dataset surpasses that of baseline models, demonstrating its superior effectiveness. Furthermore, we extend our assessment to the UCI, KEEL, AwA, and Corel5k datasets, to establish the practicality of the proposed models. The consistency error bound, the generalization error bound, and empirical findings, coupled with rigorous statistical analyses, confirm the superior generalization capabilities of the MvRVFL model compared to the baseline models.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2403.14088.pdf' target='_blank'>https://arxiv.org/pdf/2403.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14088">Protein Conformation Generation via Force-Guided SE(3) Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2503.17286.pdf' target='_blank'>https://arxiv.org/pdf/2503.17286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsu Kim, Jiayao Gu, Ye Yuan, Taeyoung Yun, Zixuan Liu, Yoshua Bengio, Can Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17286">Offline Model-Based Optimization: Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline optimization is a fundamental challenge in science and engineering, where the goal is to optimize black-box functions using only offline datasets. This setting is particularly relevant when querying the objective function is prohibitively expensive or infeasible, with applications spanning protein engineering, material discovery, neural architecture search, and beyond. The main difficulty lies in accurately estimating the objective landscape beyond the available data, where extrapolations are fraught with significant epistemic uncertainty. This uncertainty can lead to objective hacking(reward hacking), exploiting model inaccuracies in unseen regions, or other spurious optimizations that yield misleadingly high performance estimates outside the training distribution. Recent advances in model-based optimization(MBO) have harnessed the generalization capabilities of deep neural networks to develop offline-specific surrogate and generative models. Trained with carefully designed strategies, these models are more robust against out-of-distribution issues, facilitating the discovery of improved designs. Despite its growing impact in accelerating scientific discovery, the field lacks a comprehensive review. To bridge this gap, we present the first thorough review of offline MBO. We begin by formalizing the problem for both single-objective and multi-objective settings and by reviewing recent benchmarks and evaluation metrics. We then categorize existing approaches into two key areas: surrogate modeling, which emphasizes accurate function approximation in out-of-distribution regions, and generative modeling, which explores high-dimensional design spaces to identify high-performing designs. Finally, we examine the key challenges and propose promising directions for advancement in this rapidly evolving field including safe control of superintelligent systems.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2410.11224.pdf' target='_blank'>https://arxiv.org/pdf/2410.11224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxian Yan, Zaixi Zhang, Jintao Zhu, Kai Zhang, Jianfeng Pei, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11224">DeltaDock: A Unified Framework for Accurate, Efficient, and Physically Reliable Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking, a technique for predicting ligand binding poses, is crucial in structure-based drug design for understanding protein-ligand interactions. Recent advancements in docking methods, particularly those leveraging geometric deep learning (GDL), have demonstrated significant efficiency and accuracy advantages over traditional sampling methods. Despite these advancements, current methods are often tailored for specific docking settings, and limitations such as the neglect of protein side-chain structures, difficulties in handling large binding pockets, and challenges in predicting physically valid structures exist. To accommodate various docking settings and achieve accurate, efficient, and physically reliable docking, we propose a novel two-stage docking framework, DeltaDock, consisting of pocket prediction and site-specific docking. We innovatively reframe the pocket prediction task as a pocket-ligand alignment problem rather than direct prediction in the first stage. Then we follow a bi-level coarse-to-fine iterative refinement process to perform site-specific docking. Comprehensive experiments demonstrate the superior performance of DeltaDock. Notably, in the blind docking setting, DeltaDock achieves a 31\% relative improvement over the docking success rate compared with the previous state-of-the-art GDL model. With the consideration of physical validity, this improvement increases to about 300\%.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2512.08508.pdf' target='_blank'>https://arxiv.org/pdf/2512.08508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengmo Zhou, Feng Yu, Wenda Wang, Zhifeng Gao, Guolin Ke, Zhewei Wei, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08508">Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2510.04126.pdf' target='_blank'>https://arxiv.org/pdf/2510.04126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziying Zhang, Yaqing Wang, Yuxuan Sun, Min Ye, Quanming Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04126">Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cold-start drug-target interaction (DTI) prediction focuses on interaction between novel drugs and proteins. Previous methods typically learn transferable interaction patterns between structures of drug and proteins to tackle it. However, insight from proteomics suggest that protein have multi-level structures and they all influence the DTI. Existing works usually represent protein with only primary structures, limiting their ability to capture interactions involving higher-level structures. Inspired by this insight, we propose ColdDTI, a framework attending on protein multi-level structure for cold-start DTI prediction. We employ hierarchical attention mechanism to mine interaction between multi-level protein structures (from primary to quaternary) and drug structures at both local and global granularities. Then, we leverage mined interactions to fuse structure representations of different levels for final prediction. Our design captures biologically transferable priors, avoiding the risk of overfitting caused by excessive reliance on representation learning. Experiments on benchmark datasets demonstrate that ColdDTI consistently outperforms previous methods in cold-start settings.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2409.07462.pdf' target='_blank'>https://arxiv.org/pdf/2409.07462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengmo Zhou, Zhen Wang, Feng Yu, Guolin Ke, Zhewei Wei, Zhifeng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07462">S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive Molecule Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual Screening is an essential technique in the early phases of drug discovery, aimed at identifying promising drug candidates from vast molecular libraries. Recently, ligand-based virtual screening has garnered significant attention due to its efficacy in conducting extensive database screenings without relying on specific protein-binding site information. Obtaining binding affinity data for complexes is highly expensive, resulting in a limited amount of available data that covers a relatively small chemical space. Moreover, these datasets contain a significant amount of inconsistent noise. It is challenging to identify an inductive bias that consistently maintains the integrity of molecular activity during data augmentation. To tackle these challenges, we propose S-MolSearch, the first framework to our knowledge, that leverages molecular 3D information and affinity information in semi-supervised contrastive learning for ligand-based virtual screening. Drawing on the principles of inverse optimal transport, S-MolSearch efficiently processes both labeled and unlabeled data, training molecular structural encoders while generating soft labels for the unlabeled data. This design allows S-MolSearch to adaptively utilize unlabeled data within the learning process. Empirically, S-MolSearch demonstrates superior performance on widely-used benchmarks LIT-PCBA and DUD-E. It surpasses both structure-based and ligand-based virtual screening methods for AUROC, BEDROC and EF.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2510.10480.pdf' target='_blank'>https://arxiv.org/pdf/2510.10480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zishen Zhang, Xiangzhe Kong, Wenbing Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10480">Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2507.10955.pdf' target='_blank'>https://arxiv.org/pdf/2507.10955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-en Amy Tai, Alexander Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10955">Diffusion Decoding for Peptide De Novo Sequencing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptide de novo sequencing is a method used to reconstruct amino acid sequences from tandem mass spectrometry data without relying on existing protein sequence databases. Traditional deep learning approaches, such as Casanovo, mainly utilize autoregressive decoders and predict amino acids sequentially. Subsequently, they encounter cascading errors and fail to leverage high-confidence regions effectively. To address these issues, this paper investigates using diffusion decoders adapted for the discrete data domain. These decoders provide a different approach, allowing sequence generation to start from any peptide segment, thereby enhancing prediction accuracy. We experiment with three different diffusion decoder designs, knapsack beam search, and various loss functions. We find knapsack beam search did not improve performance metrics and simply replacing the transformer decoder with a diffusion decoder lowered performance. Although peptide precision and recall were still 0, the best diffusion decoder design with the DINOISER loss function obtained a statistically significant improvement in amino acid recall by 0.373 compared to the baseline autoregressive decoder-based Casanovo model. These findings highlight the potential of diffusion decoders to not only enhance model sensitivity but also drive significant advancements in peptide de novo sequencing.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2409.09828.pdf' target='_blank'>https://arxiv.org/pdf/2409.09828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09828">Latent Diffusion Models for Controllable RNA Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents RNAdiffusion, a latent diffusion model for generating and optimizing discrete RNA sequences of variable lengths. RNA is a key intermediary between DNA and protein, exhibiting high sequence diversity and complex three-dimensional structures to support a wide range of functions. We utilize pretrained BERT-type models to encode raw RNA sequences into token-level, biologically meaningful representations. A Query Transformer is employed to compress such representations into a set of fixed-length latent vectors, with an autoregressive decoder trained to reconstruct RNA sequences from these latent variables. We then develop a continuous diffusion model within this latent space. To enable optimization, we integrate the gradients of reward models--surrogates for RNA functional properties--into the backward diffusion process, thereby generating RNAs with high reward scores. Empirical results confirm that RNAdiffusion generates non-coding RNAs that align with natural distributions across various biological metrics. Further, we fine-tune the diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize sequences for high translation efficiencies. Our guided diffusion model effectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and Translation Efficiency (TE), outperforming baselines in balancing rewards and structural stability trade-off. Our findings hold potential for advancing RNA sequence-function research and therapeutic RNA design.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2405.07814.pdf' target='_blank'>https://arxiv.org/pdf/2405.07814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Keller, Chi-en Amy Tai, Yuhao Chen, Pengcheng Xi, Alexander Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07814">NutritionVerse-Direct: Exploring Deep Neural Networks for Multitask Nutrition Prediction from Food Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications. Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process. Recent work has explored using computer vision prediction systems to predict nutritional information from food images. Still, these methods are often tailored to specific situations, require other inputs in addition to a food image, or do not provide comprehensive nutritional information.
  This paper aims to enhance the efficacy of dietary intake estimation by leveraging various neural network architectures to directly predict a meal's nutritional content from its image. Through comprehensive experimentation and evaluation, we present NutritionVerse-Direct, a model utilizing a vision transformer base architecture with three fully connected layers that lead to five regression heads predicting calories (kcal), mass (g), protein (g), fat (g), and carbohydrates (g) present in a meal. NutritionVerse-Direct yields a combined mean average error score on the NutritionVerse-Real dataset of 412.6, an improvement of 25.5% over the Inception-ResNet model, demonstrating its potential for improving dietary intake estimation accuracy.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2402.12714.pdf' target='_blank'>https://arxiv.org/pdf/2402.12714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Jiao, Xiangzhe Kong, Li Zhang, Ziyang Yu, Fangyuan Ren, Wenjuan Tan, Wenbing Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12714">An Equivariant Pretrained Transformer for Unified 3D Molecular Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models in a specific domain, either proteins or small molecules, missing the opportunity to leverage cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), an all-atom foundation model that can be pretrained from multiple domain 3D molecules. Built upon an E(3)-equivariant transformer, EPT is able to not only process atom-level information but also incorporate block-level features (e.g. residuals in proteins). Additionally, we employ a block-level denoising task, rather than the conventional atom-level denoising, as the pretraining objective. To pretrain EPT, we construct a large-scale dataset of 5.89M entries, comprising small molecules, proteins, protein-protein complexes, and protein-molecule complexes. Experimental evaluations on downstream tasks including ligand binding affinity prediction, protein property prediction, and molecular property prediction, show that EPT significantly outperforms previous state-of-the-art methods in the first task and achieves competitively superior performance for the remaining two tasks. Furthermore, we demonstrate the potential of EPT in identifying small molecule drug candidates targeting 3CL protease, a critical target in the replication of SARS-CoV-2. Among 1,978 FDA-approved drugs, EPT ranks 7 out of 8 known anti-COVID-19 drugs in the top 200, indicating the high recall of EPT. By using Molecular Dynamics (MD) simulations, EPT further discoveries 7 novel compounds whose binding affinities are higher than that of the top-ranked known anti-COVID-19 drug, showcasing its powerful capabilities in drug discovery.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2401.08986.pdf' target='_blank'>https://arxiv.org/pdf/2401.08986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Yu, Wenbing Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08986">Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as DiffDock-PP and Multimer particularly for antibody-antigen docking.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2512.12932.pdf' target='_blank'>https://arxiv.org/pdf/2512.12932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wu, Jiyue Jiang, Xichen Ye, Yiqi Wang, Chang Zhou, Yitao Xu, Jiayang Chen, He Hu, Weizhong Zhang, Cheng Jin, Jiao Yuan, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12932">Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2507.18816.pdf' target='_blank'>https://arxiv.org/pdf/2507.18816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangwen Wang, Gaojie Jin, Xiaowei Huang, Ronghui Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18816">ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing mutations to optimize protein thermostability remains challenging due to the complex relationship between sequence variations, structural dynamics, and thermostability, often assessed by Î´Î´G
  (the change in free energy of unfolding). Existing methods rely on experimental random mutagenesis or prediction models tested with pre-defined datasets, using sequence-based heuristics and treating enzyme design as a one-step process without iterative refinement, which limits design space exploration and restricts discoveries beyond known variations. We present ThermoRL, a framework based on reinforcement learning (RL) that leverages graph neural networks (GNN) to design mutations with enhanced thermostability. It combines a pre-trained GNN-based encoder with a hierarchical Q-learning network and employs a surrogate model for reward feedback, guiding the RL agent on where (the position) and which (mutant amino acid) to apply for enhanced thermostability. Experimental results show that ThermoRL achieves higher or comparable rewards than baselines while maintaining computational efficiency. It filters out destabilizing mutations and identifies stabilizing mutations aligned with experimental data. Moreover, ThermoRL accurately detects key mutation sites in unseen proteins, highlighting its strong generalizability. This RL-guided approach powered by GNN embeddings offers a robust alternative to traditional protein mutation design.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2410.02143.pdf' target='_blank'>https://arxiv.org/pdf/2410.02143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Guo, Yuchen Zhu, Molei Tao, Yongxin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02143">Plug-and-Play Controllable Generation for Discrete Masked Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article makes discrete masked models for the generative modeling of discrete data controllable. The goal is to generate samples of a discrete random variable that adheres to a posterior distribution, satisfies specific constraints, or optimizes a reward function. This methodological development enables broad applications across downstream tasks such as class-specific image generation and protein design. Existing approaches for controllable generation of masked models typically rely on task-specific fine-tuning or additional modifications, which can be inefficient and resource-intensive. To overcome these limitations, we propose a novel plug-and-play framework based on importance sampling that bypasses the need for training a conditional score. Our framework is agnostic to the choice of control criteria, requires no gradient information, and is well-suited for tasks such as posterior sampling, Bayesian inverse problems, and constrained generation. We demonstrate the effectiveness of our approach through extensive experiments, showcasing its versatility across multiple domains, including protein design.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2510.03929.pdf' target='_blank'>https://arxiv.org/pdf/2510.03929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Campbell, Valentin De Bortoli, Jiaxin Shi, Arnaud Doucet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03929">Self-Speculative Masked Diffusions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled, however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. We reduce the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. We apply our method to GPT2 scale text modelling and protein sequences generation, finding that we can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2509.25509.pdf' target='_blank'>https://arxiv.org/pdf/2509.25509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Langzhou He, Junyou Zhu, Fangxin Wang, Junhua Liu, Haoyan Xu, Yue Zhao, Philip S. Yu, Qitian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25509">Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular foundation models are rapidly advancing scientific discovery, but their unreliability on out-of-distribution (OOD) samples severely limits their application in high-stakes domains such as drug discovery and protein design. A critical failure mode is chemical hallucination, where models make high-confidence yet entirely incorrect predictions for unknown molecules. To address this challenge, we introduce Molecular Preference-Aligned Instance Ranking (Mole-PAIR), a simple, plug-and-play module that can be flexibly integrated with existing foundation models to improve their reliability on OOD data through cost-effective post-training. Specifically, our method formulates the OOD detection problem as a preference optimization over the estimated OOD affinity between in-distribution (ID) and OOD samples, achieving this goal through a pairwise learning objective. We show that this objective essentially optimizes AUROC, which measures how consistently ID and OOD samples are ranked by the model. Extensive experiments across five real-world molecular datasets demonstrate that our approach significantly improves the OOD detection capabilities of existing molecular foundation models, achieving up to 45.8%, 43.9%, and 24.3% improvements in AUROC under distribution shifts of size, scaffold, and assay, respectively.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2508.15480.pdf' target='_blank'>https://arxiv.org/pdf/2508.15480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhui Wang, Wenyu Zhu, Bowen Gao, Xin Hong, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15480">Learning Protein-Ligand Binding in Hyperbolic Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-ligand binding prediction is central to virtual screening and affinity ranking, two fundamental tasks in drug discovery. While recent retrieval-based methods embed ligands and protein pockets into Euclidean space for similarity-based search, the geometry of Euclidean embeddings often fails to capture the hierarchical structure and fine-grained affinity variations intrinsic to molecular interactions. In this work, we propose HypSeek, a hyperbolic representation learning framework that embeds ligands, protein pockets, and sequences into Lorentz-model hyperbolic space. By leveraging the exponential geometry and negative curvature of hyperbolic space, HypSeek enables expressive, affinity-sensitive embeddings that can effectively model both global activity and subtle functional differences-particularly in challenging cases such as activity cliffs, where structurally similar ligands exhibit large affinity gaps. Our mode unifies virtual screening and affinity ranking in a single framework, introducing a protein-guided three-tower architecture to enhance representational structure. HypSeek improves early enrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and affinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%), demonstrating the benefits of hyperbolic geometry across both tasks and highlighting its potential as a powerful inductive bias for protein-ligand modeling.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2507.00445.pdf' target='_blank'>https://arxiv.org/pdf/2507.00445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00445">Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2506.05768.pdf' target='_blank'>https://arxiv.org/pdf/2506.05768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyu Zhu, Jianhui Wang, Bowen Gao, Yinjun Jia, Haichuan Tan, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05768">AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual screening (VS) is a critical component of modern drug discovery, yet most existing methods--whether physics-based or deep learning-based--are developed around holo protein structures with known ligand-bound pockets. Consequently, their performance degrades significantly on apo or predicted structures such as those from AlphaFold2, which are more representative of real-world early-stage drug discovery, where pocket information is often missing. In this paper, we introduce an alignment-and-aggregation framework to enable accurate virtual screening under structural uncertainty. Our method comprises two core components: (1) a tri-modal contrastive learning module that aligns representations of the ligand, the holo pocket, and cavities detected from structures, thereby enhancing robustness to pocket localization error; and (2) a cross-attention based adapter for dynamically aggregating candidate binding sites, enabling the model to learn from activity data even without precise pocket annotations. We evaluated our method on a newly curated benchmark of apo structures, where it significantly outperforms state-of-the-art methods in blind apo setting, improving the early enrichment factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong performance on holo structures. These results demonstrate the promise of our approach in advancing first-in-class drug discovery, particularly in scenarios lacking experimentally resolved protein-ligand complexes.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2410.04461.pdf' target='_blank'>https://arxiv.org/pdf/2410.04461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonah Kim, Minsu Kim, Taeyoung Yun, Sanghyeok Choi, Emmanuel Bengio, Alex HernÃ¡ndez-GarcÃ­a, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04461">Improved Off-policy Reinforcement Learning in Biological Sequence Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing biological sequences with desired properties is challenging due to vast search spaces and limited evaluation budgets. Although reinforcement learning methods use proxy models for rapid reward evaluation, insufficient training data can cause proxy misspecification on out-of-distribution inputs. To address this, we propose a novel off-policy search, $Î´$-Conservative Search, that enhances robustness by restricting policy exploration to reliable regions. Starting from high-score offline sequences, we inject noise by randomly masking tokens with probability $Î´$, then denoise them using our policy. We further adapt $Î´$ based on proxy uncertainty on each data point, aligning the level of conservativeness with model confidence. Experimental results show that our conservative search consistently enhances the off-policy training, outperforming existing machine learning methods in discovering high-score sequences across diverse tasks, including DNA, RNA, protein, and peptide design.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2409.19688.pdf' target='_blank'>https://arxiv.org/pdf/2409.19688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Zhou, Gang Chen, Bing Xue, Mengjie Zhang, Jeremy S. Rooney, Kirill Lagutin, Andrew MacKenzie, Keith C. Gordon, Daniel P. Killeen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19688">Machine Learning for Raman Spectroscopy-based Cyber-Marine Fish Biochemical Composition Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid and accurate detection of biochemical compositions in fish is a crucial real-world task that facilitates optimal utilization and extraction of high-value products in the seafood industry. Raman spectroscopy provides a promising solution for quickly and non-destructively analyzing the biochemical composition of fish by associating Raman spectra with biochemical reference data using machine learning regression models. This paper investigates different regression models to address this task and proposes a new design of Convolutional Neural Networks (CNNs) for jointly predicting water, protein, and lipids yield. To the best of our knowledge, we are the first to conduct a successful study employing CNNs to analyze the biochemical composition of fish based on a very small Raman spectroscopic dataset. Our approach combines a tailored CNN architecture with the comprehensive data preparation procedure, effectively mitigating the challenges posed by extreme data scarcity. The results demonstrate that our CNN can significantly outperform two state-of-the-art CNN models and multiple traditional machine learning models, paving the way for accurate and automated analysis of fish biochemical composition.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2406.08980.pdf' target='_blank'>https://arxiv.org/pdf/2406.08980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Gao, Haichuan Tan, Yanwen Huang, Minsi Ren, Xiao Huang, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08980">From Theory to Therapy: Reframing SBDD Model Evaluation via Practical Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in structure-based drug design (SBDD) have significantly enhanced the efficiency and precision of drug discovery by generating molecules tailored to bind specific protein pockets. Despite these technological strides, their practical application in real-world drug development remains challenging due to the complexities of synthesizing and testing these molecules. The reliability of the Vina docking score, the current standard for assessing binding abilities, is increasingly questioned due to its susceptibility to overfitting. To address these limitations, we propose a comprehensive evaluation framework that includes assessing the similarity of generated molecules to known active compounds, introducing a virtual screening-based metric for practical deployment capabilities, and re-evaluating binding affinity more rigorously. Our experiments reveal that while current SBDD models achieve high Vina scores, they fall short in practical usability metrics, highlighting a significant gap between theoretical predictions and real-world applicability. Our proposed metrics and dataset aim to bridge this gap, enhancing the practical applicability of future SBDD models and aligning them more closely with the needs of pharmaceutical research and development.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2406.08961.pdf' target='_blank'>https://arxiv.org/pdf/2406.08961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Huang, Bowen Gao, Yinjun Jia, Hongbo Ma, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08961">SIU: A Million-Scale Structural Small Molecule-Protein Interaction Dataset for Unbiased Bioactivity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small molecules play a pivotal role in modern medicine, and scrutinizing their interactions with protein targets is essential for the discovery and development of novel, life-saving therapeutics. The term "bioactivity" encompasses various biological effects resulting from these interactions, including both binding and functional responses. The magnitude of bioactivity dictates the therapeutic or toxic pharmacological outcomes of small molecules, rendering accurate bioactivity prediction crucial for the development of safe and effective drugs. However, existing structural datasets of small molecule-protein interactions are often limited in scale and lack systematically organized bioactivity labels, thereby impeding our understanding of these interactions and precise bioactivity prediction. In this study, we introduce a comprehensive dataset of small molecule-protein interactions, consisting of over a million binding structures, each annotated with real biological activity labels. This dataset is designed to facilitate unbiased bioactivity prediction. We evaluated several classical models on this dataset, and the results demonstrate that the task of unbiased bioactivity prediction is challenging yet essential.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2403.12987.pdf' target='_blank'>https://arxiv.org/pdf/2403.12987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Gao, Minsi Ren, Yuyan Ni, Yanwen Huang, Bo Qiang, Zhi-Ming Ma, Wei-Ying Ma, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12987">Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of Structure-based Drug Design (SBDD), deep learning-based generative models have achieved outstanding performance in terms of docking score. However, further study shows that the existing molecular generative methods and docking scores both have lacked consideration in terms of specificity, which means that generated molecules bind to almost every protein pocket with high affinity. To address this, we introduce the Delta Score, a new metric for evaluating the specificity of molecular binding. To further incorporate this insight for generation, we develop an innovative energy-guided approach using contrastive learning, with active compounds as decoys, to direct generative models toward creating molecules with high specificity. Our empirical results show that this method not only enhances the delta score but also maintains or improves traditional docking scores, successfully bridging the gap between SBDD and real-world needs.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2409.14617.pdf' target='_blank'>https://arxiv.org/pdf/2409.14617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bohao Xu, Yingzhou Lu, Yoshitaka Inoue, Namkyeong Lee, Tianfan Fu, Jintai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14617">Protein-Mamba: Biological Mamba Models for Protein Function Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein function prediction is a pivotal task in drug discovery, significantly impacting the development of effective and safe therapeutics. Traditional machine learning models often struggle with the complexity and variability inherent in predicting protein functions, necessitating more sophisticated approaches. In this work, we introduce Protein-Mamba, a novel two-stage model that leverages both self-supervised learning and fine-tuning to improve protein function prediction. The pre-training stage allows the model to capture general chemical structures and relationships from large, unlabeled datasets, while the fine-tuning stage refines these insights using specific labeled datasets, resulting in superior prediction performance. Our extensive experiments demonstrate that Protein-Mamba achieves competitive performance, compared with a couple of state-of-the-art methods across a range of protein function datasets. This model's ability to effectively utilize both unlabeled and labeled data highlights the potential of self-supervised learning in advancing protein function prediction and offers a promising direction for future research in drug discovery.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2401.14665.pdf' target='_blank'>https://arxiv.org/pdf/2401.14665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yipin Lei, Xu Wang, Meng Fang, Han Li, Xiang Li, Jianyang Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14665">PepGB: Facilitating peptide drug discovery via graph neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptides offer great biomedical potential and serve as promising drug candidates. Currently, the majority of approved peptide drugs are directly derived from well-explored natural human peptides. It is quite necessary to utilize advanced deep learning techniques to identify novel peptide drugs in the vast, unexplored biochemical space. Despite various in silico methods having been developed to accelerate peptide early drug discovery, existing models face challenges of overfitting and lacking generalizability due to the limited size, imbalanced distribution and inconsistent quality of experimental data. In this study, we propose PepGB, a deep learning framework to facilitate peptide early drug discovery by predicting peptide-protein interactions (PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained perturbation module and a dual-view objective with contrastive learning-based peptide pre-trained representation to predict PepPIs. Through rigorous evaluations, we demonstrated that PepGB greatly outperforms baselines and can accurately identify PepPIs for novel targets and peptide hits, thereby contributing to the target identification and hit discovery processes. Next, we derive an extended version, diPepGB, to tackle the bottleneck of modeling highly imbalanced data prevalent in lead generation and optimization processes. Utilizing directed edges to represent relative binding strength between two peptide nodes, diPepGB achieves superior performance in real-world assays. In summary, our proposed frameworks can serve as potent tools to facilitate peptide early drug discovery.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2509.21971.pdf' target='_blank'>https://arxiv.org/pdf/2509.21971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Jiang, Amina Mollaysa, Hehuan Ma, Tommaso Mansi, Junzhou Huang, Mangal Prakash, Rui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21971">GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug target interaction (DTI) prediction is a cornerstone of computational drug discovery, enabling rational design, repurposing, and mechanistic insights. While deep learning has advanced DTI modeling, existing approaches primarily rely on SMILES protein pairs and fail to exploit the rich multimodal information available for small molecules and proteins. We introduce GRAMDTI, a pretraining framework that integrates multimodal molecular and protein inputs into unified representations. GRAMDTI extends volume based contrastive learning to four modalities, capturing higher-order semantic alignment beyond conventional pairwise approaches. To handle modality informativeness, we propose adaptive modality dropout, dynamically regulating each modality's contribution during pre-training. Additionally, IC50 activity measurements, when available, are incorporated as weak supervision to ground representations in biologically meaningful interaction strengths. Experiments on four publicly available datasets demonstrate that GRAMDTI consistently outperforms state of the art baselines. Our results highlight the benefits of higher order multimodal alignment, adaptive modality utilization, and auxiliary supervision for robust and generalizable DTI prediction.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2506.19862.pdf' target='_blank'>https://arxiv.org/pdf/2506.19862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Xu, Jiahao Zhang, Mangal Prakash, Xiang Zhang, Suhang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19862">DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric graph neural networks (GNNs) that respect E(3) symmetries have achieved strong performance on small molecule modeling, but they face scalability and expressiveness challenges when applied to large biomolecules such as RNA and proteins. These systems require models that can simultaneously capture fine-grained atomic interactions, long-range dependencies across spatially distant components, and biologically relevant hierarchical structure, such as atoms forming residues, which in turn form higher-order domains. Existing geometric GNNs, which typically operate exclusively in either Euclidean or Spherical Harmonics space, are limited in their ability to capture both the fine-scale atomic details and the long-range, symmetry-aware dependencies required for modeling the multi-scale structure of large biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant Network that constructs complementary representations in both Euclidean and Spherical Harmonics spaces to capture local geometry and global symmetry-aware features. DualEquiNet employs bidirectional cross-space message passing and a novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate atomic features into biologically meaningful units, such as residues, enabling efficient and expressive multi-scale modeling for large biomolecular systems. DualEquiNet achieves state-of-the-art performance on multiple existing benchmarks for RNA property prediction and protein modeling, and outperforms prior methods on two newly introduced 3D structural benchmarks demonstrating its broad effectiveness across a range of large biomolecule modeling tasks.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2506.11152.pdf' target='_blank'>https://arxiv.org/pdf/2506.11152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiren Madhu, JoÃ£o Felipe Rocha, Tinglin Huang, Siddharth Viswanath, Smita Krishnaswamy, Rex Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11152">HEIST: A Graph Foundation Model for Spatial Transcriptomics and Proteomics Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-cell transcriptomics and proteomics have become a great source for data-driven insights into biology, enabling the use of advanced deep learning methods to understand cellular heterogeneity and gene expression at the single-cell level. With the advent of spatial-omics data, we have the promise of characterizing cells within their tissue context as it provides both spatial coordinates and intra-cellular transcriptional or protein counts. Proteomics offers a complementary view by directly measuring proteins, which are the primary effectors of cellular function and key therapeutic targets. However, existing models either ignore the spatial information or the complex genetic and proteomic programs within cells. Thus they cannot infer how cell internal regulation adapts to microenvironmental cues. Furthermore, these models often utilize fixed gene vocabularies, hindering their generalizability unseen genes. In this paper, we introduce HEIST, a hierarchical graph transformer foundation model for spatial transcriptomics and proteomics. HEIST models tissues as hierarchical graphs. The higher level graph is a spatial cell graph, and each cell in turn, is represented by its lower level gene co-expression network graph. HEIST achieves this by performing both intra-level and cross-level message passing to utilize the hierarchy in its embeddings and can thus generalize to novel datatypes including spatial proteomics without retraining. HEIST is pretrained on 22.3M cells from 124 tissues across 15 organs using spatially-aware contrastive and masked autoencoding objectives. Unsupervised analysis of HEIST embeddings reveals spatially informed subpopulations missed by prior models. Downstream evaluations demonstrate generalizability to proteomics data and state-of-the-art performance in clinical outcome prediction, cell type annotation, and gene imputation across multiple technologies.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2506.08936.pdf' target='_blank'>https://arxiv.org/pdf/2506.08936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amina Mollaysa, Artem Moskale, Pushpak Pati, Tommaso Mansi, Mangal Prakash, Rui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08936">BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present BioLangFusion, a simple approach for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations. Motivated by the central dogma of molecular biology (information flow from gene to transcript to protein), we align per-modality embeddings at the biologically meaningful codon level (three nucleotides encoding one amino acid) to ensure direct cross-modal correspondence. BioLangFusion studies three standard fusion techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized attention pooling inspired by multiple-instance learning, and (iii) cross-modal multi-head attention -- each technique providing a different inductive bias for combining modality-specific signals. These methods require no additional pre-training or modification of the base models, allowing straightforward integration with existing sequence-based foundation models. Across five molecular property prediction tasks, BioLangFusion outperforms strong unimodal baselines, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2505.22560.pdf' target='_blank'>https://arxiv.org/pdf/2505.22560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Moskalev, Mangal Prakash, Junjie Xu, Tianyu Cui, Rui Liao, Tommaso Mansi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22560">Geometric Hyena Networks for Large-scale Equivariant Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2505.20346.pdf' target='_blank'>https://arxiv.org/pdf/2505.20346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Kuang, Nuowei Liu, Changzhi Sun, Tao Ji, Yuanbin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20346">PDFBench: A Benchmark for De novo Protein Design from Function</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. Moreover, these methods often employ evaluation metrics that capture only a subset of the desired properties of designed proteins, lacking a comprehensive assessment framework. To address these, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. PDFBench supports two tasks: description-guided design and keyword-guided design. To ensure fair and multifaceted evaluation, we compile 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity. We evaluate five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Finally, we analyze inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection. PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2505.18966.pdf' target='_blank'>https://arxiv.org/pdf/2505.18966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nuowei Liu, Jiahao Kuang, Yanting Liu, Changzhi Sun, Tao Ji, Yuanbin Wu, Man Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18966">Protein Design with Dynamic Protein Vocabulary</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.6%.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2505.11194.pdf' target='_blank'>https://arxiv.org/pdf/2505.11194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Fei, Michail Chatzianastasis, Sarah Almeida Carneiro, Hadi Abdine, Lawrence P. Petalidis, Michalis Vazirgiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11194">Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2504.03847.pdf' target='_blank'>https://arxiv.org/pdf/2504.03847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaokun Liu, Sayedmohammadreza Rastegari, Yijun Huang, Sxe Chang Cheong, Weikang Liu, Wenjie Zhao, Qihao Tian, Hongming Wang, Yingjie Guo, Shuo Zhou, Sina Tabakhi, Xianyuan Liu, Zheqing Zhu, Wei Sang, Haiping Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03847">Interpretable Multimodal Learning for Tumor Protein-Metal Binding: Progress, Challenges, and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cancer therapeutics, protein-metal binding mechanisms critically govern the pharmacokinetics and targeting efficacy of drugs, thereby fundamentally shaping the rational design of anticancer metallodrugs. While conventional laboratory methods used to study such mechanisms are often costly, low throughput, and limited in capturing dynamic biological processes, machine learning (ML) has emerged as a promising alternative. Despite increasing efforts to develop protein-metal binding datasets and ML algorithms, the application of ML in tumor protein-metal binding remains limited. Key challenges include a shortage of high-quality, tumor-specific datasets, insufficient consideration of multiple data modalities, and the complexity of interpreting results due to the ''black box'' nature of complex ML models. This paper summarizes recent progress and ongoing challenges in using ML to predict tumor protein-metal binding, focusing on data, modeling, and interpretability. We present multimodal protein-metal binding datasets and outline strategies for acquiring, curating, and preprocessing them for training ML models. Moreover, we explore the complementary value provided by different data modalities and examine methods for their integration. We also review approaches for improving model interpretability to support more trustworthy decisions in cancer research. Finally, we offer our perspective on research opportunities and propose strategies to address the scarcity of tumor protein data and the limited number of predictive models for tumor protein-metal binding. We also highlight two promising directions for effective metal-based drug design: integrating protein-protein interaction data to provide structural insights into metal-binding events and predicting structural changes in tumor proteins after metal binding.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2503.03904.pdf' target='_blank'>https://arxiv.org/pdf/2503.03904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Nakis, Chrysoula Kosma, Anastasia Brativnyk, Michail Chatzianastasis, Iakovos Evdaimon, Michalis Vazirgiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03904">The Signed Two-Space Proximity Model for Learning Representations in Protein-Protein Interaction Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting complex protein-protein interactions (PPIs) is crucial for decoding biological processes, from cellular functioning to disease mechanisms. However, experimental methods for determining PPIs are computationally expensive. Thus, attention has been recently drawn to machine learning approaches. Furthermore, insufficient effort has been made toward analyzing signed PPI networks, which capture both activating (positive) and inhibitory (negative) interactions. To accurately represent biological relationships, we present the Signed Two-Space Proximity Model (S2-SPM) for signed PPI networks, which explicitly incorporates both types of interactions, reflecting the complex regulatory mechanisms within biological systems. This is achieved by leveraging two independent latent spaces to differentiate between positive and negative interactions while representing protein similarity through proximity in these spaces. Our approach also enables the identification of archetypes representing extreme protein profiles. S2-SPM's superior performance in predicting the presence and sign of interactions in SPPI networks is demonstrated in link prediction tasks against relevant baseline methods. Additionally, the biological prevalence of the identified archetypes is confirmed by an enrichment analysis of Gene Ontology (GO) terms, which reveals that distinct biological tasks are associated with archetypal groups formed by both interactions. This study is also validated regarding statistical significance and sensitivity analysis, providing insights into the functional roles of different interaction types. Finally, the robustness and consistency of the extracted archetype structures are confirmed using the Bayesian Normalized Mutual Information (BNMI) metric, proving the model's reliability in capturing meaningful SPPI patterns.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2501.19200.pdf' target='_blank'>https://arxiv.org/pdf/2501.19200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.19200">A Variational Perspective on Generative Protein Fitness Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of protein fitness optimization is to discover new protein variants with enhanced fitness for a given use. The vast search space and the sparsely populated fitness landscape, along with the discrete nature of protein sequences, pose significant challenges when trying to determine the gradient towards configurations with higher fitness. We introduce Variational Latent Generative Protein Optimization (VLGPO), a variational perspective on fitness optimization. Our method embeds protein sequences in a continuous latent space to enable efficient sampling from the fitness distribution and combines a (learned) flow matching prior over sequence mutations with a fitness predictor to guide optimization towards sequences with high fitness. VLGPO achieves state-of-the-art results on two different protein benchmarks of varying complexity. Moreover, the variational design with explicit prior and likelihood functions offers a flexible plug-and-play framework that can be easily customized to suit various protein design tasks.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2412.11618.pdf' target='_blank'>https://arxiv.org/pdf/2412.11618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nuowei Liu, Changzhi Sun, Tao Ji, Junfeng Tian, Jianxin Tang, Yuanbin Wu, Man Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11618">EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal Structure and Sequence Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Large Language Models (LLMs) for understanding proteins primarily treats amino acid sequences as a text modality. Meanwhile, Protein Language Models (PLMs), such as ESM-2, have learned massive sequential evolutionary knowledge from the universe of natural protein sequences. Furthermore, structure-based encoders like ProteinMPNN learn the structural information of proteins through Graph Neural Networks. However, whether the incorporation of protein encoders can enhance the protein understanding of LLMs has not been explored. To bridge this gap, we propose EvoLlama, a multimodal framework that connects a structure-based encoder, a sequence-based protein encoder and an LLM for protein understanding. EvoLlama consists of a ProteinMPNN structure encoder, an ESM-2 protein sequence encoder, a multimodal projector to align protein and text representations and a Llama-3 text decoder. To train EvoLlama, we fine-tune it on protein-oriented instructions and protein property prediction datasets verbalized via natural language instruction templates. Our experiments show that EvoLlama's protein understanding capabilities have been significantly enhanced, outperforming other fine-tuned protein-oriented LLMs in zero-shot settings by an average of 1%-8% and surpassing the state-of-the-art baseline with supervised fine-tuning by an average of 6%. On protein property prediction datasets, our approach achieves promising results that are competitive with state-of-the-art task-specific baselines. We will release our code in a future version.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2410.20317.pdf' target='_blank'>https://arxiv.org/pdf/2410.20317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Viswanath, Dhananjay Bhaskar, David R. Johnson, Joao Felipe Rocha, Egbert Castro, Jackson D. Grady, Alex T. Grigas, Michael A. Perlmutter, Corey S. O'Hern, Smita Krishnaswamy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20317">ProtSCAPE: Mapping the landscape of protein conformations in molecular dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the dynamic nature of protein structures is essential for comprehending their biological functions. While significant progress has been made in predicting static folded structures, modeling protein motions on microsecond to millisecond scales remains challenging. To address these challenges, we introduce a novel deep learning architecture, Protein Transformer with Scattering, Attention, and Positional Embedding (ProtSCAPE), which leverages the geometric scattering transform alongside transformer-based attention mechanisms to capture protein dynamics from molecular dynamics (MD) simulations. ProtSCAPE utilizes the multi-scale nature of the geometric scattering transform to extract features from protein structures conceptualized as graphs and integrates these features with dual attention structures that focus on residues and amino acid signals, generating latent representations of protein trajectories. Furthermore, ProtSCAPE incorporates a regression head to enforce temporally coherent latent representations.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2410.12459.pdf' target='_blank'>https://arxiv.org/pdf/2410.12459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Yazdani-Jahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, Rui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12459">HELM: Hierarchical Encoding for mRNA Language Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its codon structure directly impacting biological properties. While Language Models (LMs) have shown promise in analyzing biological sequences, existing approaches fail to account for the hierarchical nature of mRNA's codon structure. We introduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel pre-training strategy that incorporates codon-level hierarchical structure into language model training. HELM modulates the loss function based on codon synonymity, aligning the model's learning process with the biological reality of mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks, demonstrating that HELM outperforms standard language model pre-training as well as existing foundation model baselines on seven diverse downstream property prediction tasks and an antibody region annotation tasks on average by around 8%. Additionally, HELM enhances the generative capabilities of language model, producing diverse mRNA sequences that better align with the underlying true data distribution compared to non-hierarchical baselines.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2406.14142.pdf' target='_blank'>https://arxiv.org/pdf/2406.14142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michail Chatzianastasis, Yang Zhang, George Dasoulas, Michalis Vazirgiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14142">Geometric Self-Supervised Pretraining on 3D Protein Structures using Subgraphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning aims to learn informative protein embeddings capable of addressing crucial biological questions, such as protein function prediction. Although sequence-based transformer models have shown promising results by leveraging the vast amount of protein sequence data in a self-supervised way, there is still a gap in exploiting the available 3D protein structures. In this work, we propose a pre-training scheme going beyond trivial masking methods leveraging 3D and hierarchical structures of proteins. We propose a novel self-supervised method to pretrain 3D graph neural networks on 3D protein structures, by predicting the distances between local geometric centroids of protein subgraphs and the global geometric centroid of the protein. By considering subgraphs and their relationships to the global protein structure, our model can better learn the geometric properties of the protein structure. We experimentally show that our proposed pertaining strategy leads to significant improvements up to 6\%, in the performance of 3D GNNs in various protein classification tasks. Our work opens new possibilities in unsupervised learning for protein graph models while eliminating the need for multiple views, augmentations, or masking strategies which are currently used so far.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2404.00014.pdf' target='_blank'>https://arxiv.org/pdf/2404.00014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Odin Zhang, Yufei Huang, Shichen Cheng, Mengyao Yu, Xujun Zhang, Haitao Lin, Yundian Zeng, Mingyang Wang, Zhenxing Wu, Huifeng Zhao, Zaixi Zhang, Chenqing Hua, Yu Kang, Sunliang Cui, Peichen Pan, Chang-Yu Hsieh, Tingjun Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00014">Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most earlier 3D structure-based molecular generation approaches follow an atom-wise paradigm, incrementally adding atoms to a partially built molecular fragment within protein pockets. These methods, while effective in designing tightly bound ligands, often overlook other essential properties such as synthesizability. The fragment-wise generation paradigm offers a promising solution. However, a common challenge across both atom-wise and fragment-wise methods lies in their limited ability to co-design plausible chemical and geometrical structures, resulting in distorted conformations. In response to this challenge, we introduce the Deep Geometry Handling protocol, a more abstract design that extends the design focus beyond the model architecture. Through a comprehensive review of existing geometry-related models and their protocols, we propose a novel hybrid strategy, culminating in the development of FragGen - a geometry-reliable, fragment-wise molecular generation method. FragGen marks a significant leap forward in the quality of generated geometry and the synthesis accessibility of molecules. The efficacy of FragGen is further validated by its successful application in designing type II kinase inhibitors at the nanomolar level.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2511.04984.pdf' target='_blank'>https://arxiv.org/pdf/2511.04984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinheng He, Yijia Zhang, Haowei Lin, Xingang Peng, Xiangzhe Kong, Mingyu Li, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04984">Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the generation of hit and lead compounds. However, most AI-driven approaches neglect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Peptide2Mol, an E(3)-equivariant graph neural network diffusion model that generates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Additionally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2510.20943.pdf' target='_blank'>https://arxiv.org/pdf/2510.20943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srivathsan Badrinarayanan, Yue Su, Janghoon Ock, Alan Pham, Sanya Ahuja, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20943">Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein mutations can have profound effects on biological function, making accurate prediction of property changes critical for drug discovery, protein engineering, and precision medicine. Current approaches rely on fine-tuning protein-specific transformers for individual datasets, but struggle with cross-dataset generalization due to heterogeneous experimental conditions and limited target domain data. We introduce two key innovations: (1) the first application of Model-Agnostic Meta-Learning (MAML) to protein mutation property prediction, and (2) a novel mutation encoding strategy using separator tokens to directly incorporate mutations into sequence context. We build upon transformer architectures integrating them with MAML to enable rapid adaptation to new tasks through minimal gradient steps rather than learning dataset-specific patterns. Our mutation encoding addresses the critical limitation where standard transformers treat mutation positions as unknown tokens, significantly degrading performance. Evaluation across three diverse protein mutation datasets (functional fitness, thermal stability, and solubility) demonstrates significant advantages over traditional fine-tuning. In cross-task evaluation, our meta-learning approach achieves 29% better accuracy for functional fitness with 65% less training time, and 94% better accuracy for solubility with 55% faster training. The framework maintains consistent training efficiency regardless of dataset size, making it particularly valuable for industrial applications and early-stage protein design where experimental data is limited. This work establishes a systematic application of meta-learning to protein mutation analysis and introduces an effective mutation encoding strategy, offering transformative methodology for cross-domain generalization in protein engineering.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2507.02925.pdf' target='_blank'>https://arxiv.org/pdf/2507.02925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janghoon Ock, Radheesh Sharma Meda, Srivathsan Badrinarayanan, Neha S. Aluru, Achuth Chandrasekhar, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02925">Large Language Model Agent for Modular Task Execution in Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, domain-specific question answering, molecular generation, property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. In a case study targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved relevant biomolecular information-including FASTA sequences, SMILES representations, and literature-and answered mechanistic questions with improved contextual accuracy over standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55, and those passing at least four out of five empirical drug-likeness rules rose from 29 to 52, within a pool of 194 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2506.13196.pdf' target='_blank'>https://arxiv.org/pdf/2506.13196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Liu, Keyan Ding, Peilin Chen, Yinwei Wei, Liqiang Nie, Dapeng Wu, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13196">KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate Protein-Ligand Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding affinity is critical for drug discovery. While recent deep learning approaches have demonstrated promising results, they often rely solely on structural features of proteins and ligands, overlooking their valuable biochemical knowledge associated with binding affinity. To address this limitation, we propose KEPLA, a novel deep learning framework that explicitly integrates prior knowledge from Gene Ontology and ligand properties to enhance prediction performance. KEPLA takes protein sequences and ligand molecular graphs as input and optimizes two complementary objectives: (1) aligning global representations with knowledge graph relations to capture domain-specific biochemical insights, and (2) leveraging cross attention between local representations to construct fine-grained joint embeddings for prediction. Experiments on two benchmark datasets across both in-domain and cross-domain scenarios demonstrate that KEPLA consistently outperforms state-of-the-art baselines. Furthermore, interpretability analyses based on knowledge graph relations and cross attention maps provide valuable insights into the underlying predictive mechanisms.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2502.15954.pdf' target='_blank'>https://arxiv.org/pdf/2502.15954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15954">MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2410.13643.pdf' target='_blank'>https://arxiv.org/pdf/2410.13643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, Aviv Regev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13643">Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences across domains from natural language to biological sequence generation. For example, in the protein inverse folding task, conditional diffusion models have achieved impressive results in generating natural-like sequences that fold back into the original structure. However, practical design tasks often require not only modeling a conditional distribution but also optimizing specific task objectives. For instance, we may prefer protein sequences with high stability. To address this, we consider the scenario where we have pre-trained discrete diffusion models that can generate natural-like sequences, as well as reward models that map sequences to task objectives. We then formulate the reward maximization problem within discrete diffusion models, analogous to reinforcement learning (RL), while minimizing the KL divergence against pretrained diffusion models to preserve naturalness. To solve this RL problem, we propose a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models, by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick. Our theoretical analysis indicates that our approach can generate sequences that are both natural-like and yield high rewards. While similar tasks have been recently explored in diffusion models for continuous domains, our work addresses unique algorithmic and theoretical challenges specific to discrete diffusion models, which arise from their foundation in continuous-time Markov chains rather than Brownian motion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA and protein sequences that optimize enhancer activity and protein stability, respectively, important tasks for gene therapies and protein-based therapeutics.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2410.06211.pdf' target='_blank'>https://arxiv.org/pdf/2410.06211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex M. Tseng, Gokcen Eraslan, Tommaso Biancalani, Gabriele Scalia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06211">A mechanistically interpretable neural network for regulatory genomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks excel in mapping genomic DNA sequences to associated readouts (e.g., protein-DNA binding). Beyond prediction, the goal of these networks is to reveal to scientists the underlying motifs (and their syntax) which drive genome regulation. Traditional methods that extract motifs from convolutional filters suffer from the uninterpretable dispersion of information across filters and layers. Other methods which rely on importance scores can be unstable and unreliable. Instead, we designed a novel mechanistically interpretable architecture for regulatory genomics, where motifs and their syntax are directly encoded and readable from the learned weights and activations. We provide theoretical and empirical evidence of our architecture's full expressivity, while still being highly interpretable. Through several experiments, we show that our architecture excels in de novo motif discovery and motif instance calling, is robust to variable sequence contexts, and enables fully interpretable generation of novel functional sequences.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2408.05196.pdf' target='_blank'>https://arxiv.org/pdf/2408.05196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Zhewen Lu, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Yoshua Bengio, Gabriele Scalia, MichaÅ Koziarski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05196">Cell Morphology-Guided Small Molecule Generation with GFlowNets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-content phenotypic screening, including high-content imaging (HCI), has gained popularity in the last few years for its ability to characterize novel therapeutics without prior knowledge of the protein target. When combined with deep learning techniques to predict and represent molecular-phenotype interactions, these advancements hold the potential to significantly accelerate and enhance drug discovery applications. This work focuses on the novel task of HCI-guided molecular design. Generative models for molecule design could be guided by HCI data, for example with a supervised model that links molecules to phenotypes of interest as a reward function. However, limited labeled data, combined with the high-dimensional readouts, can make training these methods challenging and impractical. We consider an alternative approach in which we leverage an unsupervised multimodal joint embedding to define a latent similarity as a reward for GFlowNets. The proposed model learns to generate new molecules that could produce phenotypic effects similar to those of the given image target, without relying on pre-annotated phenotypic labels. We demonstrate that the proposed method generates molecules with high morphological and structural similarity to the target, increasing the likelihood of similar biological activity, as confirmed by an independent oracle model.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2405.19673.pdf' target='_blank'>https://arxiv.org/pdf/2405.19673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masatoshi Uehara, Yulai Zhao, Ehsan Hajiramezanali, Gabriele Scalia, GÃ¶kcen Eraslan, Avantika Lal, Sergey Levine, Tommaso Biancalani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19673">Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation. To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL. Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible. In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains. In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions. To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2403.10358.pdf' target='_blank'>https://arxiv.org/pdf/2403.10358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janghoon Ock, Parisa Mollaei, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10358">GradNav: Accelerated Exploration of Potential Energy Surfaces with Gradient-Based Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exploration of molecular systems' potential energy surface is important for comprehending their complex behaviors, particularly through identifying various metastable states. However, the transition between these states is often hindered by substantial energy barriers, demanding prolonged molecular simulations that consume considerable computational efforts. Our study introduces the GradNav algorithm, which enhances the exploration of the energy surface, accelerating the reconstruction of the potential energy surface (PES). This algorithm employs a strategy of initiating short simulation runs from updated starting points, derived from prior observations, to effectively navigate across potential barriers and explore new regions. To evaluate GradNav's performance, we introduce two metrics: the deepest well escape frame (DWEF) and the search success initialization ratio (SSIR). Through applications on Langevin dynamics within Mueller-type potential energy surfaces and molecular dynamics simulations of the Fs-Peptide protein, these metrics demonstrate GradNav's enhanced ability to escape deep energy wells, as shown by reduced DWEF values, and its reduced reliance on initial conditions, highlighted by increased SSIR values. Consequently, this improved exploration capability enables more precise energy estimations from simulation trajectories.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2509.26443.pdf' target='_blank'>https://arxiv.org/pdf/2509.26443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Bhan, Miroslav Krstic, Yuanyuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26443">Stabilization of nonlinear systems with unknown delays via delay-adaptive neural operator approximate predictors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work establishes the first rigorous stability guarantees for approximate predictors in delay-adaptive control of nonlinear systems, addressing a key challenge in practical implementations where exact predictors are unavailable. We analyze two scenarios: (i) when the actuated input is directly measurable, and (ii) when it is estimated online. For the measurable input case, we prove semi-global practical asymptotic stability with an explicit bound proportional to the approximation error $Îµ$. For the unmeasured input case, we demonstrate local practical asymptotic stability, with the region of attraction explicitly dependent on both the initial delay estimate and the predictor approximation error. To bridge theory and practice, we show that neural operators-a flexible class of neural network-based approximators-can achieve arbitrarily small approximation errors, thus satisfying the conditions of our stability theorems. Numerical experiments on two nonlinear benchmark systems-a biological protein activator/repressor model and a micro-organism growth Chemostat model-validate our theoretical results. In particular, our numerical simulations confirm stability under approximate predictors, highlight the strong generalization capabilities of neural operators, and demonstrate a substantial computational speedup of up to 15x compared to a baseline fixed-point method.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.02642.pdf' target='_blank'>https://arxiv.org/pdf/2509.02642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Feng, Jiying Zhang, Xinni Zhang, Zijing Liu, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02642">BioMD: All-atom Generative Model for Biomolecular Dynamics Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics (MD) simulations are essential tools in computational chemistry and drug discovery, offering crucial insights into dynamic molecular behavior. However, their utility is significantly limited by substantial computational costs, which severely restrict accessible timescales for many biologically relevant processes. Despite the encouraging performance of existing machine learning (ML) methods, they struggle to generate extended biomolecular system trajectories, primarily due to the lack of MD datasets and the large computational demands of modeling long historical trajectories. Here, we introduce BioMD, the first all-atom generative model to simulate long-timescale protein-ligand dynamics using a hierarchical framework of forecasting and interpolation. We demonstrate the effectiveness and versatility of BioMD on the DD-13M (ligand unbinding) and MISATO datasets. For both datasets, BioMD generates highly realistic conformations, showing high physical plausibility and low reconstruction errors. Besides, BioMD successfully generates ligand unbinding paths for 97.1% of the protein-ligand systems within ten attempts, demonstrating its ability to explore critical unbinding pathways. Collectively, these results establish BioMD as a tool for simulating complex biomolecular processes, offering broad applicability for computational chemistry and drug discovery.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2508.10629.pdf' target='_blank'>https://arxiv.org/pdf/2508.10629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Soga, Zhenyu Lei, Yinhan He, Camille Bilodeau, Jundong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10629">Energy-Based Models for Predicting Mutational Effects on Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting changes in binding free energy ($ÎÎG$) is a vital task in protein engineering and protein-protein interaction (PPI) engineering for drug discovery. Previous works have observed a high correlation between $ÎÎG$ and entropy, using probabilities of biologically important objects such as side chain angles and residue identities to estimate $ÎÎG$. However, estimating the full conformational distribution of a protein complex is generally considered intractable. In this work, we propose a new approach to $ÎÎG$ prediction that avoids this issue by instead leveraging energy-based models for estimating the probability of a complex's conformation. Specifically, we novelly decompose $ÎÎG$ into a sequence-based component estimated by an inverse folding model and a structure-based component estimated by an energy model. This decomposition is made tractable by assuming equilibrium between the bound and unbound states, allowing us to simplify the estimation of degeneracies associated with each state. Unlike previous deep learning-based methods, our method incorporates an energy-based physical inductive bias by connecting the often-used sequence log-odds ratio-based approach to $ÎÎG$ prediction with a new $ÎÎE$ term grounded in statistical mechanics. We demonstrate superiority over existing state-of-the-art structure and sequence-based deep learning methods in $ÎÎG$ prediction and antibody optimization against SARS-CoV-2.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2504.18367.pdf' target='_blank'>https://arxiv.org/pdf/2504.18367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maodong Li, Jiying Zhang, Bin Feng, Wenqi Zeng, Dechin Chen, Zhijun Pan, Yu Li, Zijing Liu, Yi Isaac Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18367">Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug-protein binding and dissociation dynamics are fundamental to understanding molecular interactions in biological systems. While many tools for drug-protein interaction studies have emerged, especially artificial intelligence (AI)-based generative models, predictive tools on binding/dissociation kinetics and dynamics are still limited. We propose a novel research paradigm that combines molecular dynamics (MD) simulations, enhanced sampling, and AI generative models to address this issue. We propose an enhanced sampling strategy to efficiently implement the drug-protein dissociation process in MD simulations and estimate the free energy surface (FES). We constructed a program pipeline of MD simulations based on this sampling strategy, thus generating a dataset including 26,612 drug-protein dissociation trajectories containing about 13 million frames. We named this dissociation dynamics dataset DD-13M and used it to train a deep equivariant generative model UnbindingFlow, which can generate collision-free dissociation trajectories. The DD-13M database and UnbindingFlow model represent a significant advancement in computational structural biology, and we anticipate its broad applicability in machine learning studies of drug-protein interactions. Our ongoing efforts focus on expanding this methodology to encompass a broader spectrum of drug-protein complexes and exploring novel applications in pathway prediction.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2502.15867.pdf' target='_blank'>https://arxiv.org/pdf/2502.15867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingying Sun, Jun A, Zhiwei Liu, Rui Sun, Liujia Qian, Samuel H. Payne, Wout Bittremieux, Markus Ralser, Chen Li, Yi Chen, Zhen Dong, Yasset Perez-Riverol, Asif Khan, Chris Sander, Ruedi Aebersold, Juan Antonio VizcaÃ­no, Jonathan R Krieger, Jianhua Yao, Han Wen, Linfeng Zhang, Yunping Zhu, Yue Xuan, Benjamin Boyang Sun, Liang Qiao, Henning Hermjakob, Haixu Tang, Huanhuan Gao, Yamin Deng, Qing Zhong, Cheng Chang, Nuno Bandeira, Ming Li, Weinan E, Siqi Sun, Yuedong Yang, Gilbert S. Omenn, Yue Zhang, Ping Xu, Yan Fu, Xiaowen Liu, Christopher M. Overall, Yu Wang, Eric W. Deutsch, Luonan Chen, JÃ¼rgen Cox, Vadim Demichev, Fuchu He, Jiaxing Huang, Huilin Jin, Chao Liu, Nan Li, Zhongzhi Luan, Jiangning Song, Kaicheng Yu, Wanggen Wan, Tai Wang, Kang Zhang, Le Zhang, Peter A. Bell, Matthias Mann, Bing Zhang, Tiannan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15867">Strategic priorities for transformative progress in advancing biology with proteomics and artificial intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is transforming scientific research, including proteomics. Advances in mass spectrometry (MS)-based proteomics data quality, diversity, and scale, combined with groundbreaking AI techniques, are unlocking new challenges and opportunities in biological discovery. Here, we highlight key areas where AI is driving innovation, from data analysis to new biological insights. These include developing an AI-friendly ecosystem for proteomics data generation, sharing, and analysis; improving peptide and protein identification and quantification; characterizing protein-protein interactions and protein complexes; advancing spatial and perturbation proteomics; integrating multi-omics data; and ultimately enabling AI-empowered virtual cells.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2502.09890.pdf' target='_blank'>https://arxiv.org/pdf/2502.09890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09890">Rao-Blackwell Gradient Estimators for Equivariant Denoising Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In domains such as molecular and protein generation, physical systems exhibit inherent symmetries that are critical to model. Two main strategies have emerged for learning invariant distributions: designing equivariant network architectures and using data augmentation to approximate equivariance. While equivariant architectures preserve symmetry by design, they often involve greater complexity and pose optimization challenges. Data augmentation, on the other hand, offers flexibility but may fall short in fully capturing symmetries. Our framework enhances both approaches by reducing training variance and providing a provably lower-variance gradient estimator. We achieve this by interpreting data augmentation as a Monte Carlo estimator of the training gradient and applying Rao-Blackwellization. This leads to more stable optimization, faster convergence, and reduced variance, all while requiring only a single forward and backward pass per sample. We also present a practical implementation of this estimator incorporating the loss and sampling procedure through a method we call Orbit Diffusion. Theoretically, we guarantee that our loss admits equivariant minimizers. Empirically, Orbit Diffusion achieves state-of-the-art results on GEOM-QM9 for molecular conformation generation, improves crystal structure prediction, and advances text-guided crystal generation on the Perov-5 and MP-20 benchmarks. Additionally, it enhances protein designability in protein structure generation.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2411.05173.pdf' target='_blank'>https://arxiv.org/pdf/2411.05173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prakash Chourasia, Tamkanat E Ali, Sarwan Ali, Murray Pattersn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05173">DWFL: Enhancing Federated Learning through Dynamic Weighted Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) is a distributed learning technique that maintains data privacy by providing a decentralized training method for machine learning models using distributed big data. This promising Federated Learning approach has also gained popularity in bioinformatics, where the privacy of biomedical data holds immense importance, especially when patient data is involved. Despite the successful implementation of Federated learning in biological sequence analysis, rigorous consideration is still required to improve accuracy in a way that data privacy should not be compromised. Additionally, the optimal integration of federated learning, especially in protein sequence analysis, has not been fully explored. We propose a deep feed-forward neural network-based enhanced federated learning method for protein sequence classification to overcome these challenges. Our method introduces novel enhancements to improve classification accuracy. We introduce dynamic weighted federated learning (DWFL) which is a federated learning-based approach, where local model weights are adjusted using weighted averaging based on their performance metrics. By assigning higher weights to well-performing models, we aim to create a more potent initial global model for the federated learning process, leading to improved accuracy. We conduct experiments using real-world protein sequence datasets to assess the effectiveness of DWFL. The results obtained using our proposed approach demonstrate significant improvements in model accuracy, making federated learning a preferred, more robust, and privacy-preserving approach for collaborative machine-learning tasks.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2410.13106.pdf' target='_blank'>https://arxiv.org/pdf/2410.13106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Grudzien Kuba, Pieter Abbeel, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13106">Cliqueformer: Model-Based Optimization with Structured Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large neural networks excel at prediction tasks, but their application to design problems, such as protein engineering or materials discovery, requires solving offline model-based optimization (MBO) problems. While predictive models may not directly translate to effective design, recent MBO algorithms incorporate reinforcement learning and generative modeling approaches. Meanwhile, theoretical work suggests that exploiting the target function's structure can enhance MBO performance. We present Cliqueformer, a transformer-based architecture that learns the black-box function's structure through functional graphical models (FGM), addressing distribution shift without relying on explicit conservative approaches. Across various domains, including chemical and genetic design tasks, Cliqueformer demonstrates superior performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2402.12558.pdf' target='_blank'>https://arxiv.org/pdf/2402.12558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MarÃ­a Teresa GarcÃ­a-OrdÃ¡s, Natalia Arias, Carmen Benavides, Oscar GarcÃ­a-Olalla, JosÃ© Alberto BenÃ­tez-Andrades
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12558">Evaluation of Country Dietary Habits Using Machine Learning Techniques in Relation to Deaths from COVID-19</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>COVID-19 disease has affected almost every country in the world. The large number of infected people and the different mortality rates between countries has given rise to many hypotheses about the key points that make the virus so lethal in some places. In this study, the eating habits of 170 countries were evaluated in order to find correlations between these habits and mortality rates caused by COVID-19 using machine learning techniques that group the countries together according to the different distribution of fat, energy, and protein across 23 different types of food, as well as the amount ingested in kilograms. Results shown how obesity and the high consumption of fats appear in countries with the highest death rates, whereas countries with a lower rate have a higher level of cereal consumption accompanied by a lower total average intake of kilocalories.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2401.05442.pdf' target='_blank'>https://arxiv.org/pdf/2401.05442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Grudzien Kuba, Masatoshi Uehara, Pieter Abbeel, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05442">Functional Graphical Models: Structure Enables Offline Data-Driven Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomposing the original high-dimensional optimization problem into smaller sub-problems. This allows us to derive much more practical regret bounds for DDO, and the result implies that DDO with FGMs can achieve nearly optimal designs in situations where naive approaches fail due to insufficient coverage of the offline data. We further present a data-driven optimization algorithm that inferes the FGM structure itself, either over the original input variables or a latent variable representation of the inputs.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2507.02724.pdf' target='_blank'>https://arxiv.org/pdf/2507.02724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02724">Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2505.15093.pdf' target='_blank'>https://arxiv.org/pdf/2505.15093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Yang, Wenda Chu, Daniel Khalil, Raul Astudillo, Bruce J. Wittmann, Frances H. Arnold, Yisong Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15093">Steering Generative Models with Experimental Data for Protein Fitness Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent developments in steering protein generative models (e.g diffusion models, language models) offer a promising approach. However, by and large, past studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured by low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages compared to alternatives such as reinforcement learning with protein language models.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2502.12981.pdf' target='_blank'>https://arxiv.org/pdf/2502.12981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Cong Liu, Max Welling, Jan-Willem van de Meent, Erik J. Bekkers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12981">Riemannian Variational Flow Matching for Material and Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Riemannian Gaussian Variational Flow Matching (RG-VFM), a geometric extension of Variational Flow Matching (VFM) for generative modeling on manifolds. In Euclidean space, predicting endpoints (VFM), velocities (FM), or noise (diffusion) are largely equivalent due to affine interpolations. On curved manifolds this equivalence breaks down, and we hypothesize that endpoint prediction provides a stronger learning signal by directly minimizing geodesic distances. Building on this insight, we derive a variational flow matching objective based on Riemannian Gaussian distributions, applicable to manifolds with closed-form geodesics. We formally analyze its relationship to Riemannian Flow Matching (RFM), exposing that the RFM objective lacks a curvature-dependent penalty - encoded via Jacobi fields - that is naturally present in RG-VFM. Experiments on synthetic spherical and hyperbolic benchmarks, as well as real-world tasks in material and protein generation, demonstrate that RG-VFM more effectively captures manifold structure and improves downstream performance over Euclidean and velocity-based baselines.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2501.01930.pdf' target='_blank'>https://arxiv.org/pdf/2501.01930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwei Miao, Yuzhi Guo, Hehuan Ma, Jingquan Yan, Feng Jiang, Rui Liao, Junzhou Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01930">GoBERT: Gene Ontology Graph Informed BERT for Universal Gene Function Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring the functions of genes and gene products is crucial to a wide range of fields, including medical research, evolutionary biology, and environmental science. However, discovering new functions largely relies on expensive and exhaustive wet lab experiments. Existing methods of automatic function annotation or prediction mainly focus on protein function prediction with sequence, 3D-structures or protein family information. In this study, we propose to tackle the gene function prediction problem by exploring Gene Ontology graph and annotation with BERT (GoBERT) to decipher the underlying relationships among gene functions. Our proposed novel function prediction task utilizes existing functions as inputs and generalizes the function prediction to gene and gene products. Specifically, two pre-train tasks are designed to jointly train GoBERT to capture both explicit and implicit relations of functions. Neighborhood prediction is a self-supervised multi-label classification task that captures the explicit function relations. Specified masking and recovering task helps GoBERT in finding implicit patterns among functions. The pre-trained GoBERT possess the ability to predict novel functions for various gene and gene products based on known functional annotations. Extensive experiments, biological case studies, and ablation studies are conducted to demonstrate the superiority of our proposed GoBERT.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2412.12688.pdf' target='_blank'>https://arxiv.org/pdf/2412.12688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuwei Miao, Yuzhi Guo, Hehuan Ma, Jingquan Yan, Feng Jiang, Weizhi An, Jean Gao, Junzhou Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12688">UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation Benchmarks with Unified Entrez Gene Identifiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gene studies are crucial for fields such as protein structure prediction, drug discovery, and cancer genomics, yet they face challenges in fully utilizing the vast and diverse information available. Gene studies require clean, factual datasets to ensure reliable results. Ontology graphs, neatly organized domain terminology graphs, provide ideal sources for domain facts. However, available gene ontology annotations are currently distributed across various databases without unified identifiers for genes and gene products. To address these challenges, we introduce Unified Entrez Gene Identifier Dataset and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale public Gene Ontology Annotations (GOA) from various databases using unique gene identifiers. UniEntrezDB includes a pre-training dataset and four downstream tasks designed to comprehensively evaluate gene embedding performance from gene, protein, and cell levels, ultimately enhancing the reliability and applicability of LLMs in gene research and other professional settings.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2411.17798.pdf' target='_blank'>https://arxiv.org/pdf/2411.17798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Qianhui Xu, Ruichen Xia, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17798">DapPep: Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying T-cell receptors (TCRs) that interact with antigenic peptides provides the technical basis for developing vaccines and immunotherapies. The emergent deep learning methods excel at learning antigen binding patterns from known TCRs but struggle with novel or sparsely represented antigens. However, binding specificity for unseen antigens or exogenous peptides is critical. We introduce a domain-adaptive peptide-agnostic learning framework DapPep for universal TCR-antigen binding affinity prediction to address this challenge. The lightweight self-attention architecture combines a pre-trained protein language model with an inner-loop self-supervised regime to enable robust TCR-peptide representations. Extensive experiments on various benchmarks demonstrate that DapPep consistently outperforms existing tools, showcasing robust generalization capability, especially for data-scarce settings and unseen peptides. Moreover, DapPep proves effective in challenging clinical tasks such as sorting reactive T cells in tumor neoantigen therapy and identifying key positions in 3D structures.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2411.17795.pdf' target='_blank'>https://arxiv.org/pdf/2411.17795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Ge Wang, Han Zhang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17795">Pan-protein Design Learning Enables Task-adaptive Generalization for Low-resource Enzyme Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational protein design (CPD) offers transformative potential for bioengineering, but current deep CPD models, focused on universal domains, struggle with function-specific designs. This work introduces a novel CPD paradigm tailored for functional design tasks, particularly for enzymes-a key protein class often lacking specific application efficiency. To address structural data scarcity, we present CrossDesign, a domain-adaptive framework that leverages pretrained protein language models (PPLMs). By aligning protein structures with sequences, CrossDesign transfers pretrained knowledge to structure models, overcoming the limitations of limited structural data. The framework combines autoregressive (AR) and non-autoregressive (NAR) states in its encoder-decoder architecture, applying it to enzyme datasets and pan-proteins. Experimental results highlight CrossDesign's superior performance and robustness, especially with out-of-domain enzymes. Additionally, the model excels in fitness prediction when tested on large-scale mutation data, showcasing its stability.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2411.10720.pdf' target='_blank'>https://arxiv.org/pdf/2411.10720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anya Chauhan, Ayush Noori, Zhaozhi Li, Yingnan He, Michelle M Li, Marinka Zitnik, Sudeshna Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10720">Multi Scale Graph Neural Network for Alzheimer's Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer's disease (AD) is a complex, progressive neurodegenerative disorder characterized by extracellular A\b{eta} plaques, neurofibrillary tau tangles, glial activation, and neuronal degeneration, involving multiple cell types and pathways. Current models often overlook the cellular context of these pathways. To address this, we developed a multiscale graph neural network (GNN) model, ALZ PINNACLE, using brain omics data from donors spanning the entire aging to AD spectrum. ALZ PINNACLE is based on the PINNACLE GNN framework, which learns context-aware protein, cell type, and tissue representations within a unified latent space. ALZ PINNACLE was trained on 14,951 proteins, 206,850 protein interactions, 7 cell types, and 48 cell subtypes or states. After pretraining, we investigated the learned embedding of APOE, the largest genetic risk factor for AD, across different cell types. Notably, APOE embeddings showed high similarity in microglial, neuronal, and CD8 cells, suggesting a similar role of APOE in these cell types. Fine tuning the model on AD risk genes revealed cell type contexts predictive of the role of APOE in AD. Our results suggest that ALZ PINNACLE may provide a valuable framework for uncovering novel insights into AD neurobiology.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2410.11499.pdf' target='_blank'>https://arxiv.org/pdf/2410.11499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weixi Xiang, Xueting Han, Xiujuan Chai, Jing Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11499">BSM: Small but Powerful Biological Sequence Model for Genes and Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling biological sequences such as DNA, RNA, and proteins is crucial for understanding complex processes like gene regulation and protein synthesis. However, most current models either focus on a single type or treat multiple types of data separately, limiting their ability to capture cross-modal relationships. We propose that by learning the relationships between these modalities, the model can enhance its understanding of each type. To address this, we introduce BSM, a small but powerful mixed-modal biological sequence foundation model, trained on three types of data: RefSeq, Gene Related Sequences, and interleaved biological sequences from the web. These datasets capture the genetic flow, gene-protein relationships, and the natural co-occurrence of diverse biological data, respectively. By training on mixed-modal data, BSM significantly enhances learning efficiency and cross-modal representation, outperforming models trained solely on unimodal data. With only 110M parameters, BSM achieves performance comparable to much larger models across both single-modal and mixed-modal tasks, and uniquely demonstrates in-context learning capability for mixed-modal tasks, which is absent in existing models. Further scaling to 270M parameters demonstrates even greater performance gains, highlighting the potential of BSM as a significant advancement in multimodal biological sequence modeling.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2408.10247.pdf' target='_blank'>https://arxiv.org/pdf/2408.10247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Han Zhang, Qianqing Xu, An-Ping Zeng, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10247">MetaEnzyme: Meta Pan-Enzyme Learning for Task-Adaptive Redesign</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzyme design plays a crucial role in both industrial production and biology. However, this field faces challenges due to the lack of comprehensive benchmarks and the complexity of enzyme design tasks, leading to a dearth of systematic research. Consequently, computational enzyme design is relatively overlooked within the broader protein domain and remains in its early stages. In this work, we address these challenges by introducing MetaEnzyme, a staged and unified enzyme design framework. We begin by employing a cross-modal structure-to-sequence transformation architecture, as the feature-driven starting point to obtain initial robust protein representation. Subsequently, we leverage domain adaptive techniques to generalize specific enzyme design tasks under low-resource conditions. MetaEnzyme focuses on three fundamental low-resource enzyme redesign tasks: functional design (FuncDesign), mutation design (MutDesign), and sequence generation design (SeqDesign). Through novel unified paradigm and enhanced representation capabilities, MetaEnzyme demonstrates adaptability to diverse enzyme design tasks, yielding outstanding results. Wet lab experiments further validate these findings, reinforcing the efficacy of the redesign process.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2404.00837.pdf' target='_blank'>https://arxiv.org/pdf/2404.00837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahan Yoruc Selcuk, Xilin Yang, Bijie Bai, Yijie Zhang, Yuzhu Li, Musa Aydin, Aras Firat Unal, Aditya Gomatam, Zhen Guo, Darrow Morgan Angus, Goren Kolodney, Karine Atlan, Tal Keidar Haran, Nir Pillar, Aydogan Ozcan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00837">Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and Pyramid Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human epidermal growth factor receptor 2 (HER2) is a critical protein in cancer cell growth that signifies the aggressiveness of breast cancer (BC) and helps predict its prognosis. Accurate assessment of immunohistochemically (IHC) stained tissue slides for HER2 expression levels is essential for both treatment guidance and understanding of cancer mechanisms. Nevertheless, the traditional workflow of manual examination by board-certified pathologists encounters challenges, including inter- and intra-observer inconsistency and extended turnaround times. Here, we introduce a deep learning-based approach utilizing pyramid sampling for the automated classification of HER2 status in IHC-stained BC tissue images. Our approach analyzes morphological features at various spatial scales, efficiently managing the computational load and facilitating a detailed examination of cellular and larger-scale tissue-level details. This method addresses the tissue heterogeneity of HER2 expression by providing a comprehensive view, leading to a blind testing classification accuracy of 84.70%, on a dataset of 523 core images from tissue microarrays. Our automated system, proving reliable as an adjunct pathology tool, has the potential to enhance diagnostic precision and evaluation speed, and might significantly impact cancer treatment planning.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2511.07919.pdf' target='_blank'>https://arxiv.org/pdf/2511.07919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoonho Lee, Joseph Boen, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07919">Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2510.06396.pdf' target='_blank'>https://arxiv.org/pdf/2510.06396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymen Alsaadi, Jonathan Ash, Mikhail Titov, Matteo Turilli, Andre Merzky, Shantenu Jha, Sagar Khare
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06396">Adaptive Protein Design Protocols and Middleware</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational protein design is experiencing a transformation driven by AI/ML. However, the range of potential protein sequences and structures is astronomically vast, even for moderately sized proteins. Hence, achieving convergence between generated and predicted structures demands substantial computational resources for sampling. The Integrated Machine-learning for Protein Structures at Scale (IMPRESS) offers methods and advanced computing systems for coupling AI to high-performance computing tasks, enabling the ability to evaluate the effectiveness of protein designs as they are developed, as well as the models and simulations used to generate data and train models. This paper introduces IMPRESS and demonstrates the development and implementation of an adaptive protein design protocol and its supporting computing infrastructure. This leads to increased consistency in the quality of protein design and enhanced throughput of protein design due to dynamic resource allocation and asynchronous workload execution.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2507.09466.pdf' target='_blank'>https://arxiv.org/pdf/2507.09466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomas Geffner, Kieran Didi, Zhonglin Cao, Danny Reidenbach, Zuobai Zhang, Christian Dallago, Emine Kucukbenli, Karsten Kreis, Arash Vahdat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09466">La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, many generative models for de novo protein structure design have emerged. Yet, only few tackle the difficult task of directly generating fully atomistic structures jointly with the underlying amino acid sequence. This is challenging, for instance, because the model must reason over side chains that change in length during generation. We introduce La-Proteina for atomistic protein design based on a novel partially latent protein representation: coarse backbone structure is modeled explicitly, while sequence and atomistic details are captured via per-residue latent variables of fixed dimensionality, thereby effectively side-stepping challenges of explicit side-chain representations. Flow matching in this partially latent space then models the joint distribution over sequences and full-atom structures. La-Proteina achieves state-of-the-art performance on multiple generation benchmarks, including all-atom co-designability, diversity, and structural validity, as confirmed through detailed structural analyses and evaluations. Notably, La-Proteina also surpasses previous models in atomistic motif scaffolding performance, unlocking critical atomistic structure-conditioned protein design tasks. Moreover, La-Proteina is able to generate co-designable proteins of up to 800 residues, a regime where most baselines collapse and fail to produce valid samples, demonstrating La-Proteina's scalability and robustness.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2506.14853.pdf' target='_blank'>https://arxiv.org/pdf/2506.14853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Ku, Sun Sun, Hongyu Guo, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14853">DisProtEdit: Exploring Disentangled Representations for Multi-Attribute Protein Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DisProtEdit, a controllable protein editing framework that leverages dual-channel natural language supervision to learn disentangled representations of structural and functional properties. Unlike prior approaches that rely on joint holistic embeddings, DisProtEdit explicitly separates semantic factors, enabling modular and interpretable control. To support this, we construct SwissProtDis, a large-scale multimodal dataset where each protein sequence is paired with two textual descriptions, one for structure and one for function, automatically decomposed using a large language model. DisProtEdit aligns protein and text embeddings using alignment and uniformity objectives, while a disentanglement loss promotes independence between structural and functional semantics. At inference time, protein editing is performed by modifying one or both text inputs and decoding from the updated latent representation. Experiments on protein editing and representation learning benchmarks demonstrate that DisProtEdit performs competitively with existing methods while providing improved interpretability and controllability. On a newly constructed multi-attribute editing benchmark, the model achieves a both-hit success rate of up to 61.7%, highlighting its effectiveness in coordinating simultaneous structural and functional edits.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2505.24203.pdf' target='_blank'>https://arxiv.org/pdf/2505.24203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, AurÃ©lie Lozano, Vijil Chenthamarakshan, Payel Das, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24203">Aligning Protein Conformation Ensemble Generation with Physical Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein dynamics play a crucial role in protein biological functions and properties, and their traditional study typically relies on time-consuming molecular dynamics (MD) simulations conducted in silico. Recent advances in generative modeling, particularly denoising diffusion models, have enabled efficient accurate protein structure prediction and conformation sampling by learning distributions over crystallographic structures. However, effectively integrating physical supervision into these data-driven approaches remains challenging, as standard energy-based objectives often lead to intractable optimization. In this paper, we introduce Energy-based Alignment (EBA), a method that aligns generative models with feedback from physical models, efficiently calibrating them to appropriately balance conformational states based on their energy differences. Experimental results on the MD ensemble benchmark demonstrate that EBA achieves state-of-the-art performance in generating high-quality protein ensembles. By improving the physical plausibility of generated structures, our approach enhances model predictions and holds promise for applications in structural biology and drug discovery.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2503.00710.pdf' target='_blank'>https://arxiv.org/pdf/2503.00710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, Karsten Kreis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00710">Proteina: Scaling Flow-based Protein Structure Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to 5x as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2501.15492.pdf' target='_blank'>https://arxiv.org/pdf/2501.15492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michaela Cohrs, Shiwoo Koak, Yejin Lee, Yu Jin Sung, Wesley De Neve, Hristo L. Svilenov, Utku Ozbulak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15492">Color Flow Imaging Microscopy Improves Identification of Stress Sources of Protein Aggregates in Biopharmaceuticals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-based therapeutics play a pivotal role in modern medicine targeting various diseases. Despite their therapeutic importance, these products can aggregate and form subvisible particles (SvPs), which can compromise their efficacy and trigger immunological responses, emphasizing the critical need for robust monitoring techniques. Flow Imaging Microscopy (FIM) has been a significant advancement in detecting SvPs, evolving from monochrome to more recently incorporating color imaging. Complementing SvP images obtained via FIM, deep learning techniques have recently been employed successfully for stress source identification of monochrome SvPs. In this study, we explore the potential of color FIM to enhance the characterization of stress sources in SvPs. To achieve this, we curate a new dataset comprising 16,000 SvPs from eight commercial monoclonal antibodies subjected to heat and mechanical stress. Using both supervised and self-supervised convolutional neural networks, as well as vision transformers in large-scale experiments, we demonstrate that deep learning with color FIM images consistently outperforms monochrome images, thus highlighting the potential of color FIM in stress source classification compared to its monochrome counterparts.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2412.09380.pdf' target='_blank'>https://arxiv.org/pdf/2412.09380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglin Wang, Yucheng Zhou, Zijie Zhai, Jianbing Shen, Kai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09380">Diffusion Model with Representation Alignment for Protein Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein inverse folding is a fundamental problem in bioinformatics, aiming to recover the amino acid sequences from a given protein backbone structure. Despite the success of existing methods, they struggle to fully capture the intricate inter-residue relationships critical for accurate sequence prediction. We propose a novel method that leverages diffusion models with representation alignment (DMRA), which enhances diffusion-based inverse folding by (1) proposing a shared center that aggregates contextual information from the entire protein structure and selectively distributes it to each residue; and (2) aligning noisy hidden representations with clean semantic representations during the denoising process. This is achieved by predefined semantic representations for amino acid types and a representation alignment method that utilizes type embeddings as semantic feedback to normalize each residue. In experiments, we conduct extensive evaluations on the CATH4.2 dataset to demonstrate that DMRA outperforms leading methods, achieving state-of-the-art performance and exhibiting strong generalization capabilities on the TS50 and TS500 datasets.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2410.20354.pdf' target='_blank'>https://arxiv.org/pdf/2410.20354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaixi Zhang, Ruofan Jin, Kaidi Fu, Le Cong, Marinka Zitnik, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20354">FoldMark: Protecting Protein Generative Models with Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure is key to understanding protein function and is essential for progress in bioengineering, drug discovery, and molecular biology. Recently, with the incorporation of generative AI, the power and accuracy of computational protein structure prediction/design have been improved significantly. However, ethical concerns such as copyright protection and harmful content generation (biosecurity) pose challenges to the wide implementation of protein generative models. Here, we investigate whether it is possible to embed watermarks into protein generative models and their outputs for copyright authentication and the tracking of generated structures. As a proof of concept, we propose a two-stage method FoldMark as a generalized watermarking strategy for protein generative models. FoldMark first pretrain watermark encoder and decoder, which can minorly adjust protein structures to embed user-specific information and faithfully recover the information from the encoded structure. In the second step, protein generative models are fine-tuned with watermark-conditioned Low-Rank Adaptation (LoRA) modules to preserve generation quality while learning to generate watermarked structures with high recovery rates. Extensive experiments are conducted on open-source protein structure prediction models (e.g., ESMFold and MultiFlow) and de novo structure design models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method is effective across all these generative models. Meanwhile, our watermarking framework only exerts a negligible impact on the original protein structure quality and is robust under potential post-processing and adaptive attacks.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2410.18403.pdf' target='_blank'>https://arxiv.org/pdf/2410.18403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Chence Shi, Hongyu Guo, Yoshua Bengio, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18403">Structure Language Models for Protein Conformation Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins adopt multiple structural conformations to perform their diverse biological functions, and understanding these conformations is crucial for advancing drug discovery. Traditional physics-based simulation methods often struggle with sampling equilibrium conformations and are computationally expensive. Recently, deep generative models have shown promise in generating protein conformations as a more efficient alternative. However, these methods predominantly rely on the diffusion process within a 3D geometric space, which typically centers around the vicinity of metastable states and is often inefficient in terms of runtime. In this paper, we introduce Structure Language Modeling (SLM) as a novel framework for efficient protein conformation generation. Specifically, the protein structures are first encoded into a compact latent space using a discrete variational auto-encoder, followed by conditional language modeling that effectively captures sequence-specific conformation distributions. This enables a more efficient and interpretable exploration of diverse ensemble modes compared to existing methods. Based on this general framework, we instantiate SLM with various popular LM architectures as well as proposing the ESMDiff, a novel BERT-like structure language model fine-tuned from ESM3 with masked diffusion. We verify our approach in various scenarios, including the equilibrium dynamics of BPTI, conformational change pairs, and intrinsically disordered proteins. SLM provides a highly efficient solution, offering a 20-100x speedup than existing methods in generating diverse conformations, shedding light on promising avenues for future research.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2405.14545.pdf' target='_blank'>https://arxiv.org/pdf/2405.14545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongzhi Zhang, Xiuwen Gong, Shirui Pan, Jia Wu, Bo Du, Wenbin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14545">A Cross-Field Fusion Strategy for Drug-Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug-target interaction (DTI) prediction is a critical component of the drug discovery process. In the drug development engineering field, predicting novel drug-target interactions is extremely crucial.However, although existing methods have achieved high accuracy levels in predicting known drugs and drug targets, they fail to utilize global protein information during DTI prediction. This leads to an inability to effectively predict interaction the interactions between novel drugs and their targets. As a result, the cross-field information fusion strategy is employed to acquire local and global protein information. Thus, we propose the siamese drug-target interaction SiamDTI prediction method, which utilizes a double channel network structure for cross-field supervised learning.Experimental results on three benchmark datasets demonstrate that SiamDTI achieves higher accuracy levels than other state-of-the-art (SOTA) methods on novel drugs and targets.Additionally, SiamDTI's performance with known drugs and targets is comparable to that of SOTA approachs. The code is available at https://anonymous.4open.science/r/DDDTI-434D.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2404.15805.pdf' target='_blank'>https://arxiv.org/pdf/2404.15805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shujian Jiao, Bingxuan Li, Lei Wang, Xiaojin Zhang, Wei Chen, Jiajie Peng, Zhongyu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15805">Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are essential to life's processes, underpinning evolution and diversity. Advances in sequencing technology have revealed millions of proteins, underscoring the need for sophisticated pre-trained protein models for biological analysis and AI development. Facebook's ESM2, the most advanced protein language model to date, leverages a masked prediction task for unsupervised learning, crafting amino acid representations with notable biochemical accuracy. Yet, it lacks in delivering functional protein insights, signaling an opportunity for enhancing representation quality.Our study addresses this gap by incorporating protein family classification into ESM2's training.This approach, augmented with Community Propagation-Based Clustering Algorithm, improves global protein representations, while a contextual prediction task fine-tunes local amino acid accuracy. Significantly, our model achieved state-of-the-art results in several downstream experiments, demonstrating the power of combining global and local methodologies to substantially boost protein representation quality.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2403.15673.pdf' target='_blank'>https://arxiv.org/pdf/2403.15673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Bi, Sajib Acharjee Dip, Daniel Hajialigol, Sindhura Kommu, Hanwen Liu, Meng Lu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15673">AI for Biomedicine in the Era of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this survey, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2402.14789.pdf' target='_blank'>https://arxiv.org/pdf/2402.14789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14789">Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2402.10433.pdf' target='_blank'>https://arxiv.org/pdf/2402.10433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Lu, Zuobai Zhang, Bozitao Zhong, Chence Shi, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10433">Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a "zero-shot" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories above to become a target-specific sampler. Experimental results demonstrated the superior performance of such few-shot conformation sampler at a tractable computational cost.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2402.07955.pdf' target='_blank'>https://arxiv.org/pdf/2402.07955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, AurÃ©lie Lozano, Payel Das, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07955">ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein function annotation is an important yet challenging task in biology. Recent deep learning advancements show significant potential for accurate function prediction by learning from protein sequences and structures. Nevertheless, these predictor-based methods often overlook the modeling of protein similarity, an idea commonly employed in traditional approaches using sequence or structure retrieval tools. To fill this gap, we first study the effect of inter-protein similarity modeling by benchmarking retriever-based methods against predictors on protein function annotation tasks. Our results show that retrievers can match or outperform predictors without large-scale pre-training. Building on these insights, we introduce a novel variational pseudo-likelihood framework, ProtIR, designed to improve function predictors by incorporating inter-protein similarity modeling. This framework iteratively refines knowledge between a function predictor and retriever, thereby combining the strengths of both predictors and retrievers. ProtIR showcases around 10% improvement over vanilla predictor-based methods. Besides, it achieves performance on par with protein language model-based methods, yet without the need for massive pre-training, highlighting the efficacy of our framework. Code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2510.21608.pdf' target='_blank'>https://arxiv.org/pdf/2510.21608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oscar Davis, Michael S. Albergo, Nicholas M. Boffi, Michael M. Bronstein, Avishek Joey Bose
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21608">Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric data and purpose-built generative models on them have become ubiquitous in high-impact deep learning application domains, ranging from protein backbone generation and computational chemistry to geospatial data. Current geometric generative models remain computationally expensive at inference -- requiring many steps of complex numerical simulation -- as they are derived from dynamical measure transport frameworks such as diffusion and flow-matching on Riemannian manifolds. In this paper, we propose Generalised Flow Maps (GFM), a new class of few-step generative models that generalises the Flow Map framework in Euclidean spaces to arbitrary Riemannian manifolds. We instantiate GFMs with three self-distillation-based training methods: Generalised Lagrangian Flow Maps, Generalised Eulerian Flow Maps, and Generalised Progressive Flow Maps. We theoretically show that GFMs, under specific design decisions, unify and elevate existing Euclidean few-step generative models, such as consistency models, shortcut models, and meanflows, to the Riemannian setting. We benchmark GFMs against other geometric generative models on a suite of geometric datasets, including geospatial data, RNA torsion angles, and hyperbolic manifolds, and achieve state-of-the-art sample quality for single- and few-step evaluations, and superior or competitive log-likelihoods using the implicit probability flow.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2509.25379.pdf' target='_blank'>https://arxiv.org/pdf/2509.25379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogesh Verma, Markus Heinonen, Vikas Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25379">Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction and folding are fundamental to understanding biology, with recent deep learning advances reshaping the field. Diffusion-based generative models have revolutionized protein design, enabling the creation of novel proteins. However, these methods often neglect the intrinsic physical realism of proteins, driven by noising dynamics that lack grounding in physical principles. To address this, we first introduce a physically motivated non-linear noising process, grounded in classical physics, that unfolds proteins into secondary structures (e.g., alpha helices, linear beta sheets) while preserving topological integrity--maintaining bonds, and preventing collisions. We then integrate this process with the flow-matching paradigm on SE(3) to model the invariant distribution of protein backbones with high fidelity, incorporating sequence information to enable sequence-conditioned folding and expand the generative capabilities of our model. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel protein structures while accurately folding monomer sequences into precise protein conformations.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2509.23405.pdf' target='_blank'>https://arxiv.org/pdf/2509.23405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Zhangzhi Peng, Zachary Bezemek, Jarrid Rector-Brooks, Shuibai Zhang, Anru R. Zhang, Michael Bronstein, Avishek Joey Bose, Alexander Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23405">Planner Aware Path Learning in Diffusion Language Models Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion language models have emerged as a powerful alternative to autoregressive models, enabling fast inference through flexible and parallel generation paths. This flexibility is enabled by new sampling strategies, or planners, that iteratively choose where to denoise along the sequence rather than sampling uniformly at random. However, by modifying reverse paths, planners introduce a mismatch between the uniformly random denoising paths used during training and the planning-based paths used at inference. In this work, we systematically investigate this mismatch and theoretically show that the standard discrete diffusion training evidence lower bound (ELBO) does not accurately describe a denoiser under non-uniform planning. To bridge this gap, we derive a new Planned Evidence Lower Bound (P-ELBO) that directly incorporates planner-based reverse dynamics into the training objective. Building on this, we propose Planner Aware Path Learning (PAPL), a simple and effective modification of the standard masked discrete diffusion loss that aligns training and inference under planned denoisers. Empirically, PAPL delivers consistent improvements across domains, including a 40% relative gain in protein sequence modeling, up to a 4x improvement in MAUVE for text generation, and a 23% relative gain in HumanEval pass@10 for code generation.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.17224.pdf' target='_blank'>https://arxiv.org/pdf/2509.17224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Jing, Bonnie Berger, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17224">AI-based Methods for Simulating, Sampling, and Predicting Protein Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in deep learning have opened an era of abundant and accurate predicted protein structures; however, similar progress in protein ensembles has remained elusive. This review highlights several recent research directions towards AI-based predictions of protein ensembles, including coarse-grained force fields, generative models, multiple sequence alignment perturbation methods, and modeling of ensemble descriptors. An emphasis is placed on realistic assessments of the technological maturity of current methods, the strengths and weaknesses of broad families of techniques, and promising machine learning frameworks at an early stage of development. We advocate for "closing the loop" between model training, simulation, and inference to overcome challenges in training data availability and to enable the next generation of models.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.15796.pdf' target='_blank'>https://arxiv.org/pdf/2509.15796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Liu, Mingxuan Cao, Songhao Jiang, Xiao Luo, Xiaotian Duan, Mengdi Wang, Tobin R. Sosnick, Jinbo Xu, Rick Stevens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15796">Monte Carlo Tree Diffusion with Multiple Experts for Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2508.05006.pdf' target='_blank'>https://arxiv.org/pdf/2508.05006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youzhi Zhang, Yufei Li, Gaofeng Meng, Hongbin Liu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05006">The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein-Ligand Binding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a crucial aspect of drug discovery, as it predicts the binding interactions between small-molecule ligands and protein pockets. However, current multi-task learning models for docking often show inferior performance in ligand docking compared to protein pocket docking. This disparity arises largely due to the distinct structural complexities of ligands and proteins. To address this issue, we propose a novel game-theoretic framework that models the protein-ligand interaction as a two-player game called the Docking Game, with the ligand docking module acting as the ligand player and the protein pocket docking module as the protein player. To solve this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which alternately trains these players through a two-level loop. In the outer loop, the players exchange predicted poses, allowing each to incorporate the other's structural predictions, which fosters mutual adaptation over multiple iterations. In the inner loop, each player dynamically refines its predictions by incorporating its own predicted ligand or pocket poses back into its model. We theoretically show the convergence of LoopPlay, ensuring stable optimization. Extensive experiments conducted on public benchmark datasets demonstrate that LoopPlay achieves approximately a 10\% improvement in predicting accurate binding modes compared to previous state-of-the-art methods. This highlights its potential to enhance the accuracy of molecular docking in drug discovery.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2507.11115.pdf' target='_blank'>https://arxiv.org/pdf/2507.11115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haruya Imamura, Yasuaki Kobayashi, Yota Otachi, Toshiki Saitoh, Keita Sato, Asahi Takaoka, Ryo Yoshinaka, Tom C. van der Zanden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11115">Finding Order-Preserving Subgraphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>(Induced) Subgraph Isomorphism and Maximum Common (Induced) Subgraph are fundamental problems in graph pattern matching and similarity computation. In graphs derived from time-series data or protein structures, a natural total ordering of vertices often arises from their underlying structure, such as temporal sequences or amino acid sequences. This motivates the study of problem variants that respect this inherent ordering. This paper addresses Ordered (Induced) Subgraph Isomorphism (O(I)SI) and its generalization, Maximum Common Ordered (Induced) Subgraph (MCO(I)S), which seek to find subgraph isomorphisms that preserve the vertex orderings of two given ordered graphs. Our main contributions are threefold: (1) We prove that these problems remain NP-complete even when restricted to small graph classes, such as trees of depth 2 and threshold graphs. (2) We establish a gap in computational complexity between OSI and OISI on certain graph classes. For instance, OSI is polynomial-time solvable for interval graphs with their interval orderings, whereas OISI remains NP-complete under the same setting. (3) We demonstrate that the tractability of these problems can depend on the vertex ordering. For example, while OISI is NP-complete on threshold graphs, its generalization, MCOIS, can be solved in polynomial time if the specific vertex orderings that characterize the threshold graphs are provided.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2504.05784.pdf' target='_blank'>https://arxiv.org/pdf/2504.05784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paola F. Antonietti, Mattia Corti, Sergio GÃ³mez, Ilaria Perugia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05784">A structure-preserving LDG discretization of the Fisher-Kolmogorov equation for modeling neurodegenerative diseases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a structure-preserving, high-order, unconditionally stable numerical method for approximating the solution to the Fisher-Kolmogorov equation on polytopic meshes, with a particular focus on its application in simulating misfolded protein spreading in neurodegenerative diseases. The model problem is reformulated using an entropy variable to guarantee solution positivity, boundedness, and satisfaction of a discrete entropy-stability inequality at the numerical level. The scheme combines a local discontinuous Galerkin method on polytopal meshes for the space discretization with a $Î½$-step backward differentiation formula for the time integration. Implementation details are discussed, including a detailed derivation of the linear systems arising from Newton's iteration. The accuracy and robustness of the proposed method are demonstrated through extensive numerical tests. Finally, the method's practical performance is demonstrated through simulations of $Î±$-synuclein propagation in a two-dimensional brain geometry segmented from MRI data, providing a relevant computational framework for modeling synucleopathies (such as Parkinson's disease) and, more generally, neurodegenerative diseases.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2504.04770.pdf' target='_blank'>https://arxiv.org/pdf/2504.04770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Liu, Songhao Jiang, Chih-chan Tien, Jinbo Xu, Rick Stevens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04770">Bidirectional Hierarchical Protein Multi-Modal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural context. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. This bidirectional and hierarchical (Bi-Hierarchical) fusion approach leverages the strengths of both modalities to capture richer and more comprehensive protein representations. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including enzyme EC classification, model quality assessment, protein-ligand binding affinity prediction, protein-protein binding site prediction, and B cell epitopes prediction. Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of Bi-Hierarchical Fusion in bridging sequence and structural modalities.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2502.03540.pdf' target='_blank'>https://arxiv.org/pdf/2502.03540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Avishek Joey Bose, Alexander Tong, Pranam Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03540">Path Planning for Masked Diffusion Model Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Any order generation of discrete data using masked diffusion models (MDMs) offers a compelling alternative to traditional autoregressive models, especially in domains that lack a natural causal ordering of data. However, current popular MDMs depart from their successful continuous diffusion model counterparts with simplified masked inference wherein unmasked tokens cannot be iteratively refined -- even if there is a mistake. In this paper, we extract the full power of MDMs by introducing a novel inference sampling strategy termed Path Planning (P2) that decomposes each generation step into two sub-stages: planning and denoising. Under P2, the planner at every step selects appropriate tokens that are marked to be updated, which can then be sampled using the denoiser. We demonstrate that P2 generalizes all existing sampling strategies for MDMs and critically enhances generative quality through the new capability of refining and updating existing unmasked tokens. We theoretically prove that P2 establishes a (new) expanded evidence lower bound (ELBO) on the log marginal likelihood of data. We instantiate P2 with a family of planners including: 1.) Self-Planning, 2.) BERT-Planning, and 3.) Trained-Planning with a learned planner leading to SOTA generative performance for MDMs on a suite of domains. Specifically, solely using P2 inference, we observe relative improvements of 22% in protein sequence foldability, 8% in RNA sequence pLDDT, 4% in math reasoning, 68% in story generation (ROUGE score), and 33% in code generation for the challenging pass@1 metric.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2412.19661.pdf' target='_blank'>https://arxiv.org/pdf/2412.19661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentina Pederzoli, Mattia Corti, Davide Riccobelli, Paola F. Antonietti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19661">A coupled mathematical and numerical model for protein spreading and tissue atrophy, applied to Alzheimer's disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of this paper is to introduce, analyse and test in practice a new mathematical model describing the interplay between biological tissue atrophy driven by pathogen diffusion, with applications to neurodegenerative disorders. This study introduces a novel mathematical and computational model comprising a Fisher-Kolmogorov equation for species diffusion coupled with an elasticity equation governing mass loss. These equations intertwine through a logistic law dictating the reduction of the medium's mass. One potential application of this model lies in understanding the onset and development of Alzheimer's disease. Here, the equations can describe the propagation of misfolded tau-proteins and the ensuing brain atrophy characteristic of the disease. To address numerically the inherited complexities, we propose a Polygonal Discontinuous Galerkin method on polygonal/polyhedral grids for spatial discretization, while time integration relies on the theta-method. We present the mathematical model, delving into its characteristics and propose discretization applied. Furthermore, convergence results are presented to validate the model, accompanied by simulations illustrating the application scenario of the onset of Alzheimer's disease.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2410.20688.pdf' target='_blank'>https://arxiv.org/pdf/2410.20688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangxin Zhou, Jiaqi Guan, Yijia Zhang, Xingang Peng, Liang Wang, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20688">Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dual-target therapeutic strategies have become a compelling approach and attracted significant attention due to various benefits, such as their potential in overcoming drug resistance in cancer therapy. Considering the tremendous success that deep generative models have achieved in structure-based drug design in recent years, we formulate dual-target drug design as a generative task and curate a novel dataset of potential target pairs based on synergistic drug combinations. We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs. Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process. Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner. We also repurpose linker design methods as strong baselines for this task. Extensive experiments demonstrate the effectiveness of our method compared with various baselines.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2410.08134.pdf' target='_blank'>https://arxiv.org/pdf/2410.08134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jarrid Rector-Brooks, Mohsin Hasan, Zhangzhi Peng, Zachary Quinn, Chenghao Liu, Sarthak Mittal, Nouha Dziri, Michael Bronstein, Yoshua Bengio, Pranam Chatterjee, Alexander Tong, Avishek Joey Bose
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08134">Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling of discrete data underlies important applications spanning text-based agents like ChatGPT to the design of the very building blocks of life in protein sequences. However, application domains need to exert control over the generated data by steering the generative process - typically via RLHF - to satisfy a specified property, reward, or affinity metric. In this paper, we study the problem of steering Masked Diffusion Models (MDMs), a recent class of discrete diffusion models that offer a compelling alternative to traditional autoregressive models. We introduce Discrete Denoising Posterior Prediction (DDPP), a novel framework that casts the task of steering pre-trained MDMs as a problem of probabilistic inference by learning to sample from a target Bayesian posterior. Our DDPP framework leads to a family of three novel objectives that are all simulation-free, and thus scalable while applying to general non-differentiable reward functions. Empirically, we instantiate DDPP by steering MDMs to perform class-conditional pixel-level image modeling, RLHF-based alignment of MDMs using text-based rewards, and finetuning protein language models to generate more diverse secondary structures and shorter proteins. We substantiate our designs via wet-lab validation, where we observe transient expression of reward-optimized protein sequences.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2410.07974.pdf' target='_blank'>https://arxiv.org/pdf/2410.07974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanqi Du, Michael Plainer, Rob Brekelmans, Chenru Duan, Frank NoÃ©, Carla P. Gomes, AlÃ¡n Aspuru-Guzik, Kirill Neklyudov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07974">Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's h-transform. However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's h-transform as an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2410.03553.pdf' target='_blank'>https://arxiv.org/pdf/2410.03553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03553">Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2410.00709.pdf' target='_blank'>https://arxiv.org/pdf/2410.00709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Liu, Songhao Jiang, Xiaotian Duan, Archit Vasan, Chong Liu, Chih-chan Tien, Heng Ma, Thomas Brettin, Fangfang Xia, Ian T. Foster, Rick L. Stevens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00709">Binding Affinity Prediction: From Conventional to Machine Learning-Based Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-ligand binding is the process by which a small molecule (drug or inhibitor) attaches to a target protein. The binding affinity, which refers to the strength of this interaction, is central to many important problems in bioinformatics such as drug design. An extensive amount of work has been devoted to predicting binding affinity over the past decades due to its significance. In this paper, we review all significant recent works, focusing on the methods, features, and benchmark datasets. We have observed a rising trend in the use of traditional machine learning and deep learning models for predicting binding affinity, accompanied by an increasing amount of data on proteins and small drug-like molecules. While prediction results are constantly improving, we also identify several open questions and potential directions that remain unexplored in the field. This paper could serve as an excellent starting point for machine learning researchers who wish to engage in the study of binding affinity, or for anyone with general interests in machine learning, drug discovery, and bioinformatics.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2406.06419.pdf' target='_blank'>https://arxiv.org/pdf/2406.06419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Berghaus, Kostadin Cvejoski, Patrick Seifner, Cesar Ojeda, Ramses J. Sanchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06419">Foundation Inference Models for Markov Jump Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observation process. Second, a neural network model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that one and the same (pretrained) model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are finetuned to the target datasets.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2405.20313.pdf' target='_blank'>https://arxiv.org/pdf/2405.20313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, Alexander Tong, Avishek Joey Bose
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20313">Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2401.15747.pdf' target='_blank'>https://arxiv.org/pdf/2401.15747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paola F. Antonietti, Mattia Corti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15747">Numerical modelling of protein misfolding in neurodegenerative diseases: a computational study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spreading of misfolded proteins is a known hallmark in some neurodegenerative diseases, known as proteinopathies. A significant example is the tau protein, associated with many pathologies, such as Alzheimer's. In this work, we discuss and compare two different models for the mathematical modelling of protein misfolding, namely the heterodimer model and the Fisher-Kolmogorov model, as well as their numerical discretizations. We introduce a discontinuous Galerkin method on polygonal and polyhedral grids for space discretization to accurately simulate the wavefronts typically observed in the prionic spreading. Starting from the semidiscrete formulations, we use a Crank-Nicolson scheme to advance in time. Finally, we simulate the spreading of the misfolded tau protein in a two-dimensional brain slice in the sagittal plane with a polygonal agglomerated grid. The simulation is performed using both the presented models, and we compare the results and the differences deriving from the modelling choices.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2512.02033.pdf' target='_blank'>https://arxiv.org/pdf/2512.02033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijun Gao, Mutian He, Shijia Sun, Hanqun Cao, Jingjie Zhang, Zihao Luo, Xiaorui Wang, Xiaojun Yao, Chang-Yu Hsieh, Chunbin Gu, Pheng Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02033">CONFIDE: Hallucination Assessment for Reliable Biomolecular Structure Prediction and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable evaluation of protein structure predictions remains challenging, as metrics like pLDDT capture energetic stability but often miss subtle errors such as atomic clashes or conformational traps reflecting topological frustration within the protein folding energy landscape. We present CODE (Chain of Diffusion Embeddings), a self evaluating metric empirically found to quantify topological frustration directly from the latent diffusion embeddings of the AlphaFold3 series of structure predictors in a fully unsupervised manner. Integrating this with pLDDT, we propose CONFIDE, a unified evaluation framework that combines energetic and topological perspectives to improve the reliability of AlphaFold3 and related models. CODE strongly correlates with protein folding rates driven by topological frustration, achieving a correlation of 0.82 compared to pLDDT's 0.33 (a relative improvement of 148\%). CONFIDE significantly enhances the reliability of quality evaluation in molecular glue structure prediction benchmarks, achieving a Spearman correlation of 0.73 with RMSD, compared to pLDDT's correlation of 0.42, a relative improvement of 73.8\%. Beyond quality assessment, our approach applies to diverse drug design tasks, including all-atom binder design, enzymatic active site mapping, mutation induced binding affinity prediction, nucleic acid aptamer screening, and flexible protein modeling. By combining data driven embeddings with theoretical insight, CODE and CONFIDE outperform existing metrics across a wide range of biomolecular systems, offering robust and versatile tools to refine structure predictions, advance structural biology, and accelerate drug discovery.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2511.06449.pdf' target='_blank'>https://arxiv.org/pdf/2511.06449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Cai, Xinyuan Guo, Yu Pei, JiangTao Feng, Jiangjie Chen, Ya-Qin Zhang, Wei-Ying Ma, Mingxuan Wang, Hao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06449">FLEX: Continuous Agent Evolution via Forward Learning from Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2510.23492.pdf' target='_blank'>https://arxiv.org/pdf/2510.23492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjie Zhang, Hanqun Cao, Zijun Gao, Yu Wang, Shaoning Li, Jun Xu, Cheng Tan, Jun Zhu, Chang-Yu Hsieh, Chunbin Gu, Pheng Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23492">Learning the PTM Code through a Coarse-to-Fine, Mechanism-Aware Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Post-translational modifications (PTMs) form a combinatorial "code" that regulates protein function, yet deciphering this code - linking modified sites to their catalytic enzymes - remains a central unsolved problem in understanding cellular signaling and disease. We introduce COMPASS-PTM, a mechanism-aware, coarse-to-fine learning framework that unifies residue-level PTM profiling with enzyme-substrate assignment. COMPASS-PTM integrates evolutionary representations from protein language models with physicochemical priors and a crosstalk-aware prompting mechanism that explicitly models inter-PTM dependencies. This design allows the model to learn biologically coherent patterns of cooperative and antagonistic modifications while addressing the dual long-tail distribution of PTM data. Across multiple proteome-scale benchmarks, COMPASS-PTM establishes new state-of-the-art performance, including a 122% relative F1 improvement in multi-label site prediction and a 54% gain in zero-shot enzyme assignment. Beyond accuracy, the model demonstrates interpretable generalization, recovering canonical kinase motifs and predicting disease-associated PTM rewiring caused by missense variants. By bridging statistical learning with biochemical mechanism, COMPASS-PTM unifies site-level and enzyme-level prediction into a single framework that learns the grammar underlying protein regulation and signaling.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2510.05747.pdf' target='_blank'>https://arxiv.org/pdf/2510.05747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Ma, Hongzong Li, Ye-Fan Hu, Jian-Dong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05747">Physicochemically Informed Dual-Conditioned Generative Model of T-Cell Receptor Variable Regions for Cellular Therapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physicochemically informed biological sequence generation has the potential to accelerate computer-aided cellular therapy, yet current models fail to \emph{jointly} ensure novelty, diversity, and biophysical plausibility when designing variable regions of T-cell receptors (TCRs). We present \textbf{PhysicoGPTCR}, a large generative protein Transformer that is \emph{dual-conditioned} on peptide and HLA context and trained to autoregressively synthesise TCR sequences while embedding residue-level physicochemical descriptors. The model is optimised on curated TCR--peptide--HLA triples with a maximum-likelihood objective and compared against ANN, GPTCR, LSTM, and VAE baselines. Across multiple neoantigen benchmarks, PhysicoGPTCR substantially improves edit-distance, similarity, and longest-common-subsequence scores, while populating a broader region of sequence space. Blind in-silico docking and structural modelling further reveal a higher proportion of binding-competent clones than the strongest baseline, validating the benefit of explicit context conditioning and physicochemical awareness. Experimental results demonstrate that dual-conditioned, physics-grounded generative modelling enables end-to-end design of functional TCR candidates, reducing the discovery timeline from months to minutes without sacrificing wet-lab verifiability.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2507.07032.pdf' target='_blank'>https://arxiv.org/pdf/2507.07032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqun Cao, Xinyi Zhou, Zijun Gao, Chenyu Wang, Xin Gao, Zhi Zhang, Cesar de la Fuente-Nunez, Chunbin Gu, Ge Liu, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07032">Lightweight MSA Design Advances Protein Folding From Evolutionary Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction often hinges on multiple sequence alignments (MSAs), which underperform on low-homology and orphan proteins. We introduce PLAME, a lightweight MSA design framework that leverages evolutionary embeddings from pretrained protein language models to generate MSAs that better support downstream folding. PLAME couples these embeddings with a conservation--diversity loss that balances agreement on conserved positions with coverage of plausible sequence variation. Beyond generation, we develop (i) an MSA selection strategy to filter high-quality candidates and (ii) a sequence-quality metric that is complementary to depth-based measures and predictive of folding gains. On AlphaFold2 low-homology/orphan benchmarks, PLAME delivers state-of-the-art improvements in structure accuracy (e.g., lDDT/TM-score), with consistent gains when paired with AlphaFold3. Ablations isolate the benefits of the selection strategy, and case studies elucidate how MSA characteristics shape AlphaFold confidence and error modes. Finally, we show PLAME functions as a lightweight adapter, enabling ESMFold to approach AlphaFold2-level accuracy while retaining ESMFold-like inference speed. PLAME thus provides a practical path to high-quality folding for proteins lacking strong evolutionary neighbors.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2504.10283.pdf' target='_blank'>https://arxiv.org/pdf/2504.10283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Cheng, Jiahan Li, Jiajun Fan, Ge Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10283">$Î±$-Flow: A Unified Framework for Continuous-State Discrete Flow Matching Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ significantly in their representations and geometric assumptions. This work presents a unified framework for CS-DFM models, under which the existing variants can be understood as operating on different $Î±$-representations of probabilities. Building upon the theory of information geometry, we introduce $Î±$-Flow, a family of CS-DFM models that adheres to the canonical $Î±$-geometry of the statistical manifold, and demonstrate its optimality in minimizing the generalized kinetic energy. Theoretically, we show that the flow matching loss for $Î±$-flow establishes a unified variational bound for the discrete negative log-likelihood. We comprehensively evaluate different instantiations of $Î±$-flow on various discrete generation domains to demonstrate their effectiveness in discrete generative modeling, including intermediate values whose geometries have never been explored before. $Î±$-flow significantly outperforms its discrete-state counterpart in image and protein sequence generation and better captures the entropy in language modeling.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2411.10548.pdf' target='_blank'>https://arxiv.org/pdf/2411.10548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter St. John, Dejun Lin, Polina Binder, Malcolm Greaves, Vega Shah, John St. John, Adrian Lange, Patrick Hsu, Rajesh Illango, Arvind Ramanathan, Anima Anandkumar, David H Brookes, Akosua Busia, Abhishaike Mahajan, Stephen Malina, Neha Prasad, Sam Sinai, Lindsay Edwards, Thomas Gaudelet, Cristian Regep, Martin Steinegger, Burkhard Rost, Alexander Brace, Kyle Hippe, Luca Naef, Keisuke Kamata, George Armstrong, Kevin Boyd, Zhonglin Cao, Han-Yi Chou, Simon Chu, Allan dos Santos Costa, Sajad Darabi, Eric Dawson, Kieran Didi, Cong Fu, Mario Geiger, Michelle Gill, Darren J Hsu, Gagan Kaushik, Maria Korshunova, Steven Kothen-Hill, Youhan Lee, Meng Liu, Micha Livne, Zachary McClure, Jonathan Mitchell, Alireza Moradzadeh, Ohad Mosafi, Youssef Nashed, Saee Paliwal, Yuxing Peng, Sara Rabhi, Farhad Ramezanghorbani, Danny Reidenbach, Camir Ricketts, Brian C Roland, Kushal Shah, Tyler Shimko, Hassan Sirelkhatim, Savitha Srinivasan, Abraham C Stern, Dorota Toczydlowska, Srimukh Prasad Veccham, NiccolÃ² Alberto Elia Venanzi, Anton Vorontsov, Jared Wilber, Isabel Wilkinson, Wei Jing Wong, Eva Xue, Cory Ye, Xin Yu, Yang Zhang, Guoqing Zhou, Becca Zandstein, Alejandro Chacon, Prashant Sohani, Maximilian Stadler, Christian Hundt, Feiwen Zhu, Christian Dallago, Bruno Trentini, Emine Kucukbenli, Saee Paliwal, Timur Rvachov, Eddie Calleja, Johnny Israeli, Harry Clifford, Risto Haukioja, Nicholas Haemel, Kyle Tretina, Neha Tadimeti, Anthony B Costa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10548">BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence models encoding biology and chemistry are opening new routes to high-throughput and high-quality in-silico drug development. However, their training increasingly relies on computational scale, with recent protein language models (pLM) training on hundreds of graphical processing units (GPUs). We introduce the BioNeMo Framework to facilitate the training of computational biology and chemistry AI models across hundreds of GPUs. Its modular design allows the integration of individual components, such as data loaders, into existing workflows and is open to community contributions. We detail technical features of the BioNeMo Framework through use cases such as pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains a three billion parameter BERT-based pLM on over one trillion tokens in 4.2 days. The BioNeMo Framework is open-source and free for everyone to use.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2410.18070.pdf' target='_blank'>https://arxiv.org/pdf/2410.18070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luran Wang, Chaoran Cheng, Yizhen Liao, Yanru Qu, Ge Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18070">Training Free Guided Flow Matching with Optimal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2410.14946.pdf' target='_blank'>https://arxiv.org/pdf/2410.14946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqun Cao, Mutian He, Ning Ma, Chang-yu Hsieh, Chunbin Gu, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14946">DEL-Ranking: Ranking-Correction Denoising Framework for Elucidating Molecular Affinities in DNA-Encoded Libraries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA-encoded library (DEL) screening has revolutionized the detection of protein-ligand interactions through read counts, enabling rapid exploration of vast chemical spaces. However, noise in read counts, stemming from nonspecific interactions, can mislead this exploration process. We present DEL-Ranking, a novel distribution-correction denoising framework that addresses these challenges. Our approach introduces two key innovations: (1) a novel ranking loss that rectifies relative magnitude relationships between read counts, enabling the learning of causal features determining activity levels, and (2) an iterative algorithm employing self-training and consistency loss to establish model coherence between activity label and read count predictions. Furthermore, we contribute three new DEL screening datasets, the first to comprehensively include multi-dimensional molecular representations, protein-ligand enrichment values, and their activity labels. These datasets mitigate data scarcity issues in AI-driven DEL screening research. Rigorous evaluation on diverse DEL datasets demonstrates DEL-Ranking's superior performance across multiple correlation metrics, with significant improvements in binding affinity prediction accuracy. Our model exhibits zero-shot generalization ability across different protein targets and successfully identifies potential motifs determining compound binding affinity. This work advances DEL screening analysis and provides valuable resources for future research in this area.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2410.04060.pdf' target='_blank'>https://arxiv.org/pdf/2410.04060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ignacio Hounie, Charilaos Kanatsoulis, Arnuv Tandon, Alejandro Ribeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04060">LoRTA: Low Rank Tensor Adaptation of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning (PEFT) method that effectively adapts large pre-trained models for downstream tasks. LoRA parameterizes model updates using low-rank matrices at each layer, significantly reducing the number of trainable parameters and, consequently, resource requirements during fine-tuning. However, the lower bound on the number of trainable parameters remains high due to the use of the low-rank matrix model. Recent works have addressed this limitation by proposing low rank tensor parameterizations for model updates. However, they only exploit redundancy across layers, or tensorize individual matrices using ad-hoc schemes that introduce additional hyperparameters. In this work, we propose a higher-order Candecomp/Parafac (CP) decomposition, enabling a more compact and flexible representation compared to existing matrix and tensor based PEFT methods. Our experiments on Natural Language Understanding, Instruction Tuning, Preference Optimization and Protein Folding benchmarks demonstrate that our method can achieve a reduction in the number of parameters while maintaining comparable performance.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2405.13964.pdf' target='_blank'>https://arxiv.org/pdf/2405.13964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Yuan, Youyuan Zhang, Can Chen, Haolun Wu, Zixuan Li, Jianmo Li, James J. Clark, Xue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13964">Design Editing for Offline Model-based Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparameters, DEMOs score is competitive with the best previously reported scores in the literature.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2511.01698.pdf' target='_blank'>https://arxiv.org/pdf/2511.01698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Kang, Ziyu Su, Tianyang Wang, Zaibo Li, Wei Chen, Muhammad Khalid Khan Niazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01698">Progressive Translation of H&E to IHC with Enhanced Structural Fidelity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not only maintains the structural features of tissue samples, but also provides high-resolution protein localization, which is essential for aiding in pathology diagnosis. Despite its diagnostic value, IHC remains a costly and labor-intensive technique. Its limited scalability and constraints in multiplexing further hinder widespread adoption, especially in resource-limited settings. Consequently, researchers are increasingly exploring computational stain translation techniques to synthesize IHC-equivalent images from H&E-stained slides, aiming to extract protein-level information more efficiently and cost-effectively. However, most existing stain translation techniques rely on a linearly weighted summation of multiple loss terms within a single objective function, strategy that often overlooks the interdepedence among these components-resulting in suboptimal image quality and an inability to simultaneously preserve structural authenticity and color fidelity. To address this limitation, we propose a novel network architecture that follows a progressive structure, incorporating color and cell border generation logic, which enables each visual aspect to be optimized in a stage-wise and decoupled manner. To validate the effectiveness of our proposed network architecture, we build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We introduce additional loss functions based on 3,3'-diaminobenzidine (DAB) chromogen concentration and image gradient, enhancing color fidelity and cell boundary clarity in the generated IHC images. By reconstructing the generation pipeline using our structure-color-cell boundary progressive mechanism, experiments on HER2 and ER datasets demonstrated that the model significantly improved visual quality and achieved finer structural details.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2509.24779.pdf' target='_blank'>https://arxiv.org/pdf/2509.24779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kacper KapuÅniak, Cristian Gabellini, Michael Bronstein, Prudencio Tossou, Francesco Di Giovanni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24779">MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular Dynamics (MD) is a powerful computational microscope for probing protein functions. However, the need for fine-grained integration and the long timescales of biomolecular events make MD computationally expensive. To address this, several generative models have been proposed to generate surrogate trajectories at lower cost. Yet, these models typically learn a fixed-lag transition density, causing the training signal to be dominated by frequent but uninformative transitions. We introduce a new class of generative models, MSM Emulators, which instead learn to sample transitions across discrete states defined by an underlying Markov State Model (MSM). We instantiate this class with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two orders of magnitude speedup compared to implicit- or explicit-solvent MD simulations. We benchmark Mars-FM ability to reproduce MD statistics through structural observables such as RMSD, radius of gyration, and secondary structure content. Our evaluation spans protein domains (up to 500 residues) with significant chemical and structural diversity, including unfolding events, and enforces strict sequence dissimilarity between training and test sets to assess generalization. Across all metrics, MarS-FM outperforms existing methods, often by a substantial margin.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2509.18480.pdf' target='_blank'>https://arxiv.org/pdf/2509.18480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh Susskind, Miguel Angel Bautista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18480">SimpleFold: Folding Proteins is Simpler than You Think</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2509.15242.pdf' target='_blank'>https://arxiv.org/pdf/2509.15242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaydeep Rade, Md Hasibul Hasan Hasib, Meric Ozturk, Baboucarr Faal, Sheng Yang, Dipali G. Sashital, Vincenzo Venditti, Baoyu Chen, Soumik Sarkar, Adarsh Krishnamurthy, Anwesha Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15242">ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues. Experimental techniques like Cryo-EM are accurate but costly and time-consuming. We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction. However, generating a large-scale AFM imaging data set sufficient to train deep learning models is impractical. Therefore, we developed a virtual AFM framework that simulates the imaging process and generated a dataset of ~542,000 proteins with multi-view synthetic AFM images. We train a conditional diffusion model to synthesize novel views from unposed inputs and an instance-specific Neural Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D protein structures achieve an average Chamfer Distance within the AFM imaging resolution, reflecting high structural fidelity. Our method is extensively validated on experimental AFM images of various PCs, demonstrating strong potential for accurate, cost-effective protein complex structure prediction and rapid iterative validation using AFM experiments.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2506.19820.pdf' target='_blank'>https://arxiv.org/pdf/2506.19820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Faltings, Hannes Stark, Regina Barzilay, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19820">ProxelGen: Generating Proteins as 3D Densities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop ProxelGen, a protein structure generative model that operates on 3D densities as opposed to the prevailing 3D point cloud representations. Representing proteins as voxelized densities, or proxels, enables new tasks and conditioning capabilities. We generate proteins encoded as proxels via a 3D CNN-based VAE in conjunction with a diffusion model operating on its latent space. Compared to state-of-the-art models, ProxelGen's samples achieve higher novelty, better FID scores, and the same level of designability as the training set. ProxelGen's advantages are demonstrated in a standard motif scaffolding benchmark, and we show how 3D density-based generation allows for more flexible shape conditioning.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2503.20913.pdf' target='_blank'>https://arxiv.org/pdf/2503.20913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuyuan Hu, Guoqing Liu, Can Chen, Yang Zhao, Hao Zhang, Xue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20913">TransDiffSBDD: Causality-Aware Multi-Modal Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) is a critical task in drug discovery, requiring the generation of molecular information across two distinct modalities: discrete molecular graphs and continuous 3D coordinates. However, existing SBDD methods often overlook two key challenges: (1) the multi-modal nature of this task and (2) the causal relationship between these modalities, limiting their plausibility and performance. To address both challenges, we propose TransDiffSBDD, an integrated framework combining autoregressive transformers and diffusion models for SBDD. Specifically, the autoregressive transformer models discrete molecular information, while the diffusion model samples continuous distributions, effectively resolving the first challenge. To address the second challenge, we design a hybrid-modal sequence for protein-ligand complexes that explicitly respects the causality between modalities. Experiments on the CrossDocked2020 benchmark demonstrate that TransDiffSBDD outperforms existing baselines.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2503.01203.pdf' target='_blank'>https://arxiv.org/pdf/2503.01203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01203">Hypergraph Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 10 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.3\%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2409.13361.pdf' target='_blank'>https://arxiv.org/pdf/2409.13361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumukh Pinge, Weihong Xu, Wout Bittremieux, Niema Moshiri, Sang-Woo Jun, Tajana Rosing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13361">RapidOMS: FPGA-based Open Modification Spectral Library Searching with HD Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mass spectrometry (MS) is essential for protein analysis but faces significant challenges with large datasets and complex post-translational modifications, resulting in difficulties in spectral identification. Open Modification Search (OMS) improves the analysis of these modifications. We present RapidOMS, a solution leveraging the Samsung SmartSSD, which integrates SSD and FPGA in a near-storage configuration to minimize data movement and enhance the efficiency of large-scale database searching. RapidOMS employs hyperdimensional computing (HDC), a brain-inspired, high-dimensional data processing approach, exploiting the parallel processing and low-latency capabilities of FPGAs, making it well-suited for MS. Utilizing the parallelism and efficiency of bitwise operations in HDC, RapidOMS delivers up to a 60x speedup over the state-of-the-art (SOTA) CPU tool ANN-Solo and is 2.72x faster than the GPU tool HyperOMS. Furthermore, RapidOMS achieves an 11x improvement in energy efficiency compared to conventional systems, providing scalable, energy-efficient solutions for large-scale proteomics applications and advancing the efficient processing of proteomic data.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2408.06244.pdf' target='_blank'>https://arxiv.org/pdf/2408.06244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaydeep Rade, Ethan Herron, Soumik Sarkar, Anwesha Sarkar, Adarsh Krishnamurthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06244">3D Reconstruction of Protein Structures from Multi-view AFM Images using Neural Radiance Fields (NeRFs)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in deep learning for predicting 3D protein structures have shown promise, particularly when leveraging inputs like protein sequences and Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often fall short when predicting the structures of protein complexes (PCs), which involve multiple proteins. In our study, we investigate using atomic force microscopy (AFM) combined with deep learning to predict the 3D structures of PCs. AFM generates height maps that depict the PCs in various random orientations, providing a rich information for training a neural network to predict the 3D structures. We then employ the pre-trained UpFusion model (which utilizes a conditional diffusion model for synthesizing novel views) to train an instance-specific NeRF model for 3D reconstruction. The performance of UpFusion is evaluated through zero-shot predictions of 3D protein structures using AFM images. The challenge, however, lies in the time-intensive and impractical nature of collecting actual AFM images. To address this, we use a virtual AFM imaging process that transforms a `PDB' protein file into multi-view 2D virtual AFM images via volume rendering techniques. We extensively validate the UpFusion architecture using both virtual and actual multi-view AFM images. Our results include a comparison of structures predicted with varying numbers of views and different sets of views. This novel approach holds significant potential for enhancing the accuracy of protein complex structure predictions with further fine-tuning of the UpFusion network.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2406.19755.pdf' target='_blank'>https://arxiv.org/pdf/2406.19755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tan, Lirong Zheng, Bozitao Zhong, Liang Hong, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19755">Protein Representation Learning with Sequence Information Embedding: Does it Always Lead to a Better Performance?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has become a crucial tool in studying proteins. While the significance of modeling protein structure has been discussed extensively in the literature, amino acid types are typically included in the input as a default operation for many inference tasks. This study demonstrates with structure alignment task that embedding amino acid types in some cases may not help a deep learning model learn better representation. To this end, we propose ProtLOCA, a local geometry alignment method based solely on amino acid structure representation. The effectiveness of ProtLOCA is examined by a global structure-matching task on protein pairs with an independent test dataset based on CATH labels. Our method outperforms existing sequence- and structure-based representation learning methods by more quickly and accurately matching structurally consistent protein domains. Furthermore, in local structure pairing tasks, ProtLOCA for the first time provides a valid solution to highlight common local structures among proteins with different overall structures but the same function. This suggests a new possibility for using deep learning methods to analyze protein structure to infer function.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2406.05766.pdf' target='_blank'>https://arxiv.org/pdf/2406.05766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijia Song, Zelin Zang, Yelin Wang, Guozheng Yang, Kaicheng yu, Wanyu Chen, Miaoyu Wang, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05766">Set-CLIP: Exploring Aligned Semantic From Low-Alignment Multimodal Data Through A Distribution View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion breaks through the boundaries between diverse modalities and has already achieved notable performances. However, in many specialized fields, it is struggling to obtain sufficient alignment data for training, which seriously limits the use of previously effective models. Therefore, semi-supervised learning approaches are attempted to facilitate multimodal alignment by learning from low-alignment data with fewer matched pairs, but traditional techniques like pseudo-labeling may run into troubles in the label-deficient scenarios. To tackle these challenges, we reframe semi-supervised multimodal alignment as a manifold matching issue and propose a new methodology based on CLIP, termed Set-CLIP. Specifically, by designing a novel semantic density distribution loss, we constrain the latent representation distribution with fine granularity and extract implicit semantic alignment from unpaired multimodal data, thereby reducing the reliance on numerous strictly matched pairs. Furthermore, we apply coarse-grained modality adaptation and unimodal self-supervised guidance to narrow the gaps between modality spaces and improve the stability of representation distributions. Extensive experiments conducted on a range of tasks in various fields, including protein analysis, remote sensing, and the general vision-language field, validate the efficacy of our proposed Set-CLIP method. Especially with no paired data for supervised training, Set-CLIP is still outstanding, which brings an improvement of 144.83% over CLIP.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2405.17802.pdf' target='_blank'>https://arxiv.org/pdf/2405.17802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanle Mo, Xin Hong, Bowen Gao, Yinjun Jia, Yanyan Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17802">Multi-level Interaction Modeling for Protein Mutational Effect Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions are central mediators in many biological processes. Accurately predicting the effects of mutations on interactions is crucial for guiding the modulation of these interactions, thereby playing a significant role in therapeutic development and drug discovery. Mutations generally affect interactions hierarchically across three levels: mutated residues exhibit different sidechain conformations, which lead to changes in the backbone conformation, eventually affecting the binding affinity between proteins. However, existing methods typically focus only on sidechain-level interaction modeling, resulting in suboptimal predictions. In this work, we propose a self-supervised multi-level pre-training framework, ProMIM, to fully capture all three levels of interactions with well-designed pretraining objectives. Experiments show ProMIM outperforms all the baselines on the standard benchmark, especially on mutations where significant changes in backbone conformations may occur. In addition, leading results from zero-shot evaluations for SARS-CoV-2 mutational effect prediction and antibody optimization underscore the potential of ProMIM as a powerful next-generation tool for developing novel therapeutic approaches and new drugs.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2405.12868.pdf' target='_blank'>https://arxiv.org/pdf/2405.12868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, Wenbing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12868">Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to represent and simulate the dynamics of physical systems is a crucial yet challenging task. Existing equivariant Graph Neural Network (GNN) based methods have encapsulated the symmetry of physics, \emph{e.g.}, translations, rotations, etc, leading to better generalization ability. Nevertheless, their frame-to-frame formulation of the task overlooks the non-Markov property mainly incurred by unobserved dynamics in the environment. In this paper, we reformulate dynamics simulation as a spatio-temporal prediction task, by employing the trajectory in the past period to recover the Non-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to fulfill our purpose. At its core, we design a novel Equivariant Discrete Fourier Transform (EDFT) to extract periodic patterns from the history frames, and then construct an Equivariant Spatial Module (ESM) to accomplish spatial message passing, and an Equivariant Temporal Module (ETM) with the forward attention and equivariant pooling mechanisms to aggregate temporal message. We evaluate our model on three real datasets corresponding to the molecular-, protein- and macro-level. Experimental results verify the effectiveness of ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2404.16866.pdf' target='_blank'>https://arxiv.org/pdf/2404.16866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohao Yuan, Songyou Li, Geyan Ye, Yikun Zhang, Long-Kai Huang, Wenbing Huang, Wei Liu, Jianhua Yao, Yu Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16866">Annotation-guided Protein Design with Multi-Level Domain Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The core challenge of de novo protein design lies in creating proteins with specific functions or properties, guided by certain conditions. Current models explore to generate protein using structural and evolutionary guidance, which only provide indirect conditions concerning functions and properties. However, textual annotations of proteins, especially the annotations for protein domains, which directly describe the protein's high-level functionalities, properties, and their correlation with target amino acid sequences, remain unexplored in the context of protein design tasks. In this paper, we propose Protein-Annotation Alignment Generation, PAAG, a multi-modality protein design framework that integrates the textual annotations extracted from protein database for controllable generation in sequence space. Specifically, within a multi-level alignment module, PAAG can explicitly generate proteins containing specific domains conditioned on the corresponding domain annotations, and can even design novel proteins with flexible combinations of different kinds of annotations. Our experimental results underscore the superiority of the aligned protein representations from PAAG over 7 prediction tasks. Furthermore, PAAG demonstrates a significant increase in generation success rate (24.7% vs 4.7% in zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison to the existing model. We anticipate that PAAG will broaden the horizons of protein design by leveraging the knowledge from between textual annotation and proteins.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2402.18396.pdf' target='_blank'>https://arxiv.org/pdf/2402.18396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Corso, Arthur Deng, Benjamin Fry, Nicholas Polizzi, Regina Barzilay, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18396">Deep Confident Steps to New Pockets: Strategies for Docking Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks. Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2402.04997.pdf' target='_blank'>https://arxiv.org/pdf/2402.04997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04997">Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2512.15930.pdf' target='_blank'>https://arxiv.org/pdf/2512.15930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Sinclair, Moeen Meigooni, Archit Vasan, Ozan Gokdemir, Xinran Lian, Heng Ma, Yadu Babuji, Alexander Brace, Khalid Hossain, Carlo Siebenschuh, Thomas Brettin, Kyle Chard, Christopher Henry, Venkatram Vishwanath, Rick L. Stevens, Ian T. Foster, Arvind Ramanathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15930">Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them "undruggable". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2511.22640.pdf' target='_blank'>https://arxiv.org/pdf/2511.22640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22640">Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2509.25035.pdf' target='_blank'>https://arxiv.org/pdf/2509.25035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Zheng, Xinyang Liu, Cindy Xiangrui Kong, Nan Jiang, Zheyuan Hu, Weijian Luo, Wei Deng, Guang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25035">Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and high-quality language generation is the holy grail that people pursue in the age of AI. In this work, we introduce Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that initializes from a pre-trained (masked) discrete diffusion language model (dLLM) and distills a few-step student for fast generation. The resulting DiDi-Instruct model achieves comparable or superior performance to its dLLM teacher and the GPT-2 baseline while enabling up to 64$\times$ acceleration. The theoretical foundation of DiDi-Instruct is a novel framework based on integral KL-divergence minimization, which yields a practical training algorithm. We further introduce grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler that significantly improve training stability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct achieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms prior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible entropy loss (around $1\%$) and reduce additional training wall-clock time by more than $20\times$ compared to competing dLLM distillation methods. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at github.com/haoyangzheng-ai/didi-instruct.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2509.21655.pdf' target='_blank'>https://arxiv.org/pdf/2509.21655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Ren, Wenhao Gao, Lexing Ying, Grant M. Rotskoff, Jiequn Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21655">DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study inference-time scaling for diffusion models, where the goal is to adapt a pre-trained model to new target distributions without retraining. Existing guidance-based methods are simple but introduce bias, while particle-based corrections suffer from weight degeneracy and high computational cost. We introduce DriftLite, a lightweight, training-free particle-based approach that steers the inference dynamics on the fly with provably optimal stability control. DriftLite exploits a previously unexplored degree of freedom in the Fokker-Planck equation between the drift and particle potential, and yields two practical instantiations: Variance- and Energy-Controlling Guidance (VCG/ECG) for approximating the optimal drift with minimal overhead. Across Gaussian mixture models, particle systems, and large-scale protein-ligand co-folding problems, DriftLite consistently reduces variance and improves sample quality over pure guidance and sequential Monte Carlo baselines. These results highlight a principled, efficient route toward scalable inference-time adaptation of diffusion models.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2508.18236.pdf' target='_blank'>https://arxiv.org/pdf/2508.18236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Tang, Arash Lagzian, Srinivas Anumasa, Qiran Zou, Yingtao Zhu, Ye Zhang, Trang Nguyen, Yih-Chung Tham, Ehsan Adeli, Ching-Yu Cheng, Yilun Du, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18236">Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2507.07887.pdf' target='_blank'>https://arxiv.org/pdf/2507.07887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Achuth Chandrasekhar, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07887">Automating MD simulations for Proteins using Large language Models: NAMD-Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2507.06366.pdf' target='_blank'>https://arxiv.org/pdf/2507.06366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupu Zhang, Zelin Xu, Tingsong Xiao, Gustavo Seabra, Yanjun Li, Chenglong Li, Zhe Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06366">DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the binding affinity of protein-ligand complexes plays a vital role in drug discovery. Unfortunately, progress has been hindered by the lack of large-scale and high-quality binding affinity labels. The widely used PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning, especially graph contrastive learning (GCL), provides a unique opportunity to break the barrier by pre-training graph neural network models based on vast unlabeled complexes and fine-tuning the models on much fewer labeled complexes. However, the problem faces unique challenges, including a lack of a comprehensive unlabeled dataset with well-defined positive/negative complex pairs and the need to design GCL algorithms that incorporate the unique characteristics of such data. To fill the gap, we propose DecoyDB, a large-scale, structure-aware dataset specifically designed for self-supervised GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground truth complexes (less than 2.5 Angstrom) and diverse decoy structures with computationally generated binding poses that range from realistic to suboptimal (negative pairs). Each decoy is annotated with a Root Mean Squared Deviation (RMSD) from the native pose. We further design a customized GCL framework to pre-train graph neural networks based on DecoyDB and fine-tune the models with labels from PDBbind. Extensive experiments confirm that models pre-trained with DecoyDB achieve superior accuracy, label efficiency, and generalizability.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2507.02006.pdf' target='_blank'>https://arxiv.org/pdf/2507.02006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shakya Jayakody, Youpeng Zhao, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02006">AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph convolutional networks (GCNs) are fundamental in various scientific applications, ranging from biomedical protein-protein interactions (PPI) to large-scale recommendation systems. An essential component for modeling graph structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As the size of graph data continues to scale up, SpGEMMs are often conducted in an out-of-core fashion due to limited GPU memory space in resource-constrained systems. Albeit recent efforts that aim to alleviate the memory constraints of out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory layout, or performing the computation in sparse format, current systems suffer from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where sparse format data alignment and memory allocation are the main performance bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the algorithm angle, AIRES proposes to alleviate the data alignment issues on the block level for matrices in sparse formats and develops a tiling algorithm to facilitate row block-wise alignment. On the system level, AIRES employs a three-phase dynamic scheduling that features a dual-way data transfer strategy utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage (GDS), and host memory to reduce I/O latency and improve throughput. Evaluations show that AIRES significantly outperforms the state-of-the-art methods, achieving up to 1.8x lower latency in real-world graph processing benchmarks.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2506.05864.pdf' target='_blank'>https://arxiv.org/pdf/2506.05864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiakai Zhang, Shouchen Zhou, Haizhao Dai, Xinhang Liu, Peihao Wang, Zhiwen Fan, Yuan Pei, Jingyi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05864">CryoFastAR: Fast Cryo-EM Ab Initio Reconstruction Made Easy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pose estimation from unordered images is fundamental for 3D reconstruction, robotics, and scientific imaging. Recent geometric foundation models, such as DUSt3R, enable end-to-end dense 3D reconstruction but remain underexplored in scientific imaging fields like cryo-electron microscopy (cryo-EM) for near-atomic protein reconstruction. In cryo-EM, pose estimation and 3D reconstruction from unordered particle images still depend on time-consuming iterative optimization, primarily due to challenges such as low signal-to-noise ratios (SNR) and distortions from the contrast transfer function (CTF). We introduce CryoFastAR, the first geometric foundation model that can directly predict poses from Cryo-EM noisy images for Fast ab initio Reconstruction. By integrating multi-view features and training on large-scale simulated cryo-EM data with realistic noise and CTF modulations, CryoFastAR enhances pose estimation accuracy and generalization. To enhance training stability, we propose a progressive training strategy that first allows the model to extract essential features under simpler conditions before gradually increasing difficulty to improve robustness. Experiments show that CryoFastAR achieves comparable quality while significantly accelerating inference over traditional iterative approaches on both synthetic and real datasets.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2503.06340.pdf' target='_blank'>https://arxiv.org/pdf/2503.06340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Wang, Samin Karim, Yuan Hong, Binghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06340">Backdoor Attacks on Discrete Graph Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models are powerful generative models in continuous data domains such as image and video data. Discrete graph diffusion models (DGDMs) have recently extended them for graph generation, which are crucial in fields like molecule and protein modeling, and obtained the SOTA performance. However, it is risky to deploy DGDMs for safety-critical applications (e.g., drug discovery) without understanding their security vulnerabilities. In this work, we perform the first study on graph diffusion models against backdoor attacks, a severe attack that manipulates both the training and inference/generation phases in graph diffusion models. We first define the threat model, under which we design the attack such that the backdoored graph diffusion model can generate 1) high-quality graphs without backdoor activation, 2) effective, stealthy, and persistent backdoored graphs with backdoor activation, and 3) graphs that are permutation invariant and exchangeable--two core properties in graph generative models. 1) and 2) are validated via empirical evaluations without and with backdoor defenses, while 3) is validated via theoretical results.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2502.06173.pdf' target='_blank'>https://arxiv.org/pdf/2502.06173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanket Jantre, Tianle Wang, Gilchan Park, Kriti Chopra, Nicholas Jeon, Xiaoning Qian, Nathan M. Urban, Byung-Jun Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06173">Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2412.03791.pdf' target='_blank'>https://arxiv.org/pdf/2412.03791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Wang, Anurag Ranjan, Josh Susskind, Miguel Angel Bautista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03791">INRFlow: Flow Matching for INRs in Ambient Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on irregular or unstructured data like 3D point clouds or even protein structures. These models are commonly trained in two stages: first, a data compressor is trained, and in a subsequent training stage a flow matching generative model is trained in the latent space of the data compressor. This two-stage paradigm sets obstacles for unifying models across data domains, as hand-crafted compressors architectures are used for different data modalities. To this end, we introduce INRFlow, a domain-agnostic approach to learn flow matching transformers directly in ambient space. Drawing inspiration from INRs, we introduce a conditionally independent point-wise training objective that enables INRFlow to make predictions continuously in coordinate space. Our empirical results demonstrate that INRFlow effectively handles different data modalities such as images, 3D point clouds and protein structure data, achieving strong performance in different domains and outperforming comparable approaches. INRFlow is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2410.10447.pdf' target='_blank'>https://arxiv.org/pdf/2410.10447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabin Schieffer, Ivy Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10447">Accelerating Drug Discovery in AutoDock-GPU with Tensor Cores</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In drug discovery, molecular docking aims at characterizing the binding of a drug-like molecule to a macromolecule. AutoDock-GPU, a state-of-the-art docking software, estimates the geometrical conformation of a docked ligand-protein complex by minimizing a scoring function. Our profiling results indicate that the current reduction operation that is heavily used in the scoring function is sub-optimal. Thus, we developed a method to accelerate the sum reduction of four-element vectors using matrix operations on NVIDIA Tensor Cores. We integrated the new reduction operation into AutoDock-GPU and evaluated it on multiple chemical complexes on three GPUs. Our results show that our method for reduction operation is 4-7 times faster than the AutoDock-GPU baseline. We also evaluated the impact of our method on the overall simulation time in the real-world docking simulation and achieved a 27% improvement on the average docking time.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2408.00521.pdf' target='_blank'>https://arxiv.org/pdf/2408.00521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengdan Fan, Wei Zhang, Haiyan Zhao, Zhi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00521">A new approach for encoding code and assisting code understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Some companies (e.g., Microsoft Research and Google DeepMind) have discovered some of the limitations of GPTs' autoregressive paradigm next-word prediction, manifested in the model's lack of planning, working memory, backtracking, and reasoning skills. GPTs rely on a local and greedy process of generating the next word, without a global understanding of the task or the output. We have confirmed the above limitations through specialized empirical studies of code comprehension. Although GPT-4 is good at producing fluent and coherent text, it cannot handle complex logic and generate new code that hasn't been seen, and it relies too much on the formatting of the prompt to generate the correct code. We propose a new paradigm for code understanding that goes beyond the next-word prediction paradigm, inspired by the successful application of diffusion techniques to image generation (Dalle-2, Sora) and protein structure generation (AlphaFold-3), which have no autoregressive constraints. Instead of encoding the code in a form that mimics natural language, we encode the code as a heterogeneous image paradigm with a memory of global information that mimics both images and protein structures. We then refer to Sora's CLIP upstream text-to-image encoder model to design a text-to-code encoder model that can be applied to various downstream code understanding tasks. The model learns the global understanding of code under the new paradigm heterogeneous image, connects the encoding space of text and code, and encodes the input of text into the vector of code most similar to it. Using self-supervised comparative learning on 456,360 text-code pairs, the model achieved a zero-shot prediction of new data. This work is the basis for future work on code generation using diffusion techniques under a new paradigm to avoid autoregressive limitations.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2510.04377.pdf' target='_blank'>https://arxiv.org/pdf/2510.04377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarui Li, Zixiang Yin, Zhengming Ding, Samuel J. Landry, Ramgopal R. Mettu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04377">TCR-EML: Explainable Model Layers for TCR-pMHC Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2510.03365.pdf' target='_blank'>https://arxiv.org/pdf/2510.03365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhi Chawla, David M. Bortz, Vanja Dukic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03365">Bias and Coverage Properties of the WENDy-IRLS Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently proposed class of parameter estimation algorithms that exhibits notable noise robustness and computational efficiency. This work examines the coverage and bias properties of the original WENDy-IRLS algorithm's parameter and state estimators in the context of the following differential equations: Logistic, Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark. The estimators' performance was studied in simulated data examples, under four different noise distributions (normal, log-normal, additive censored normal, and additive truncated normal), and a wide range of noise, reaching levels much higher than previously tested for this algorithm.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2509.24933.pdf' target='_blank'>https://arxiv.org/pdf/2509.24933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian W. Ober, Calvin McCarter, Aniruddh Raghu, Yucen Lily Li, Alan N. Amin, Andrew Gordon Wilson, Hunter Elliott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24933">Is Sequence Information All You Need for Bayesian Optimization of Antibodies?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian optimization is a natural candidate for the engineering of antibody therapeutic properties, which is often iterative and expensive. However, finding the optimal choice of surrogate model for optimization over the highly structured antibody space is difficult, and may differ depending on the property being optimized. Moreover, to the best of our knowledge, no prior works have attempted to incorporate structural information into antibody Bayesian optimization. In this work, we explore different approaches to incorporating structural information into Bayesian optimization, and compare them to a variety of sequence-only approaches on two different antibody properties, binding affinity and stability. In addition, we propose the use of a protein language model-based ``soft constraint,'' which helps guide the optimization to promising regions of the space. We find that certain types of structural information improve data efficiency in early optimization rounds for stability, but have equivalent peak performance. Moreover, when incorporating the protein language model soft constraint we find that the data efficiency gap is diminished for affinity and eliminated for stability, resulting in sequence-only methods that match the performance of structure-based methods, raising questions about the necessity of structure in Bayesian optimization for antibodies.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.19604.pdf' target='_blank'>https://arxiv.org/pdf/2509.19604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Xin, Aniruddh Raghu, Nick Bhattacharya, Adam Carr, Melanie Montgomery, Hunter Elliott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19604">Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern therapeutic antibody design often involves composing multi-part assemblages of individual functional domains, each of which may be derived from a different source or engineered independently. While these complex formats can expand disease applicability and improve safety, they present a significant engineering challenge: the function and stability of individual domains are not guaranteed in the novel format, and the entire molecule may no longer be synthesizable. To address these challenges, we develop a machine learning framework to predict "reformatting success" -- whether converting an antibody from one format to another will succeed or not. Our framework incorporates both antibody sequence and structural context, incorporating an evaluation protocol that reflects realistic deployment scenarios. In experiments on a real-world antibody reformatting dataset, we find the surprising result that large pretrained protein language models (PLMs) fail to outperform simple, domain-tailored, multimodal representations. This is particularly evident in the most difficult evaluation setting, where we test model generalization to a new starting antibody. In this challenging "new antibody, no data" scenario, our best multimodal model achieves high predictive accuracy, enabling prioritization of promising candidates and reducing wasted experimental effort.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2509.16357.pdf' target='_blank'>https://arxiv.org/pdf/2509.16357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniruddh Raghu, Sebastian Ober, Maxwell Kazman, Hunter Elliott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16357">Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Therapeutic antibody candidates often require extensive engineering to improve key functional and developability properties before clinical development. This can be achieved through iterative design, where starting molecules are optimized over several rounds of in vitro experiments. While protein structure can provide a strong inductive bias, it is rarely used in iterative design due to the lack of structural data for continually evolving lead molecules over the course of optimization. In this work, we propose a strategy for iterative antibody optimization that leverages both sequence and structure as well as accumulating lab measurements of binding and developability. Building on prior work, we first train a sequence-structure diffusion generative model that operates on antibody-antigen complexes. We then outline an approach to use this model, together with carefully predicted antibody-antigen complexes, to optimize lead candidates throughout the iterative design process. Further, we describe a guided sampling approach that biases generation toward desirable properties by integrating models trained on experimental data from iterative design. We evaluate our approach in multiple in silico and in vitro experiments, demonstrating that it produces high-affinity binders at multiple stages of an active antibody optimization campaign.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2507.11839.pdf' target='_blank'>https://arxiv.org/pdf/2507.11839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyue Gong, Xinshi Chen, Yuxuan Zhang, Yuxuan Song, Hao Zhou, Wenzhi Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11839">Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight inference is critical for biomolecular structure prediction and other downstream tasks, enabling efficient real-world deployment and inference-time scaling for large-scale applications. In this work, we address the challenge of balancing model efficiency and prediction accuracy by making several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step ODE sampler, significantly reducing computational overhead for the diffusion module part during inference; 2) In the open-source Protenix framework, a subset of pairformer or diffusion transformer blocks doesn't make contributions to the final structure prediction, presenting opportunities for architectural pruning and lightweight redesign; 3) A model incorporating an ESM module is trained to substitute the conventional MSA module, reducing MSA preprocessing time. Building on these key insights, we present Protenix-Mini, a compact and optimized model designed for efficient protein structure prediction. This streamlined version incorporates a more efficient architectural design with a two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating redundant Transformer components and refining the sampling process, Protenix-Mini significantly reduces model complexity with slight accuracy drop. Evaluations on benchmark datasets demonstrate that it achieves high-fidelity predictions, with only a negligible 1 to 5 percent decrease in performance on benchmark datasets compared to its full-scale counterpart. This makes Protenix-Mini an ideal choice for applications where computational resources are limited but accurate structure prediction remains crucial.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2507.07426.pdf' target='_blank'>https://arxiv.org/pdf/2507.07426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zerui Yang, Yuwei Wan, Siyu Yan, Yudai Matsuda, Tong Xie, Bram Hoex, Linqi Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07426">DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug repositioning. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repositioning. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug repositioning.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2506.11420.pdf' target='_blank'>https://arxiv.org/pdf/2506.11420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqiao Song, Tiaoxiao Li, Lei Li, Martin Renqiang Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11420">PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2506.09332.pdf' target='_blank'>https://arxiv.org/pdf/2506.09332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqiao Song, Ramith Hettiarachchi, Chuan Li, Jianwen Xie, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09332">Natural Language Guided Ligand-Binding Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can AI protein models follow human language instructions and design proteins with desired functions (e.g. binding to a ligand)? Designing proteins that bind to a given ligand is crucial in a wide range of applications in biology and chemistry. Most prior AI models are trained on protein-ligand complex data, which is scarce due to the high cost and time requirements of laboratory experiments. In contrast, there is a substantial body of human-curated text descriptions about protein-ligand interactions and ligand formula. In this paper, we propose InstructPro, a family of protein generative models that follow natural language instructions to design ligand-binding proteins. Given a textual description of the desired function and a ligand formula in SMILES, InstructPro generates protein sequences that are functionally consistent with the specified instructions. We develop the model architecture, training strategy, and a large-scale dataset, InstructProBench, to support both training and evaluation. InstructProBench consists of 9,592,829 triples of (function description, ligand formula, protein sequence). We train two model variants: InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion parameters). Both variants consistently outperform strong baselines, including ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking success rate (81.52% at moderate confidence) and the lowest average root mean square deviation (RMSD) compared to ground truth structures (4.026Ã). InstructPro-3B further descreases the average RMSD to 2.527Ã, demonstrating InstructPro's ability to generate ligand-binding proteins that align with the functional specifications.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2502.06027.pdf' target='_blank'>https://arxiv.org/pdf/2502.06027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Chen, Bo Peng, Tianhua Zhai, Daniel Adu-Ampratwum, Xia Ning
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06027">Generating 3D Binding Molecules Using Shape-Conditioned Diffusion Models with Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug development is a critical but notoriously resource- and time-consuming process. In this manuscript, we develop a novel generative artificial intelligence (genAI) method DiffSMol to facilitate drug development. DiffSmol generates 3D binding molecules based on the shapes of known ligands. DiffSMol encapsulates geometric details of ligand shapes within pre-trained, expressive shape embeddings and then generates new binding molecules through a diffusion model. DiffSMol further modifies the generated 3D structures iteratively via shape guidance to better resemble the ligand shapes. It also tailors the generated molecules toward optimal binding affinities under the guidance of protein pockets. Here, we show that DiffSMol outperforms the state-of-the-art methods on benchmark datasets. When generating binding molecules resembling ligand shapes, DiffSMol with shape guidance achieves a success rate 61.4%, substantially outperforming the best baseline (11.2%), meanwhile producing molecules with novel molecular graph structures. DiffSMol with pocket guidance also outperforms the best baseline in binding affinities by 13.2%, and even by 17.7% when combined with shape guidance. Case studies for two critical drug targets demonstrate very favorable physicochemical and pharmacokinetic properties of the generated molecules, thus, the potential of DiffSMol in developing promising drug candidates.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2412.15790.pdf' target='_blank'>https://arxiv.org/pdf/2412.15790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zhang, Di Huang, Yixin Chen, Fuhai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15790">GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of multi-omic data is pivotal for understanding complex diseases, but its high dimensionality and noise present significant challenges. Graph Neural Networks (GNNs) offer a robust framework for analyzing large-scale signaling pathways and protein-protein interaction networks, yet they face limitations in expressivity when capturing intricate biological relationships. To address this, we propose Graph Sequence Language Model (GraphSeqLM), a framework that enhances GNNs with biological sequence embeddings generated by Large Language Models (LLMs). These embeddings encode structural and biological properties of DNA, RNA, and proteins, augmenting GNNs with enriched features for analyzing sample-specific multi-omic data. By integrating topological, sequence-derived, and biological information, GraphSeqLM demonstrates superior predictive accuracy and outperforms existing methods, paving the way for more effective multi-omic data integration in precision medicine.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2412.11137.pdf' target='_blank'>https://arxiv.org/pdf/2412.11137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hezha O. Rasul, Dlzar D. Ghafour, Bakhtyar K. Aziz, Bryar A. Hassan, Tarik A. Rashid, Arif Kivrak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11137">Decoding Drug Discovery: Exploring A-to-Z In silico Methods for Beginners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The drug development process is a critical challenge in the pharmaceutical industry due to its time-consuming nature and the need to discover new drug potentials to address various ailments. The initial step in drug development, drug target identification, often consumes considerable time. While valid, traditional methods such as in vivo and in vitro approaches are limited in their ability to analyze vast amounts of data efficiently, leading to wasteful outcomes. To expedite and streamline drug development, an increasing reliance on computer-aided drug design (CADD) approaches has merged. These sophisticated in silico methods offer a promising avenue for efficiently identifying viable drug candidates, thus providing pharmaceutical firms with significant opportunities to uncover new prospective drug targets. The main goal of this work is to review in silico methods used in the drug development process with a focus on identifying therapeutic targets linked to specific diseases at the genetic or protein level. This article thoroughly discusses A-to-Z in silico techniques, which are essential for identifying the targets of bioactive compounds and their potential therapeutic effects. This review intends to improve drug discovery processes by illuminating the state of these cutting-edge approaches, thereby maximizing the effectiveness and duration of clinical trials for novel drug target investigation.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2410.24022.pdf' target='_blank'>https://arxiv.org/pdf/2410.24022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang He, Peiran Jin, Yaosen Min, Shufang Xie, Lijun Wu, Tao Qin, Xiaozhuan Liang, Kaiyuan Gao, Yuliang Jiang, Tie-Yan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24022">SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced Protein Sequence Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins, essential to biological systems, perform functions intricately linked to their three-dimensional structures. Understanding the relationship between protein structures and their amino acid sequences remains a core challenge in protein modeling. While traditional protein foundation models benefit from pre-training on vast unlabeled datasets, they often struggle to capture critical co-evolutionary information, which evolutionary-based methods excel at. In this study, we introduce a novel pre-training strategy for protein foundation models that emphasizes the interactions among amino acid residues to enhance the extraction of both short-range and long-range co-evolutionary features from sequence data. Trained on a large-scale protein sequence dataset, our model demonstrates superior generalization ability, outperforming established baselines of similar size, including the ESM model, across diverse downstream tasks. Experimental results confirm the model's effectiveness in integrating co-evolutionary information, marking a significant step forward in protein sequence-based modeling.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2410.02847.pdf' target='_blank'>https://arxiv.org/pdf/2410.02847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiexin Qin, Mengxu Zhu, Chunyang Li, Terry Lyons, Hong Yan, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02847">Deep Signature: Characterization of Large-Scale Molecular Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding protein dynamics are essential for deciphering protein functional mechanisms and developing molecular therapies. However, the complex high-dimensional dynamics and interatomic interactions of biological processes pose significant challenge for existing computational techniques. In this paper, we approach this problem for the first time by introducing Deep Signature, a novel computationally tractable framework that characterizes complex dynamics and interatomic interactions based on their evolving trajectories. Specifically, our approach incorporates soft spectral clustering that locally aggregates cooperative dynamics to reduce the size of the system, as well as signature transform that collects iterated integrals to provide a global characterization of the non-smooth interactive dynamics. Theoretical analysis demonstrates that Deep Signature exhibits several desirable properties, including invariance to translation, near invariance to rotation, equivariance to permutation of atomic coordinates, and invariance under time reparameterization. Furthermore, experimental results on three benchmarks of biological processes verify that our approach can achieve superior performance compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2405.08205.pdf' target='_blank'>https://arxiv.org/pdf/2405.08205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Wengong Jin, Yang Yang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08205">Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes are genetically encoded biocatalysts capable of accelerating chemical reactions. How can we automatically design functional enzymes? In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families. Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function. These sites are automatically mined from enzyme databases. EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss. We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB). Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity. These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2405.06693.pdf' target='_blank'>https://arxiv.org/pdf/2405.06693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenqiao Song, Tinglin Huang, Lei Li, Wengong Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06693">SurfPro: Functional Protein Design Based on Continuous Surface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can we design proteins with desired functions? We are motivated by a chemical intuition that both geometric structure and biochemical properties are critical to a protein's function. In this paper, we propose SurfPro, a new method to generate functional proteins given a desired surface and its associated biochemical properties. SurfPro comprises a hierarchical encoder that progressively models the geometric shape and biochemical features of a protein surface, and an autoregressive decoder to produce an amino acid sequence. We evaluate SurfPro on a standard inverse folding benchmark CATH 4.2 and two functional protein design tasks: protein binder design and enzyme design. Our SurfPro consistently surpasses previous state-of-the-art inverse folding methods, achieving a recovery rate of 57.78% on CATH 4.2 and higher success rates in terms of protein-protein binding and enzyme-substrate interaction scores.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2402.07242.pdf' target='_blank'>https://arxiv.org/pdf/2402.07242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Boccato, Matteo Ferrante, Nicola Toschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07242">A Differentiable Model for Optimizing the Genetic Drivers of Synaptogenesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is growing consensus among neuroscientists that neural circuits critical for survival are the result of genomic decompression processes. We introduce SynaptoGen, a novel computational framework--member of the Connectome Models family--bringing synthetic biological intelligence closer, facilitating neural biological agent development through precise genetic control of synaptogenesis. SynaptoGen is the first model of its kind offering mechanistic explanation of synaptic multiplicity based on genetic expression and protein interaction probabilities. The framework connects genetic factors through a differentiable function, working as a neural network where synaptic weights equal average numbers of synapses between neurons, multiplied by conductance, derived from genetic profiles. Differentiability enables gradient-based optimization, allowing generation of genetic expression patterns producing pre-wired biological agents for specific tasks. Validation in simulated synaptogenesis scenarios shows agents successfully solving four reinforcement learning benchmarks, consistently surpassing control baselines. Despite gaps in biological realism requiring mitigation, this framework has potential to accelerate synthetic biological intelligence research.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2510.27671.pdf' target='_blank'>https://arxiv.org/pdf/2510.27671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhang, Zekun Guo, Yingce Xia, Peiran Jin, Shufang Xie, Tao Qin, Xiang-Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27671">MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD), which maps target proteins to candidate molecular ligands, is a fundamental task in drug discovery. Effectively aligning protein structural representations with molecular representations, and ensuring alignment between generated drugs and their pharmacological properties, remains a critical challenge. To address these challenges, we propose MolChord, which integrates two key techniques: (1) to align protein and molecule structures with their textual descriptions and sequential representations (e.g., FASTA for proteins and SMILES for molecules), we leverage NatureLM, an autoregressive model unifying text, small molecules, and proteins, as the molecule generator, alongside a diffusion-based structure encoder; and (2) to guide molecules toward desired properties, we curate a property-aware dataset by integrating preference data and refine the alignment process using Direct Preference Optimization (DPO). Experimental results on CrossDocked2020 demonstrate that our approach achieves state-of-the-art performance on key evaluation metrics, highlighting its potential as a practical tool for SBDD.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2510.21052.pdf' target='_blank'>https://arxiv.org/pdf/2510.21052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel M. Steinberg, Asiri Wijesinghe, Rafael Oliveira, Piotr Koniusz, Cheng Soon Ong, Edwin V. Bonilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21052">Amortized Active Generation of Pareto Sets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce active generation of Pareto sets (A-GPS), a new framework for online discrete black-box multi-objective optimization (MOO). A-GPS learns a generative model of the Pareto set that supports a-posteriori conditioning on user preferences. The method employs a class probability estimator (CPE) to predict non-dominance relations and to condition the generative model toward high-performing regions of the search space. We also show that this non-dominance CPE implicitly estimates the probability of hypervolume improvement (PHVI). To incorporate subjective trade-offs, A-GPS introduces preference direction vectors that encode user-specified preferences in objective space. At each iteration, the model is updated using both Pareto membership and alignment with these preference directions, producing an amortized generative model capable of sampling across the Pareto front without retraining. The result is a simple yet powerful approach that achieves high-quality Pareto set approximations, avoids explicit hypervolume computation, and flexibly captures user preferences. Empirical results on synthetic benchmarks and protein design tasks demonstrate strong sample efficiency and effective preference incorporation.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2510.17187.pdf' target='_blank'>https://arxiv.org/pdf/2510.17187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Aghili, Andy Bruce, Daniel Sabo, Sanya Murdeshwar, Kevin Bachelor, Ionut Mistreanu, Ashwin Lokapally, Razvan Marinescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17187">A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of molecular dynamics (MD) methods, including machine-learned dynamics, has outpaced the development of standardized tools for method validation. Objective comparison between simulation approaches is often hindered by inconsistent evaluation metrics, insufficient sampling of rare conformational states, and the absence of reproducible benchmarks. To address these challenges, we introduce a modular benchmarking framework that systematically evaluates protein MD methods using enhanced sampling analysis. Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble Simulation Toolkit with Parallelization and Analysis (WESTPA), based on progress coordinates derived from Time-lagged Independent Component Analysis (TICA), enabling fast and efficient exploration of protein conformational space. The framework includes a flexible, lightweight propagator interface that supports arbitrary simulation engines, allowing both classical force fields and machine learning-based models. Additionally, the framework offers a comprehensive evaluation suite capable of computing more than 19 different metrics and visualizations across a variety of domains. We further contribute a dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a variety of folding complexities and topologies. Each protein has been extensively simulated at 300K for one million MD steps per starting point (4 ns). To demonstrate the utility of our framework, we perform validation tests using classic MD simulations with implicit solvent and compare protein conformational sampling using a fully trained versus under-trained CGSchNet model. By standardizing evaluation protocols and enabling direct, reproducible comparisons across MD approaches, our open-source platform lays the groundwork for consistent, rigorous benchmarking across the molecular simulation community.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2510.08350.pdf' target='_blank'>https://arxiv.org/pdf/2510.08350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Jason Tan, Jiayang Chen, Dilruk Perera, Kay Choong See, Mengling Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08350">DeepEN: Personalized Enteral Nutrition for Critically Ill Patients using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DeepEN, a deep reinforcement learning (RL) framework for personalized enteral nutrition (EN) in critically ill patients. Trained offline on over 11,000 ICU patients from the MIMIC-IV database, DeepEN generates 4-hourly recommendations for caloric, protein, and fluid intake tailored to each patient's evolving physiology. The model integrates a curated, clinically informed state space with a custom reward function that balances short-term physiological and nutrition-related goals with long-term survival outcomes. Using a dueling double deep Q-network with conservative Q-learning regularization, DeepEN learns clinically realistic policies that align with high-value clinician actions while discouraging unsafe deviations. Across various qualitative and quantitative metrics, DeepEN outperforms clinician-derived and guideline-based policies, achieving a 3.7 $\pm$ 0.17 percentage-point reduction in estimated mortality (18.8% vs 22.5%) and improvements in key nutritional biomarkers. These findings highlight the potential of safe, data-driven personalization of EN therapy to improve outcomes beyond traditional guideline- or heuristic-based approaches.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.17208.pdf' target='_blank'>https://arxiv.org/pdf/2509.17208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Bachelor, Sanya Murdeshwar, Daniel Sabo, Razvan Marinescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17208">Active Learning for Machine Learning Driven Molecular Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learned coarse grained (CG) potentials are fast, but degrade over time when simulations reach undersampled biomolecular conformations, and generating widespread all atom (AA) data to combat this is computationally infeasible. We propose a novel active learning framework for CG neural network potentials in molecular dynamics (MD). Building on the CGSchNet model, our method employs root mean squared deviation (RMSD) based frame selection from MD simulations in order to generate data on the fly by querying an oracle during the training of a neural network potential. This framework preserves CG level efficiency while correcting the model at precise, RMSD identified coverage gaps. By training CGSchNet, a coarse grained neural network potential, we empirically show that our framework explores previously unseen configurations and trains the model on unexplored regions of conformational space. Our active learning framework enables a CGSchNet model trained on the Chignolin protein to achieve a 33.05% improvement in the Wasserstein 1 (W1) metric in Time lagged Independent Component Analysis (TICA) space on an in house benchmark suite.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.14600.pdf' target='_blank'>https://arxiv.org/pdf/2509.14600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Aghili, Andy Bruce, Daniel Sabo, Razvan Marinescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14600">TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics (MD) simulations provide atomistic insight into biomolecular systems but are often limited by high computational costs required to access long timescales. Coarse-grained machine learning models offer a promising avenue for accelerating sampling, yet conventional force matching approaches often fail to capture the full thermodynamic landscape as fitting a model on the gradient may not fit the absolute differences between low-energy conformational states. In this work, we incorporate a complementary energy matching term into the loss function. We evaluate our framework on the Chignolin protein using the CGSchNet model, systematically varying the weight of the energy loss term. While energy matching did not yield statistically significant improvements in accuracy, it revealed distinct tendencies in how models generalize the free energy surface. Our results suggest future opportunities to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2509.02069.pdf' target='_blank'>https://arxiv.org/pdf/2509.02069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srinivas Anumasa, Barath Chandran. C, Tingting Chen, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02069">Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as a powerful class of generative models by learning to iteratively reverse the noising process. Their ability to generate high-quality samples has extended beyond high-dimensional image data to other complex domains such as proteins, where data distributions are typically sparse and unevenly spread. Importantly, the sparsity itself is uneven. Empirically, we observed that while a small fraction of samples lie in dense clusters, the majority occupy regions of varying sparsity across the data space. Existing approaches largely ignore this data-dependent variability. In this work, we introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel density estimation (KDE) as a preprocessing step to estimate the noise scale $Ï$ for each data point, followed by training a score model with these data-dependent $Ï$ values. By incorporating local data geometry into the denoising process, our method accounts for the heterogeneous distribution of protein data. Empirical evaluations demonstrate that our approach yields consistent improvements across multiple metrics, highlighting the importance of data-aware sigma prediction for generative modeling in sparse, high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2509.01038.pdf' target='_blank'>https://arxiv.org/pdf/2509.01038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihir Bafna, Bowen Jing, Bonnie Berger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01038">Learning residue level protein dynamics with multiscale Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many methods have been developed to predict static protein structures, however understanding the dynamics of protein structure is essential for elucidating biological function. While molecular dynamics (MD) simulations remain the in silico gold standard, its high computational cost limits scalability. We present DynaProt, a lightweight, SE(3)-invariant framework that predicts rich descriptors of protein dynamics directly from static structures. By casting the problem through the lens of multivariate Gaussians, DynaProt estimates dynamics at two complementary scales: (1) per-residue marginal anisotropy as $3 \times 3$ covariance matrices capturing local flexibility, and (2) joint scalar covariances encoding pairwise dynamic coupling across residues. From these dynamics outputs, DynaProt achieves high accuracy in predicting residue-level flexibility (RMSF) and, remarkably, enables reasonable reconstruction of the full covariance matrix for fast ensemble generation. Notably, it does so using orders of magnitude fewer parameters than prior methods. Our results highlight the potential of direct protein dynamics prediction as a scalable alternative to existing methods.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2505.21236.pdf' target='_blank'>https://arxiv.org/pdf/2505.21236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan de Kock, Claude Formanek, Sasha Abramowitz, Oumayma Mahjoub, Wiem Khlifi, Simon Du Toit, Louay Ben Nessir, Refiloe Shabe, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21236">Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inf-marl.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2505.11037.pdf' target='_blank'>https://arxiv.org/pdf/2505.11037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqing Sun, Dawei Feng, Sen Yang, Yijie Wang, Huaimin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11037">Evolutionary training-free guidance in diffusion model for 3D multi-objective molecular generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discovering novel 3D molecular structures that simultaneously satisfy multiple property targets remains a central challenge in materials and drug design. Although recent diffusion-based models can generate 3D conformations, they require expensive retraining for each new property or property-combination and lack flexibility in enforcing structural constraints. We introduce EGD (Evolutionary Guidance in Diffusion), a training-free framework that embeds evolutionary operators directly into the diffusion sampling process. By performing crossover on noise-perturbed samples and then denoising them with a pretrained Unconditional diffusion model, EGD seamlessly blends structural fragments and steers generation toward user-specified objectives without any additional model updates. On both single- and multi-target 3D conditional generation tasks-and on multi-objective optimization of quantum properties EGD outperforms state-of-the-art conditional diffusion methods in accuracy and runs up to five times faster per generation. In the single-objective optimization of protein ligands, EGD enables customized ligand generation. Moreover, EGD can embed arbitrary 3D fragments into the generated molecules while optimizing multiple conflicting properties in one unified process. This combination of efficiency, flexibility, and controllable structure makes EGD a powerful tool for rapid, guided exploration of chemical space.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2501.15007.pdf' target='_blank'>https://arxiv.org/pdf/2501.15007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Liu, Yi Liu, Silei Chen, Wei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15007">Controllable Protein Sequence Generation with LLM Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing proteins with specific attributes offers an important solution to address biomedical challenges. Pre-trained protein large language models (LLMs) have shown promising results on protein sequence generation. However, to control sequence generation for specific attributes, existing work still exhibits poor functionality and structural stability. In this paper, we propose a novel controllable protein design method called CtrlProt. We finetune a protein LLM with a new multi-listwise preference optimization strategy to improve generation quality and support multi-attribute controllable generation. Experiments demonstrate that CtrlProt can meet functionality and structural stability requirements effectively, achieving state-of-the-art performance in both single-attribute and multi-attribute protein sequence generation.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2412.15086.pdf' target='_blank'>https://arxiv.org/pdf/2412.15086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15086">Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the conditional generation of 3D drug-like molecules with \textit{explicit control} over molecular properties such as drug-like properties (e.g., Quantitative Estimate of Druglikeness or Synthetic Accessibility score) and effectively binding to specific protein sites. To tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and factorize the latent space of our generative model into two disentangled aspects: molecular properties and the remaining structural context of 3D molecules. Our model ensures explicit control over these molecular attributes while maintaining equivariance of coordinate representation and invariance of data likelihood. Furthermore, we introduce a novel alignment-based coordinate loss to adapt equivariant networks for auto-regressive de-novo 3D molecule generation from scratch. Extensive experiments validate our model's effectiveness on property-guided and context-guided molecule generation, both for de-novo 3D molecule design and structure-based drug discovery against protein targets.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2412.07778.pdf' target='_blank'>https://arxiv.org/pdf/2412.07778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuqi Li, Shufang Xie, Hongda Sun, Yuhan Chen, Tao Qin, Tianjun Ke, Rui Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07778">MIN: Multi-channel Interaction Network for Drug-Target Interaction with Protein Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional drug discovery processes are both time-consuming and require extensive professional expertise. With the accumulation of drug-target interaction (DTI) data from experimental studies, leveraging modern machine-learning techniques to discern patterns between drugs and target proteins has become increasingly feasible. In this paper, we introduce the Multi-channel Interaction Network (MIN), a novel framework designed to predict DTIs through two primary components: a representation learning module and a multi-channel interaction module. The representation learning module features a C-Score Predictor-assisted screening mechanism, which selects critical residues to enhance prediction accuracy and reduce noise. The multi-channel interaction module incorporates a structure-agnostic channel, a structure-aware channel, and an extended-mixture channel, facilitating the identification of interaction patterns at various levels for optimal complementarity. Additionally, contrastive learning is utilized to harmonize the representations of diverse data types. Our experimental evaluations on public datasets demonstrate that MIN surpasses other strong DTI prediction methods. Furthermore, the case study reveals a high overlap between the residues selected by the C-Score Predictor and those in actual binding pockets, underscoring MIN's explainability capability. These findings affirm that MIN is not only a potent tool for DTI prediction but also offers fresh insights into the prediction of protein binding sites.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2411.16686.pdf' target='_blank'>https://arxiv.org/pdf/2411.16686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Ma, Fei Ye, Yi Zhou, Zaixiang Zheng, Dongyu Xue, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16686">ProteinWeaver: A Divide-and-Assembly Approach for Protein Backbone Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nature creates diverse proteins through a 'divide and assembly' strategy. Inspired by this idea, we introduce ProteinWeaver, a two-stage framework for protein backbone design. Our method first generates individual protein domains and then employs an SE(3) diffusion model to flexibly assemble these domains. A key challenge lies in the assembling step, given the complex and rugged nature of the inter-domain interaction landscape. To address this challenge, we employ preference alignment to discern complex relationships between structure and interaction landscapes through comparative analysis of generated samples. Comprehensive experiments demonstrate that ProteinWeaver: (1) generates high-quality, novel protein backbones through versatile domain assembly; (2) outperforms RFdiffusion, the current state-of-the-art in backbone design, by 13\% and 39\% for long-chain proteins; (3) shows the potential for cooperative function design through illustrative case studies. To sum up, by introducing a `divide-and-assembly' paradigm, ProteinWeaver advances protein engineering and opens new avenues for functional protein design.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2410.13782.pdf' target='_blank'>https://arxiv.org/pdf/2410.13782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13782">DPLM-2: A Multimodal Diffusion Protein Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2409.06744.pdf' target='_blank'>https://arxiv.org/pdf/2409.06744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Ye, Zaixiang Zheng, Dongyu Xue, Yuning Shen, Lihao Wang, Yiming Ma, Yan Wang, Xinyou Wang, Xiangxin Zhou, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06744">ProteinBench: A Holistic Evaluation of Protein Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2409.06142.pdf' target='_blank'>https://arxiv.org/pdf/2409.06142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel M. Steinberg, Rafael Oliveira, Cheng Soon Ong, Edwin V. Bonilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06142">Variational Search Distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop VSD, a method for conditioning a generative model of discrete, combinatorial designs on a rare desired class by efficiently evaluating a black-box (e.g. experiment, simulation) in a batch sequential manner. We call this task active generation; we formalize active generation's requirements and desiderata, and formulate a solution via variational inference. VSD uses off-the-shelf gradient based optimization routines, can learn powerful generative models for desirable designs, and can take advantage of scalable predictive models. We derive asymptotic convergence rates for learning the true conditional generative distribution of designs with certain configurations of our method. After illustrating the generative model on images, we empirically demonstrate that VSD can outperform existing baseline methods on a set of real sequence-design problems in various protein and DNA/RNA engineering tasks.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2408.06396.pdf' target='_blank'>https://arxiv.org/pdf/2408.06396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06396">Design Proteins Using Large Language Models: Enhancements and Comparative Analyses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2407.12053.pdf' target='_blank'>https://arxiv.org/pdf/2407.12053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoning Li, Mingyu Li, Yusong Wang, Xinheng He, Nanning Zheng, Jian Zhang, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12053">Improving AlphaFlow for Efficient Protein Ensembles Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Investigating conformational landscapes of proteins is a crucial way to understand their biological functions and properties. AlphaFlow stands out as a sequence-conditioned generative model that introduces flexibility into structure prediction models by fine-tuning AlphaFold under the flow-matching framework. Despite the advantages of efficient sampling afforded by flow-matching, AlphaFlow still requires multiple runs of AlphaFold to finally generate one single conformation. Due to the heavy consumption of AlphaFold, its applicability is limited in sampling larger set of protein ensembles or the longer chains within a constrained timeframe. In this work, we propose a feature-conditioned generative model called AlphaFlow-Lit to realize efficient protein ensembles generation. In contrast to the full fine-tuning on the entire structure, we focus solely on the light-weight structure module to reconstruct the conformation. AlphaFlow-Lit performs on-par with AlphaFlow and surpasses its distilled version without pretraining, all while achieving a significant sampling acceleration of around 47 times. The advancement in efficiency showcases the potential of AlphaFlow-Lit in enabling faster and more scalable generation of protein ensembles.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2406.15534.pdf' target='_blank'>https://arxiv.org/pdf/2406.15534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Liu, Yijia Xiao, Xiao Luo, Hua Xu, W. Jim Zheng, Hongyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15534">Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The applications of large language models (LLMs) are promising for biomedical and healthcare research. Despite the availability of open-source LLMs trained using a wide range of biomedical data, current research on the applications of LLMs to genomics and proteomics is still limited. To fill this gap, we propose a collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse, for three novel tasks in genomic and proteomic research. The models in Geneverse are trained and evaluated based on domain-specific datasets, and we use advanced parameter-efficient finetuning techniques to achieve the model adaptation for tasks including the generation of descriptions for gene functions, protein function inference from its structure, and marker gene selection from spatial transcriptomic data. We demonstrate that adapted LLMs and MLLMs perform well for these tasks and may outperform closed-source large-scale models based on our evaluations focusing on both truthfulness and structural correctness. All of the training strategies and base models we used are freely accessible.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2406.13864.pdf' target='_blank'>https://arxiv.org/pdf/2406.13864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arian R. Jamasb, Alex Morehead, Chaitanya K. Joshi, Zuobai Zhang, Kieran Didi, Simon V. Mathis, Charles Harris, Jian Tang, Jianlin Cheng, Pietro Lio, Tom L. Blundell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13864">Evaluating representation learning on the protein structure universe</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ProteinWorkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks. We consider large-scale pre-training and downstream tasks on both experimental and predicted structures to enable the systematic evaluation of the quality of the learned structural representation and their usefulness in capturing functional relationships for downstream tasks. We find that: (1) large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs, and (2) more expressive equivariant GNNs benefit from pretraining to a greater extent compared to invariant models. We aim to establish a common ground for the machine learning and computational biology communities to rigorously compare and advance protein structure representation learning. Our open-source codebase reduces the barrier to entry for working with large protein structure datasets by providing: (1) storage-efficient dataloaders for large-scale structural databases including AlphaFoldDB and ESM Atlas, as well as (2) utilities for constructing new tasks from the entire PDB. ProteinWorkshop is available at: github.com/a-r-j/ProteinWorkshop.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2405.00751.pdf' target='_blank'>https://arxiv.org/pdf/2405.00751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoning Li, Yusong Wang, Mingyu Li, Jian Zhang, Bin Shao, Nanning Zheng, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00751">F$^3$low: Frame-to-Frame Coarse-grained Molecular Dynamics with SE(3) Guided Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics (MD) is a crucial technique for simulating biological systems, enabling the exploration of their dynamic nature and fostering an understanding of their functions and properties. To address exploration inefficiency, emerging enhanced sampling approaches like coarse-graining (CG) and generative models have been employed. In this work, we propose a \underline{Frame-to-Frame} generative model with guided \underline{Flow}-matching (F$3$low) for enhanced sampling, which (a) extends the domain of CG modeling to the SE(3) Riemannian manifold; (b) retreating CGMD simulations as autoregressively sampling guided by the former frame via flow-matching models; (c) targets the protein backbone, offering improved insights into secondary structure formation and intricate folding pathways. Compared to previous methods, F$3$low allows for broader exploration of conformational space. The ability to rapidly generate diverse conformations via force-free generative paradigm on SE(3) paves the way toward efficient enhanced sampling methods.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2402.04384.pdf' target='_blank'>https://arxiv.org/pdf/2402.04384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard E. Turner, Cristiana-Diana Diaconu, Stratis Markou, Aliaksandra Shysheya, Andrew Y. K. Foong, Bruno Mlodozeniec
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04384">Denoising Diffusion Probabilistic Models in Six Simple Steps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2512.15133.pdf' target='_blank'>https://arxiv.org/pdf/2512.15133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhou, Haohao Qu, Yunqing Liu, Shanru Lin, Le Song, Wenqi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15133">HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2512.02030.pdf' target='_blank'>https://arxiv.org/pdf/2512.02030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Qian, Pu You, Lin Zeng, Jingyuan Zhou, Dengdeng Huang, Kaicheng Li, Shikui Tu, Lei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02030">Generative design and validation of therapeutic peptides for glioblastoma based on a potential target ATP5A</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glioblastoma (GBM) remains the most aggressive tumor, urgently requiring novel therapeutic strategies. Here, we present a dry-to-wet framework combining generative modeling and experimental validation to optimize peptides targeting ATP5A, a potential peptide-binding protein for GBM. Our framework introduces the first lead-conditioned generative model, which focuses exploration on geometrically relevant regions around lead peptides and mitigates the combinatorial complexity of de novo methods. Specifically, we propose POTFlow, a \underline{P}rior and \underline{O}ptimal \underline{T}ransport-based \underline{Flow}-matching model for peptide optimization. POTFlow employs secondary structure information (e.g., helix, sheet, loop) as geometric constraints, which are further refined by optimal transport to produce shorter flow paths. With this design, our method achieves state-of-the-art performance compared with five popular approaches. When applied to GBM, our method generates peptides that selectively inhibit cell viability and significantly prolong survival in a patient-derived xenograft (PDX) model. As the first lead peptide-conditioned flow matching model, POTFlow holds strong potential as a generalizable framework for therapeutic peptide design.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2509.14029.pdf' target='_blank'>https://arxiv.org/pdf/2509.14029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Tovey, Julian HoÃbach, Sandro Kuppel, Tobias Ensslen, Jan C. Behrends, Christian Holm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14029">Deep Learning-Driven Peptide Classification in Biological Nanopores</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A device capable of performing real time classification of proteins in a clinical setting would allow for inexpensive and rapid disease diagnosis. One such candidate for this technology are nanopore devices. These devices work by measuring a current signal that arises when a protein or peptide enters a nanometer-length-scale pore. Should this current be uniquely related to the structure of the peptide and its interactions with the pore, the signals can be used to perform identification. While such a method would allow for real time identification of peptides and proteins in a clinical setting, to date, the complexities of these signals limit their accuracy. In this work, we tackle the issue of classification by converting the current signals into scaleogram images via wavelet transforms, capturing amplitude, frequency, and time information in a modality well-suited to machine learning algorithms. When tested on 42 peptides, our method achieved a classification accuracy of ~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward practical peptide/protein diagnostics at the point of care. In addition, we demonstrate model transfer techniques that will be critical when deploying these models into real hardware, paving the way to a new method for real-time disease diagnosis.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2509.01486.pdf' target='_blank'>https://arxiv.org/pdf/2509.01486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Zhou, Hao Qian, Shikui Tu, Lei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01486">Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD), aiming to generate 3D molecules with high binding affinity toward target proteins, is a vital approach in novel drug discovery. Although recent generative models have shown great potential, they suffer from unstable probability dynamics and mismatch between generated molecule size and the protein pockets geometry, resulting in inconsistent quality and off-target effects. We propose PAFlow, a novel target-aware molecular generation model featuring prior interaction guidance and a learnable atom number predictor. PAFlow adopts the efficient flow matching framework to model the generation process and constructs a new form of conditional flow matching for discrete atom types. A protein-ligand interaction predictor is incorporated to guide the vector field toward higher-affinity regions during generation, while an atom number predictor based on protein pocket information is designed to better align generated molecule size with target geometry. Extensive experiments on the CrossDocked2020 benchmark show that PAFlow achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina Score), simultaneously maintains favorable molecular properties.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2508.14351.pdf' target='_blank'>https://arxiv.org/pdf/2508.14351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwei Su, Chuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14351">A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Score-based graph generative models (SGGMs) have proven effective in critical applications such as drug discovery and protein synthesis. However, their theoretical behavior, particularly regarding convergence, remains underexplored. Unlike common score-based generative models (SGMs), which are governed by a single stochastic differential equation (SDE), SGGMs involve a system of coupled SDEs. In SGGMs, the graph structure and node features are governed by separate but interdependent SDEs. This distinction makes existing convergence analyses from SGMs inapplicable for SGGMs. In this work, we present the first non-asymptotic convergence analysis for SGGMs, focusing on the convergence bound (the risk of generative error) across three key graph generation paradigms: (1) feature generation with a fixed graph structure, (2) graph structure generation with fixed node features, and (3) joint generation of both graph structure and node features. Our analysis reveals several unique factors specific to SGGMs (e.g., the topological properties of the graph structure) which affect the convergence bound. Additionally, we offer theoretical insights into the selection of hyperparameters (e.g., sampling steps and diffusion length) and advocate for techniques like normalization to improve convergence. To validate our theoretical findings, we conduct a controlled empirical study using synthetic graph models, and the results align with our theoretical predictions. This work deepens the theoretical understanding of SGGMs, demonstrates their applicability in critical domains, and provides practical guidance for designing effective models.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2508.04415.pdf' target='_blank'>https://arxiv.org/pdf/2508.04415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Chen, Yu Huang, Miaowen Wen, Shahid Mumtaz, Fatih Gulec, Anwer Al-Dulaimi, Andrew W. Eckford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04415">Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary healthcare paradigm, shows promise for epidemic control. This paper explores the potential of using molecular communication (MC) to address the challenges in constructing IoBNT for epidemic prevention, specifically focusing on modeling viral transmission, detecting the virus/infected individuals, and identifying virus mutations. First, the MC channels in macroscale and microscale scenarios are discussed to match viral transmission in both scales separately. Besides, the detection methods for these two scales are also studied, along with the localization mechanism designed for the virus/infected individuals. Moreover, an identification strategy is proposed to determine potential virus mutations, which is validated through simulation using the ORF3a protein as a benchmark. Finally, open research issues are discussed. In summary, this paper aims to analyze viral transmission through MC and combat viral spread using signal processing techniques within MC.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2507.03318.pdf' target='_blank'>https://arxiv.org/pdf/2507.03318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zanyu Shi, Yang Wang, Pathum Weerawarna, Jie Zhang, Timothy Richardson, Yijie Wang, Kun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03318">Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainable artificial intelligence (XAI) approaches have been increasingly applied in drug discovery to learn molecular representations and identify substructures driving property predictions. However, building end-to-end explainable machine learning models for structure-activity relationship (SAR) modeling for compound property prediction faces many challenges, such as limited activity data per target and the sensitivity of properties to subtle molecular changes. To address this, we leveraged activity-cliff molecule pairs, i.e., compounds sharing a common scaffold but differing sharply in potency, targeting three proto-oncogene tyrosine-protein kinase Src proteins (i.e., PDB IDs 1O42, 2H8H, and 4MXO). We implemented graph neural network (GNN) methods to obtain atom-level feature information and predict compound-protein affinity (i.e., half maximal inhibitory concentration, IC50). In addition, we trained GNN models with different structure-aware loss functions to adequately leverage molecular property and structure information. We also utilized group lasso and sparse group lasso to prune and highlight molecular subgraphs and enhance the structure-specific model explainability for the predicted property difference in molecular activity-cliff pairs. We improved drug property prediction by integrating common and uncommon node information and using sparse group lasso, reducing the average root mean squared error (RMSE) by 12.70%, and achieving the lowest averaged RMSE=0.2551 and the highest PCC=0.9572. Furthermore, applying regularization enhances feature attribution methods that estimate the contribution of each atom in the molecular graphs by boosting global direction scores and atom-level accuracy in atom coloring accuracy, which improves model interpretability in drug discovery pipelines, particularly in investigating important molecular substructures in lead optimization.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2505.20301.pdf' target='_blank'>https://arxiv.org/pdf/2505.20301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Liu, Mingchen Li, Yang Tan, Wenrui Gou, Guisheng Fan, Bingxin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20301">Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A pivotal area of research in antibody engineering is to find effective modifications that enhance antibody-antigen binding affinity. Traditional wet-lab experiments assess mutants in a costly and time-consuming manner. Emerging deep learning solutions offer an alternative by modeling antibody structures to predict binding affinity changes. However, they heavily depend on high-quality complex structures, which are frequently unavailable in practice. Therefore, we propose ProtAttBA, a deep learning model that predicts binding affinity changes based solely on the sequence information of antibody-antigen complexes. ProtAttBA employs a pre-training phase to learn protein sequence patterns, following a supervised training phase using labeled antibody-antigen complex data to train a cross-attention-based regressor for predicting binding affinity changes. We evaluated ProtAttBA on three open benchmarks under different conditions. Compared to both sequence- and structure-based prediction methods, our approach achieves competitive performance, demonstrating notable robustness, especially with uncertain complex structures. Notably, our method possesses interpretability from the attention mechanism. We show that the learned attention scores can identify critical residues with impacts on binding affinity. This work introduces a rapid and cost-effective computational tool for antibody engineering, with the potential to accelerate the development of novel therapeutic antibodies.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2504.02899.pdf' target='_blank'>https://arxiv.org/pdf/2504.02899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuseppe Russo, Kristina GligoriÄ, Vincent Moreau, Robert West
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02899">Meat-Free Day Reduces Greenhouse Gas Emissions but Poses Challenges for Customer Retention and Adherence to Dietary Guidelines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reducing meat consumption is crucial for achieving global environmental and nutritional targets. Meat-Free Day (MFD) is a widely adopted strategy to address this challenge by encouraging plant-based diets through the removal of animal-based meals. We assessed the environmental, behavioral, and nutritional impacts of MFD by implementing 67 MFDs over 18 months (once a week on a randomly chosen day) across 12 cafeterias on a large university campus, analyzing over 400,000 food purchases. MFD reduced on-campus food-related greenhouse gas (GHG) emissions on treated days by 52.9% and contributed to improved fiber (+26.9%) and cholesterol (-4.5%) consumption without altering caloric intake. These nutritional benefits were, however, accompanied by a 27.6% decrease in protein intake and a 34.2% increase in sugar consumption. Moreover, the increase in plant-based meals did not carry over to subsequent days, as evidenced by a 3.5% rebound in animal-based meal consumption on days immediately following treated days. MFD also led to a 16.8% drop in on-campus meal sales on treated days.Monte Carlo simulations suggest that if 8.7% of diners were to eat burgers off-campus on treated days, MFD's GHG savings would be fully negated. As our analysis identifies on-campus customer retention as the main challenge to MFD effectiveness, we recommend combining MFD with customer retention interventions to ensure environmental and nutritional benefits.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2503.06687.pdf' target='_blank'>https://arxiv.org/pdf/2503.06687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gongbo Zhang, Yanting Li, Renqian Luo, Pipi Hu, Yang Yang, Zeru Zhao, Lingbo Li, Guoqing Liu, Zun Wang, Ran Bi, Kaiyuan Gao, Liya Guo, Yu Xie, Chang Liu, Jia Zhang, Tian Xie, Robert Pinsler, Claudio Zeni, Ziheng Lu, Hongxia Hao, Yingce Xia, Marwin Segler, Maik Riechert, Wei Yang, Hao Jiang, Wen-Bin Zhang, Zhijun Zeng, Yi Zhu, Li Dong, Xiuyuan Hu, Li Yuan, Lei Chen, Haiguang Liu, Tao Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06687">UniGenX: a unified generative foundation model that couples sequence, structure and function to accelerate scientific design across proteins, molecules and materials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Function in natural systems arises from one-dimensional sequences forming three-dimensional structures with specific properties. However, current generative models suffer from critical limitations: training objectives seldom target function directly, discrete sequences and continuous coordinates are optimized in isolation, and conformational ensembles are under-modeled. We present UniGenX, a unified generative foundation model that addresses these gaps by co-generating sequences and coordinates under direct functional and property objectives across proteins, molecules, and materials. UniGenX represents heterogeneous inputs as a mixed stream of symbolic and numeric tokens, where a decoder-only autoregressive transformer provides global context and a conditional diffusion head generates numeric fields steered by task-specific tokens. Besides the new high SOTAs on structure prediction tasks, the model demonstrates state-of-the-art or competitive performance for the function-aware generation across domains: in materials, it achieves "conflicted" multi-property conditional generation, yielding 436 crystal candidates meeting triple constraints, including 11 with novel compositions; in chemistry, it sets new benchmarks on five property targets and conformer ensemble generation on GEOM; and in biology, it improves success in modeling protein induced fit (RMSD < 2 Ã) by over 23-fold and enhances EC-conditioned enzyme design. Ablation studies and cross-domain transfer substantiate the benefits of joint discrete-continuous training, establishing UniGenX as a significant advance from prediction to controllable, function-aware generation.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2502.19395.pdf' target='_blank'>https://arxiv.org/pdf/2502.19395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyi Zhang, Kun Xie, Ningqiao Huang, Wei Liu, Peilin Zhao, Sibo Wang, Kangfei Zhao, Biaobin Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19395">Fast and Accurate Antibody Sequence Design via Structure Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in protein design have leveraged diffusion models to generate structural scaffolds, followed by a process known as protein inverse folding, which involves sequence inference on these scaffolds. However, these methodologies face significant challenges when applied to hyper-variable structures such as antibody Complementarity-Determining Regions (CDRs), where sequence inference frequently results in non-functional sequences due to hallucinations. Distinguished from prevailing protein inverse folding approaches, this paper introduces Igseek, a novel structure-retrieval framework that infers CDR sequences by retrieving similar structures from a natural antibody database. Specifically, Igseek employs a simple yet effective multi-channel equivariant graph neural network to generate high-quality geometric representations of CDR backbone structures. Subsequently, it aligns sequences of structurally similar CDRs and utilizes structurally conserved sequence motifs to enhance inference accuracy. Our experiments demonstrate that Igseek not only proves to be highly efficient in structural retrieval but also outperforms state-of-the-art approaches in sequence recovery for both antibodies and T-Cell Receptors, offering a new retrieval-based perspective for therapeutic protein design.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2502.12280.pdf' target='_blank'>https://arxiv.org/pdf/2502.12280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Ma, Alexander Brace, Carlo Siebenschuh, Greg Pauloski, Ian Foster, Arvind Ramanathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12280">Connecting Large Language Model Agent to High Performance Computing Resource</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Large Language Model agent workflow enables the LLM to invoke tool functions to increase the performance on specific scientific domain questions. To tackle large scale of scientific research, it requires access to computing resource and parallel computing setup. In this work, we implemented Parsl to the LangChain/LangGraph tool call setup, to bridge the gap between the LLM agent to the computing resource. Two tool call implementations were set up and tested on both local workstation and HPC environment on Polaris/ALCF. The first implementation with Parsl-enabled LangChain tool node queues the tool functions concurrently to the Parsl workers for parallel execution. The second configuration is implemented by converting the tool functions into Parsl ensemble functions, and is more suitable for large task on super computer environment. The LLM agent workflow was prompted to run molecular dynamics simulations, with different protein structure and simulation conditions. These results showed the LLM agent tools were managed and executed concurrently by Parsl on the available computing resource.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2501.06108.pdf' target='_blank'>https://arxiv.org/pdf/2501.06108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AurÃ©lien Decelle, Alfonso de JesÃºs Navas GÃ³mez, Beatriz Seoane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06108">Inferring Higher-Order Couplings with Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maximum entropy methods, rooted in the inverse Ising/Potts problem from statistical physics, are widely used to model pairwise interactions in complex systems across disciplines such as bioinformatics and neuroscience. While successful, these approaches often fail to capture higher-order interactions that are critical for understanding collective behavior. In contrast, modern machine learning methods can model such interactions, but their interpretability often comes at a prohibitive computational cost. Restricted Boltzmann Machines (RBMs) provide a computationally efficient alternative by encoding statistical correlations through hidden units in a bipartite architecture. In this work, we introduce a method that maps RBMs onto generalized Potts models, enabling the systematic extraction of interactions up to arbitrary order. Leveraging large-$N$ approximations, made tractable by the RBM's structure, we extract effective many-body couplings with minimal computational effort. We further propose a robust framework for recovering higher-order interactions in more complex generative models, and introduce a simple gauge-fixing scheme for the effective Potts representation. Validation on synthetic data demonstrates accurate recovery of two- and three-body interactions. Applied to protein sequence data, our method reconstructs contact maps with high fidelity and outperforms state-of-the-art inverse Potts models. These results establish RBMs as a powerful and efficient tool for modeling higher-order structure in high-dimensional categorical data.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2410.13487.pdf' target='_blank'>https://arxiv.org/pdf/2410.13487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Zuidberg Dos Martires, Vincent Derkinderen, Luc De Raedt, Marcus Krantz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13487">Automated Reasoning in Systems Biology: a Necessity for Precision Medicine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in AI have reinvigorated pursuits to advance the (life) sciences using AI techniques, thereby creating a renewed opportunity to bridge different fields and find synergies. Headlines for AI and the life sciences have been dominated by data-driven techniques, for instance, to solve protein folding with next to no expert knowledge. In contrast to this, we argue for the necessity of a formal representation of expert knowledge - either to develop explicit scientific theories or to compensate for the lack of data. Specifically, we argue that the fields of knowledge representation (KR) and systems biology (SysBio) exhibit important overlaps that have been largely ignored so far. This, in turn, means that relevant scientific questions are ready to be answered using the right domain knowledge (SysBio), encoded in the right way (SysBio/KR), and by combining it with modern automated reasoning tools (KR). Hence, the formal representation of domain knowledge is a natural meeting place for SysBio and KR. On the one hand, we argue that such an interdisciplinary approach will advance the field SysBio by exposing it to industrial-grade reasoning tools and thereby allowing novel scientific questions to be tackled. On the other hand, we see ample opportunities to move the state-of-the-art in KR by tailoring KR methods to the field of SysBio, which comes with challenging problem characteristics, e.g. scale, partial knowledge, noise, or sub-symbolic data. We stipulate that this proposed interdisciplinary research is necessary to attain a prominent long-term goal in the health sciences: precision medicine.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2410.12031.pdf' target='_blank'>https://arxiv.org/pdf/2410.12031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marko DjukanoviÄ, Jaume Reixach, Ana Nikolikj, Tome Eftimov, Aleksandar Kartelj, Christian Blum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12031">A Learning Search Algorithm for the Restricted Longest Common Subsequence Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the Restricted Longest Common Subsequence (RLCS) problem, an extension of the well-known Longest Common Subsequence (LCS) problem. This problem has significant applications in bioinformatics, particularly for identifying similarities and discovering mutual patterns and important motifs among DNA, RNA, and protein sequences. Building on recent advancements in solving this problem through a general search framework, this paper introduces two novel heuristic approaches designed to enhance the search process by steering it towards promising regions in the search space. The first heuristic employs a probabilistic model to evaluate partial solutions during the search process. The second heuristic is based on a neural network model trained offline using a genetic algorithm. A key aspect of this approach is extracting problem-specific features of partial solutions and the complete problem instance. An effective hybrid method, referred to as the learning beam search, is developed by combining the trained neural network model with a beam search framework. An important contribution of this paper is found in the generation of real-world instances where scientific abstracts serve as input strings, and a set of frequently occurring academic words from the literature are used as restricted patterns. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed approaches in solving the RLCS problem. Finally, an empirical explainability analysis is applied to the obtained results. In this way, key feature combinations and their respective contributions to the success or failure of the algorithms across different problem types are identified.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2406.01651.pdf' target='_blank'>https://arxiv.org/pdf/2406.01651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaohan Meng, Zaiqiao Meng, Ke Yuan, Iadh Ounis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01651">FusionDTI: Fine-grained Binding Discovery with Token-level Fusion for Drug-Target Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting drug-target interaction (DTI) is critical in the drug discovery process. Despite remarkable advances in recent DTI models through the integration of representations from diverse drug and target encoders, such models often struggle to capture the fine-grained interactions between drugs and protein, i.e. the binding of specific drug atoms (or substructures) and key amino acids of proteins, which is crucial for understanding the binding mechanisms and optimising drug design. To address this issue, this paper introduces a novel model, called FusionDTI, which uses a token-level Fusion module to effectively learn fine-grained information for Drug-Target Interaction. In particular, our FusionDTI model uses the SELFIES representation of drugs to mitigate sequence fragment invalidation and incorporates the structure-aware (SA) vocabulary of target proteins to address the limitation of amino acid sequences in structural information, additionally leveraging pre-trained language models extensively trained on large-scale biomedical datasets as encoders to capture the complex information of drugs and targets. Experiments on three well-known benchmark datasets show that our proposed FusionDTI model achieves the best performance in DTI prediction compared with seven existing state-of-the-art baselines. Furthermore, our case study indicates that FusionDTI could highlight the potential binding sites, enhancing the explainability of the DTI prediction.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2405.17902.pdf' target='_blank'>https://arxiv.org/pdf/2405.17902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Xu, Xinjian Zhao, Xiaozhuang Song, Benyou Wang, Tianshu Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17902">Boosting Protein Language Models with Negative Sample Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning. Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories. By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space. This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction. We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models. This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2402.13297.pdf' target='_blank'>https://arxiv.org/pdf/2402.13297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanglu Yan, Weiran Chu, Yuhua Sheng, Kaiwen Tang, Shida Wang, Yanfeng Liu, Weng-Fai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13297">Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>N-terminal coding sequence (NCS) influences gene expression by impacting the translation initiation rate. The NCS optimization problem is to find an NCS that maximizes gene expression. The problem is important in genetic engineering. However, current methods for NCS optimization such as rational design and statistics-guided approaches are labor-intensive yield only relatively small improvements. This paper introduces a deep learning/synthetic biology co-designed few-shot training workflow for NCS optimization. Our method utilizes k-nearest encoding followed by word2vec to encode the NCS, then performs feature extraction using attention mechanisms, before constructing a time-series network for predicting gene expression intensity, and finally a direct search algorithm identifies the optimal NCS with limited training data. We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a reporting protein of NCSs, and employed the fluorescence enhancement factor as the metric of NCS optimization. Within just six iterative experiments, our model generated an NCS (MLD62) that increased average GFP expression by 5.41-fold, outperforming the state-of-the-art NCS designs. Extending our findings beyond GFP, we showed that our engineered NCS (MLD62) can effectively boost the production of N-acetylneuraminic acid by enhancing the expression of the crucial rate-limiting GNA1 gene, demonstrating its practical utility. We have open-sourced our NCS expression database and experimental procedures for public use.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2511.12489.pdf' target='_blank'>https://arxiv.org/pdf/2511.12489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingsong Zhong, Haomin Yu, Yan Lin, Wangmeng Shen, Long Zeng, Jilin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12489">SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2511.12316.pdf' target='_blank'>https://arxiv.org/pdf/2511.12316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijun Zeng, Junqing Chen, Zuoqiang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12316">BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2510.22454.pdf' target='_blank'>https://arxiv.org/pdf/2510.22454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhan Wang, Jianwen Dou, Wang Li, Shengkun Wang, Zhiwu Xie, Chang-Tien Lu, Yinlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22454">SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution. Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck. Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized. In this work, we present a fast, label-efficient semi-supervised framework that exploits this untapped data. Our framework consists of two components: (i) an end-to-end heatmap-supervised detection model inspired by keypoint detection, and (ii) a teacher-student co-training mechanism that enhances performance under sparse labeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a CryoET-specific DropBlock augmentation strategy to further boost performance. Extensive evaluations on the large-scale CZII dataset show that our approach improves F1 by 10% over supervised baselines, underscoring the promise of semi-supervised learning for leveraging unlabeled CryoET data.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2510.19036.pdf' target='_blank'>https://arxiv.org/pdf/2510.19036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suswitha Pericharla, Daniel B. Hier, Tayo Obafemi-Ajayi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19036">From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective biomedical data integration depends on automated term normalization, the mapping of natural language biomedical terms to standardized identifiers. This linking of terms to identifiers is essential for semantic interoperability. Large language models (LLMs) show promise for this task but perform unevenly across terminologies. We evaluated both memorization (training-term performance) and generalization (validation-term performance) across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked differences by terminology. GO mappings showed strong memorization gains (up to 77% improvement in term-to-identifier accuracy), whereas HPO showed minimal improvement. Generalization occurred only for protein-gene (GENE) mappings (13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer. Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama variants for all terminologies. Embedding analyses showed tight semantic alignment between gene symbols and protein names but weak alignment between terms and identifiers for GO or HPO, consistent with limited lexicalization. Fine-tuning success depended on two interacting factors: identifier popularity and lexicalization. Popular identifiers were more likely encountered during pretraining, enhancing memorization. Lexicalized identifiers, such as gene symbols, enabled semantic generalization. By contrast, arbitrary identifiers in GO and HPO constrained models to rote learning. These findings provide a predictive framework for when fine-tuning enhances factual recall versus when it fails due to sparse or non-lexicalized identifiers.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2510.13328.pdf' target='_blank'>https://arxiv.org/pdf/2510.13328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Menet, Aleksandar Terzić, Andreas Krause, Abbas Rahimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13328">Thompson Sampling via Fine-Tuning of LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximization by directly parameterizing the probability that a candidate yields the maximum reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the prior knowledge embedded in prompt-conditioned large language models, and incrementally adapts them toward the posterior. Theoretically, we derive a novel regret bound for a variational formulation of Thompson Sampling that matches the strong guarantees of its standard counterpart. Our analysis reveals the critical role of careful adaptation to the posterior probability of maximality--a principle that underpins our ToSFiT algorithm. Empirically, we validate our method on three diverse tasks: FAQ response refinement, thermally stable protein search, and quantum circuit design. We demonstrate that online fine-tuning significantly improves sample efficiency, with negligible impact on computational efficiency.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2508.15103.pdf' target='_blank'>https://arxiv.org/pdf/2508.15103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Yazdani-Jahromi, Ali Khodabandeh Yalabadi, Ozlem Ozmen Garibay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15103">Equi-mRNA: Protein Translation Equivariant Encoding for mRNA Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing importance of mRNA therapeutics and synthetic biology highlights the need for models that capture the latent structure of synonymous codon (different triplets encoding the same amino acid) usage, which subtly modulates translation efficiency and gene expression. While recent efforts incorporate codon-level inductive biases through auxiliary objectives, they often fall short of explicitly modeling the structured relationships that arise from the genetic code's inherent symmetries. We introduce Equi-mRNA, the first codon-level equivariant mRNA language model that explicitly encodes synonymous codon symmetries as cyclic subgroups of 2D Special Orthogonal matrix (SO(2)). By combining group-theoretic priors with an auxiliary equivariance loss and symmetry-aware pooling, Equi-mRNA learns biologically grounded representations that outperform vanilla baselines across multiple axes. On downstream property-prediction tasks including expression, stability, and riboswitch switching Equi-mRNA delivers up to approximately 10% improvements in accuracy. In sequence generation, it produces mRNA constructs that are up to approximately 4x more realistic under Frechet BioDistance metrics and approximately 28% better preserve functional properties compared to vanilla baseline. Interpretability analyses further reveal that learned codon-rotation distributions recapitulate known GC-content biases and tRNA abundance patterns, offering novel insights into codon usage. Equi-mRNA establishes a new biologically principled paradigm for mRNA modeling, with significant implications for the design of next-generation therapeutics.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2508.02229.pdf' target='_blank'>https://arxiv.org/pdf/2508.02229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Bariffi, Antonia Wachter-Zeh, Eitan Yaakobi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02229">Sequence Reconstruction over Coloring Channels for Protein Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the sequence reconstruction problem for a channel inspired by protein identification. We introduce a coloring channel, where a sequence is transmitted through a channel that deletes all symbols not belonging to a fixed subset (the coloring) of the alphabet. By extending this to a coloring profile, a tuple of distinct colorings, we analyze the channel's information rate and capacity. We prove that optimal (i.e., achieving maximum information rate) coloring profiles correspond to 2-covering designs and identify the minimal covering number required for maximum information rate, as well as the minimum number for which any coloring profile is optimal.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2507.10877.pdf' target='_blank'>https://arxiv.org/pdf/2507.10877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Zhu, Jihong Chen, Yitong Li, Xiaomin Fang, Xianbin Ye, Jingzhou He, Xujun Zhang, Jingxuan Ge, Chao Shen, Xiaonan Zhang, Tingjun Hou, Chang-Yu Hsieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10877">BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural assessment of biomolecular complexes is vital for translating molecular models into functional insights, shaping our understanding of biology and aiding drug discovery. However, current structure-based scoring functions often lack generalizability across diverse biomolecular systems. We present BioScore, a foundational scoring function that addresses key challenges -- data sparsity, cross-system representation, and task compatibility -- through a dual-scale geometric graph learning framework with tailored modules for structure assessment and affinity prediction. BioScore supports a wide range of tasks, including affinity prediction, conformation ranking, and structure-based virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids, small molecules, and carbohydrates, BioScore consistently outperforms or matches 70 traditional and deep learning methods. Our newly proposed PPI Benchmark further enables comprehensive evaluation of protein-protein complex scoring. BioScore demonstrates broad applicability: (1) pretraining on mixed-structure data boosts protein-protein affinity prediction by up to 40% and antigen-antibody binding correlation by over 90%; (2) cross-system generalizability enables zero- and few-shot prediction with up to 71% correlation gain; and (3) its unified representation captures chemically challenging systems such as cyclic peptides, improving affinity prediction by over 60%. BioScore establishes a robust and generalizable framework for structural assessment across complex biomolecular landscapes.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2507.10737.pdf' target='_blank'>https://arxiv.org/pdf/2507.10737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Chen, Thai-Hoang Pham, Yuanlong Wang, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10737">Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for \textit{de novo} cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to \textit{de novo} cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for \textit{de novo} cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2506.23184.pdf' target='_blank'>https://arxiv.org/pdf/2506.23184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anran Liu, Xiaofei Wang, Jing Cai, Chao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23184">Score-based Diffusion Model for Unpaired Virtual Histology Staining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hematoxylin and eosin (H&E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2506.07459.pdf' target='_blank'>https://arxiv.org/pdf/2506.07459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwen Wang, Jiajun Fan, Ruihan Guo, Thao Nguyen, Heng Ji, Ge Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07459">ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein generative models have shown remarkable promise in protein design but still face limitations in success rate, due to the scarcity of high-quality protein datasets for supervised pretraining. We present ProteinZero, a novel framework that enables scalable, automated, and continuous self-improvement of the inverse folding model through online reinforcement learning. To achieve computationally tractable online feedback, we introduce efficient proxy reward models based on ESM-fold and a novel rapid ddG predictor that significantly accelerates evaluation speed. ProteinZero employs a general RL framework balancing multi-reward maximization, KL-divergence from a reference model, and a novel protein-embedding level diversity regularization that prevents mode collapse while promoting higher sequence diversity. Through extensive experiments, we demonstrate that ProteinZero substantially outperforms existing methods across every key metric in protein design, achieving significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity. Most impressively, ProteinZero reduces design failure rates by approximately 36% - 48% compared to widely-used methods like ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates exceeding 90% across diverse and complex protein folds. Notably, the entire RL run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days, including reward computation. Our work establishes a new paradigm for protein design where models evolve continuously from their own generated outputs, opening new possibilities for exploring the vast protein design space.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2505.15118.pdf' target='_blank'>https://arxiv.org/pdf/2505.15118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Xia, Kaiqiang Yu, Shengxin Liu, Cheng Long, Xun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15118">Maximum Degree-Based Quasi-Clique Search via an Iterative Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cohesive subgraph mining is a fundamental problem in graph theory with numerous real-world applications, such as social network analysis and protein-protein interaction modeling. Among various cohesive subgraphs, the $Î³$-quasi-clique is widely studied for its flexibility in requiring each vertex to connect to at least a $Î³$ proportion of other vertices in the subgraph. However, solving the maximum $Î³$-quasi-clique problem is NP-hard and further complicated by the lack of the hereditary property, which makes designing efficient pruning strategies challenging. Existing algorithms, such as DDA and FastQC, either struggle with scalability or exhibit significant performance declines for small values of $Î³$. In this paper, we propose a novel algorithm, IterQC, which reformulates the maximum $Î³$-quasi-clique problem as a series of $k$-plex problems that possess the hereditary property. IterQC introduces a non-trivial iterative framework and incorporates two key optimization techniques: (1) the pseudo lower bound (pseudo LB) technique, which leverages information across iterations to improve the efficiency of branch-and-bound searches, and (2) the preprocessing technique that reduces problem size and unnecessary iterations. Extensive experiments demonstrate that IterQC achieves up to four orders of magnitude speedup and solves significantly more graph instances compared to state-of-the-art algorithms DDA and FastQC.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2504.03715.pdf' target='_blank'>https://arxiv.org/pdf/2504.03715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannah Janmohamed, Antoine Cully
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03715">Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality-Diversity algorithms are powerful tools for discovering diverse, high-performing solutions. Recently, Multi-Objective Quality-Diversity (MOQD) extends QD to problems with several objectives while preserving solution diversity. MOQD has shown promise in fields such as robotics and materials science, where finding trade-offs between competing objectives like energy efficiency and speed, or material properties is essential. However, existing methods in MOQD rely on tessellating the feature space into a grid structure, which prevents their application in domains where feature spaces are unknown or must be learned, such as complex biological systems or latent exploration tasks. In this work, we introduce Multi-Objective Unstructured Repertoire for Quality-Diversity (MOUR-QD), a MOQD algorithm designed for unstructured and unbounded feature spaces. We evaluate MOUR-QD on five robotic tasks. Importantly, we show that our method excels in tasks where features must be learned, paving the way for applying MOQD to unsupervised domains. We also demonstrate that MOUR-QD is advantageous in domains with unbounded feature spaces, outperforming existing grid-based methods. Finally, we demonstrate that MOUR-QD is competitive with established MOQD methods on existing MOQD tasks and achieves double the MOQD-score in some environments. MOUR-QD opens up new opportunities for MOQD in domains like protein design and image generation.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2501.15055.pdf' target='_blank'>https://arxiv.org/pdf/2501.15055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Guan, Jiahan Li, Xiangxin Zhou, Xingang Peng, Sheng Wang, Yunan Luo, Jian Peng, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15055">Group Ligands Docking to Protein Pockets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a key task in computational biology that has attracted increasing interest from the machine learning community. While existing methods have achieved success, they generally treat each protein-ligand pair in isolation. Inspired by the biochemical observation that ligands binding to the same target protein tend to adopt similar poses, we propose \textsc{GroupBind}, a novel molecular docking framework that simultaneously considers multiple ligands docking to a protein. This is achieved by introducing an interaction layer for the group of ligands and a triangle attention module for embedding protein-ligand and group-ligand pairs. By integrating our approach with diffusion-based docking model, we set a new S performance on the PDBBind blind docking benchmark, demonstrating the effectiveness of our proposed molecular docking paradigm.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2412.07815.pdf' target='_blank'>https://arxiv.org/pdf/2412.07815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhen Bai, Filip MiljkoviÄ, Xianyuan Liu, Leonardo De Maria, Rebecca Croasdale-Wood, Owen Rackham, Haiping Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07815">Mask prior-guided denoising diffusion improves inverse protein folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse protein folding generates valid amino acid sequences that can fold into a desired protein structure, with recent deep-learning advances showing strong potential and competitive performance. However, challenges remain, such as predicting elements with high structural uncertainty, including disordered regions. To tackle such low-confidence residue prediction, we propose a Mask-prior-guided denoising Diffusion (MapDiff) framework that accurately captures both structural information and residue interactions for inverse protein folding. MapDiff is a discrete diffusion probabilistic model that iteratively generates amino acid sequences with reduced noise, conditioned on a given protein backbone. To incorporate structural information and residue interactions, we develop a graph-based denoising network with a mask-prior pre-training strategy. Moreover, in the generative process, we combine the denoising diffusion implicit model with Monte-Carlo dropout to reduce uncertainty. Evaluation on four challenging sequence design benchmarks shows that MapDiff substantially outperforms state-of-the-art methods. Furthermore, the in silico sequences generated by MapDiff closely resemble the physico-chemical and structural characteristics of native proteins across different protein families and architectures.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2409.13746.pdf' target='_blank'>https://arxiv.org/pdf/2409.13746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13746">Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.
  In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2407.19073.pdf' target='_blank'>https://arxiv.org/pdf/2407.19073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolai Schapin, Carles Navarro, Albert Bou, Gianni De Fabritiis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19073">On Machine Learning Approaches for Protein-Ligand Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Binding affinity optimization is crucial in early-stage drug discovery. While numerous machine learning methods exist for predicting ligand potency, their comparative efficacy remains unclear. This study evaluates the performance of classical tree-based models and advanced neural networks in protein-ligand binding affinity prediction. Our comprehensive benchmarking encompasses 2D models utilizing ligand-only RDKit embeddings and Large Language Model (LLM) ligand representations, as well as 3D neural networks incorporating bound protein-ligand conformations. We assess these models across multiple standard datasets, examining various predictive scenarios including classification, ranking, regression, and active learning. Results indicate that simpler models can surpass more complex ones in specific tasks, while 3D models leveraging structural information become increasingly competitive with larger training datasets containing compounds with labelled affinity data against multiple targets. Pre-trained 3D models, by incorporating protein pocket environments, demonstrate significant advantages in data-scarce scenarios for specific binding pockets. Additionally, LLM pretraining on 2D ligand data enhances complex model performance, providing versatile embeddings that outperform traditional RDKit features in computational efficiency. Finally, we show that combining 2D and 3D model strengths improves active learning outcomes beyond current state-of-the-art approaches. These findings offer valuable insights for optimizing machine learning strategies in drug discovery pipelines.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2407.01649.pdf' target='_blank'>https://arxiv.org/pdf/2407.01649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruidong Wu, Ruihan Guo, Rui Wang, Shitong Luo, Yue Xu, Jiahan Li, Jianzhu Ma, Qiang Liu, Yunan Luo, Jian Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01649">FAFE: Immune Complex Modeling with Geodesic Distance Loss on Noisy Group Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the striking success of general protein folding models such as AlphaFold2(AF2, Jumper et al. (2021)), the accurate computational modeling of antibody-antigen complexes remains a challenging task. In this paper, we first analyze AF2's primary loss function, known as the Frame Aligned Point Error (FAPE), and raise a previously overlooked issue that FAPE tends to face gradient vanishing problem on high-rotational-error targets. To address this fundamental limitation, we propose a novel geodesic loss called Frame Aligned Frame Error (FAFE, denoted as F2E to distinguish from FAPE), which enables the model to better optimize both the rotational and translational errors between two frames. We then prove that F2E can be reformulated as a group-aware geodesic loss, which translates the optimization of the residue-to-residue error to optimizing group-to-group geodesic frame distance. By fine-tuning AF2 with our proposed new loss function, we attain a correct rate of 52.3\% (DockQ $>$ 0.23) on an evaluation set and 43.8\% correct rate on a subset with low homology, with substantial improvement over AF2 by 182\% and 100\% respectively.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2406.00735.pdf' target='_blank'>https://arxiv.org/pdf/2406.00735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahan Li, Chaoran Cheng, Zuofan Wu, Ruihan Guo, Shitong Luo, Zhizhou Ren, Jian Peng, Jianzhu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00735">Full-Atom Peptide Design based on Multi-modal Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptides, short chains of amino acid residues, play a vital role in numerous biological processes by interacting with other target molecules, offering substantial potential in drug discovery. In this work, we present PepFlow, the first multi-modal deep generative model grounded in the flow-matching framework for the design of full-atom peptides that target specific protein receptors. Drawing inspiration from the crucial roles of residue backbone orientations and side-chain dynamics in protein-peptide interactions, we characterize the peptide structure using rigid backbone frames within the $\mathrm{SE}(3)$ manifold and side-chain angles on high-dimensional tori. Furthermore, we represent discrete residue types in the peptide sequence as categorical distributions on the probability simplex. By learning the joint distributions of each modality using derived flows and vector fields on corresponding manifolds, our method excels in the fine-grained design of full-atom peptides. Harnessing the multi-modal paradigm, our approach adeptly tackles various tasks such as fix-backbone sequence design and side-chain packing through partial sampling. Through meticulously crafted experiments, we demonstrate that PepFlow exhibits superior performance in comprehensive benchmarks, highlighting its significant potential in computational peptide design and analysis.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2403.08167.pdf' target='_blank'>https://arxiv.org/pdf/2403.08167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Xiao, Chao Cui, Huaisheng Zhu, Vasant G. Honavar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08167">MolBind: Multimodal Alignment of Language, Molecules, and Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2402.10516.pdf' target='_blank'>https://arxiv.org/pdf/2402.10516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Zhu, Zitai Kong, Jialu Wu, Weize Liu, Yuqiang Han, Mingze Yin, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10516">Generative AI for Controllable Protein Sequence Design: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finally, we identify the unresolved challenges and highlight research opportunities that merit deeper exploration.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2512.10147.pdf' target='_blank'>https://arxiv.org/pdf/2512.10147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarwan Ali, Taslim Murad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10147">Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2512.02328.pdf' target='_blank'>https://arxiv.org/pdf/2512.02328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Brad Wang, Siyuan Cao, Hongxuan Wu, Yiliang Yuan, Mustafa Misir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02328">Molecular Embedding-Based Algorithm Selection in Protein-Ligand Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Selecting an effective docking algorithm is highly context-dependent, and no single method performs reliably across structural, chemical, or protocol regimes. We introduce MolAS, a lightweight algorithm selection system that predicts per-algorithm performance from pretrained protein-ligand embeddings using attentional pooling and a shallow residual decoder. With only hundreds to a few thousand labelled complexes, MolAS achieves up to 15% absolute improvement over the single-best solver (SBS) and closes 17-66% of the Virtual Best Solver (VBS)-SBS gap across five diverse docking benchmarks. Analyses of reliability, embedding geometry, and solver-selection patterns show that MolAS succeeds when the oracle landscape exhibits low entropy and separable solver behaviour, but collapses under protocol-induced hierarchy shifts. These findings indicate that the main barrier to robust docking AS is not representational capacity but instability in solver rankings across pose-generation regimes, positioning MolAS as both a practical in-domain selector and a diagnostic tool for assessing when AS is feasible.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2511.22735.pdf' target='_blank'>https://arxiv.org/pdf/2511.22735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajun Yu, Guoping Xu, Steve Jiang, Robert Timmerman, John Minna, Yuanyuan Zhang, Hao Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22735">Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2511.05510.pdf' target='_blank'>https://arxiv.org/pdf/2511.05510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Xu, Di Wang, Zihan Zhou, Tianshu Yu, Mingchen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05510">TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the dynamic behavior of proteins is critical to elucidating their functional mechanisms, yet generating realistic, temporally coherent trajectories of protein ensembles remains a significant challenge. In this work, we introduce a novel hierarchical autoregressive framework for modeling protein dynamics that leverages the intrinsic multi-scale organization of molecular motions. Unlike existing methods that focus on generating static conformational ensembles or treat dynamic sampling as an independent process, our approach characterizes protein dynamics as a Markovian process. The framework employs a two-scale architecture: a low-resolution model captures slow, collective motions driving major conformational transitions, while a high-resolution model generates detailed local fluctuations conditioned on these large-scale movements. This hierarchical design ensures that the causal dependencies inherent in protein dynamics are preserved, enabling the generation of temporally coherent and physically realistic trajectories. By bridging high-level biophysical principles with state-of-the-art generative modeling, our approach provides an efficient framework for simulating protein dynamics that balances computational efficiency with physical accuracy.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2511.02042.pdf' target='_blank'>https://arxiv.org/pdf/2511.02042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Z. Haider, M. U. Ghouri, Tayyaba Noreen, M. Salman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02042">Quantum-Enhanced Generative Models for Rare Event Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare events such as financial crashes, climate extremes, and biological anomalies are notoriously difficult to model due to their scarcity and heavy-tailed distributions. Classical deep generative models often struggle to capture these rare occurrences, either collapsing low-probability modes or producing poorly calibrated uncertainty estimates. In this work, we propose the Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework that integrates deep latent-variable models with variational quantum circuits. The framework introduces two key innovations: (1) a hybrid loss function that jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2) quantum randomness-driven noise injection to enhance sample diversity and mitigate mode collapse. Training proceeds via a hybrid loop where classical parameters are updated through backpropagation while quantum parameters are optimized using parameter-shift gradients. We evaluate QEGM on synthetic Gaussian mixtures and real-world datasets spanning finance, climate, and protein structure. Results demonstrate that QEGM reduces tail KL divergence by up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion), while improving rare-event recall and coverage calibration. These findings highlight the potential of QEGM as a principled approach for rare-event prediction, offering robustness beyond what is achievable with purely classical methods.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2510.14989.pdf' target='_blank'>https://arxiv.org/pdf/2510.14989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob K. Christopher, Austin Seamann, Jingyi Cui, Sagar Khare, Ferdinando Fioretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14989">Constrained Diffusion for Protein Design with Hard Structural Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models offer a powerful means of capturing the manifold of realistic protein structures, enabling rapid design for protein engineering tasks. However, existing approaches observe critical failure modes when precise constraints are necessary for functional design. To this end, we present a constrained diffusion framework for structure-guided protein design, ensuring strict adherence to functional requirements while maintaining precise stereochemical and geometric feasibility. The approach integrates proximal feasibility updates with ADMM decomposition into the generative process, scaling effectively to the complex constraint sets of this domain. We evaluate on challenging protein design tasks, including motif scaffolding and vacancy-constrained pocket design, while introducing a novel curated benchmark dataset for motif scaffolding in the PDZ domain. Our approach achieves state-of-the-art, providing perfect satisfaction of bonding and geometric constraints with no degradation in structural diversity.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2509.26377.pdf' target='_blank'>https://arxiv.org/pdf/2509.26377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Cao, Hongxuan Wu, Jiabao Brad Wang, Yiliang Yuan, Mustafa Misir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26377">MC-GNNAS-Dock: Multi-criteria GNN-based Algorithm Selection for Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a core tool in drug discovery for predicting ligand-target interactions. Despite the availability of diverse search-based and machine learning approaches, no single docking algorithm consistently dominates, as performance varies by context. To overcome this challenge, algorithm selection frameworks such as GNNAS-Dock, built on graph neural networks, have been proposed. This study introduces an enhanced system, MC-GNNAS-Dock, with three key advances. First, a multi-criteria evaluation integrates binding-pose accuracy (RMSD) with validity checks from PoseBusters, offering a more rigorous assessment. Second, architectural refinements by inclusion of residual connections strengthen predictive robustness. Third, rank-aware loss functions are incorporated to sharpen rank learning. Extensive experiments are performed on a curated dataset containing approximately 3200 protein-ligand complexes from PDBBind. MC-GNNAS-Dock demonstrates consistently superior performance, achieving up to 5.4% (3.4%) gains under composite criteria of RMSD below 1Ã (2Ã ) with PoseBuster-validity compared to the single best solver (SBS) Uni-Mol Docking V2.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2509.20693.pdf' target='_blank'>https://arxiv.org/pdf/2509.20693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammadsaleh Refahi, Bahrad A. Sokhansanj, James R. Brown, Gail Rosen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20693">Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of drug-target binding affinity can accelerate drug discovery by prioritizing promising compounds before costly wet-lab screening. While deep learning has advanced this task, most models fuse ligand and protein representations via simple concatenation and lack explicit geometric regularization, resulting in poor generalization across chemical space and time. We introduce FIRM-DTI, a lightweight framework that conditions molecular embeddings on protein embeddings through a feature-wise linear modulation (FiLM) layer and enforces metric structure with a triplet loss. An RBF regression head operating on embedding distances yields smooth, interpretable affinity predictions. Despite its modest size, FIRM-DTI achieves state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark, as demonstrated by an extensive ablation study and out-of-domain evaluation. Our results underscore the value of conditioning and metric learning for robust drug-target affinity prediction.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2509.20345.pdf' target='_blank'>https://arxiv.org/pdf/2509.20345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20345">Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2509.14788.pdf' target='_blank'>https://arxiv.org/pdf/2509.14788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W. Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14788">Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2508.01799.pdf' target='_blank'>https://arxiv.org/pdf/2508.01799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gerald W. Y. Cheng, Zongxi Li, Jing Cai, Liang-ting Lin, Jung Sun Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01799">Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand interactions is essential for computer-aided drug discovery. However, existing methods often fail to capture solvent-dependent conformational changes and lack the ability to jointly learn multiple related tasks. To address these limitations, we introduce a pre-training method that incorporates ligand conformational ensembles generated under diverse solvent conditions as augmented input. This design enables the model to learn both structural flexibility and environmental context in a unified manner. The training process integrates molecular reconstruction to capture local geometry, interatomic distance prediction to model spatial relationships, and contrastive learning to build solvent-invariant molecular representations. Together, these components lead to significant improvements, including a 3.7% gain in binding affinity prediction, an 82% success rate on the PoseBusters Astex docking benchmarks, and an area under the curve of 97.1% in virtual screening. The framework supports solvent-aware, multi-task modeling and produces consistent results across benchmarks. A case study further demonstrates sub-angstrom docking accuracy with a root-mean-square deviation of 0.157 angstroms, offering atomic-level insight into binding mechanisms and advancing structure-based drug design.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2506.08023.pdf' target='_blank'>https://arxiv.org/pdf/2506.08023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Wu, Zhengzhe Liu, Han Zhu, Yizhou Zhao, Daisuke Kihara, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08023">Aligning Proteins and Language: A Foundation Model for Protein Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2411.12597.pdf' target='_blank'>https://arxiv.org/pdf/2411.12597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiliang Yuan, Mustafa Misir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12597">GNNAS-Dock: Budget Aware Algorithm Selection with Graph Neural Networks for Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a major element in drug discovery and design. It enables the prediction of ligand-protein interactions by simulating the binding of small molecules to proteins. Despite the availability of numerous docking algorithms, there is no single algorithm consistently outperforms the others across a diverse set of docking scenarios. This paper introduces GNNAS-Dock, a novel Graph Neural Network (GNN)-based automated algorithm selection system for molecular docking in blind docking situations. GNNs are accommodated to process the complex structural data of both ligands and proteins. They benefit from the inherent graph-like properties to predict the performance of various docking algorithms under different conditions. The present study pursues two main objectives: 1) predict the performance of each candidate docking algorithm, in terms of Root Mean Square Deviation (RMSD), thereby identifying the most accurate method for specific scenarios; and 2) choose the best computationally efficient docking algorithm for each docking case, aiming to reduce the time required for docking while maintaining high accuracy. We validate our approach on PDBBind 2020 refined set, which contains about 5,300 pairs of protein-ligand complexes.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2411.04165.pdf' target='_blank'>https://arxiv.org/pdf/2411.04165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Schmidinger, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, Pieter-Jan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, GÃ¼nter Klambauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04165">Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on the sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent xLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space model (SSM) architectures in the natural language domain. Similar to SSMs, xLSTMs have a linear runtime dependency on the sequence length and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor xLSTM towards these domains and propose a suite of architectural variants called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM's ability to model biological and chemical sequences. The results show that models based on Bio-xLSTM a) can serve as proficient generative models for DNA, protein, and chemical sequences, b) learn rich representations for those modalities, and c) can perform in-context learning for proteins and small molecules.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2411.00004.pdf' target='_blank'>https://arxiv.org/pdf/2411.00004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RafaÅ Powalski, Bazyli Klockiewicz, Maciej JaÅkowski, Bartosz Topolski, PaweÅ DÄbrowski-TumaÅski, Maciej WiÅniewski, Åukasz KuciÅski, Piotr MiÅoÅ, Dariusz Plewczynski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00004">RapidDock: Unlocking Proteome-scale Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accelerating molecular docking -- the process of predicting how molecules bind to protein targets -- could boost small-molecule drug discovery and revolutionize medicine. Unfortunately, current molecular docking tools are too slow to screen potential drugs against all relevant proteins, which often results in missed drug candidates or unexpected side effects occurring in clinical trials. To address this gap, we introduce RapidDock, an efficient transformer-based model for blind molecular docking. RapidDock achieves at least a $100 \times$ speed advantage over existing methods without compromising accuracy. On the Posebusters and DockGen benchmarks, our method achieves $52.1\%$ and $44.0\%$ success rates ($\text{RMSD}<2$Ã), respectively. The average inference time is $0.04$ seconds on a single GPU, highlighting RapidDock's potential for large-scale docking studies. We examine the key features of RapidDock that enable leveraging the transformer architecture for molecular docking, including the use of relative distance embeddings of $3$D structures in attention matrices, pre-training on protein folding, and a custom loss function invariant to molecular symmetries.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2408.16978.pdf' target='_blank'>https://arxiv.org/pdf/2408.16978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, Hari Subramoni, Dhabaleswar K. Panda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16978">Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2407.13981.pdf' target='_blank'>https://arxiv.org/pdf/2407.13981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwei Cheng, Xiangxin Zhou, Yuwei Yang, Yu Bao, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13981">Decomposed Direct Preference Optimization for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved promising results for Structure-Based Drug Design (SBDD). Nevertheless, high-quality protein subpocket and ligand data are relatively scarce, which hinders the models' generation capabilities. Recently, Direct Preference Optimization (DPO) has emerged as a pivotal tool for aligning generative models with human preferences. In this paper, we propose DecompDPO, a structure-based optimization method aligns diffusion models with pharmaceutical needs using multi-granularity preference pairs. DecompDPO introduces decomposition into the optimization objectives and obtains preference pairs at the molecule or decomposed substructure level based on each objective's decomposability. Additionally, DecompDPO introduces a physics-informed energy term to ensure reasonable molecular conformations in the optimization results. Notably, DecompDPO can be effectively used for two main purposes: (1) fine-tuning pretrained diffusion models for molecule generation across various protein families, and (2) molecular optimization given a specific protein subpocket after generation. Extensive experiments on the CrossDocked2020 benchmark show that DecompDPO significantly improves model performance, achieving up to 95.2% Med. High Affinity and a 36.2% success rate for molecule generation, and 100% Med. High Affinity and a 52.1% success rate for molecular optimization.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2406.04239.pdf' target='_blank'>https://arxiv.org/pdf/2406.04239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Levy, Eric R. Chan, Sara Fridovich-Keil, FrÃ©dÃ©ric Poitevin, Ellen D. Zhong, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04239">Solving Inverse Problems in Protein Space Using Diffusion-Based Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interaction of a protein with its environment can be understood and controlled via its 3D structure. Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems. Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement. Here, we introduce a versatile framework to turn biophysical measurements, such as cryo-EM density maps, into 3D atomic models. Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior. Our method outperforms posterior sampling baselines on linear and non-linear inverse problems. In particular, it is the first diffusion-based method for refining atomic models from cryo-EM maps and building atomic models from sparse distance matrices.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2406.01636.pdf' target='_blank'>https://arxiv.org/pdf/2406.01636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Akmal Raheem, Muhammad Ajwad Rahim, Ijaz Gul, Md. Reyad-ul-Ferdous, Liyan Le, Junguo Hui, Shuiwei Xia, Minjiang Chen, Dongmei Yu, Vijay Pandey, Peiwu Qin, Jiansong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01636">COVID-19: post infection implications in different age groups, mechanism, diagnosis, effective prevention, treatment, and recommendations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SARS-CoV-2, the highly contagious pathogen responsible for the COVID-19 pandemic, has persistent effects that begin four weeks after initial infection and last for an undetermined duration. These chronic effects are more harmful than acute ones. This review explores the long-term impact of the virus on various human organs, including the pulmonary, cardiovascular, neurological, reproductive, gastrointestinal, musculoskeletal, endocrine, and lymphoid systems, particularly in older adults. Regarding diagnosis, RT-PCR is the gold standard for detecting COVID-19, though it requires specialized equipment, skilled personnel, and considerable time to produce results. To address these limitations, artificial intelligence in imaging and microfluidics technologies offers promising alternatives for diagnosing COVID-19 efficiently. Pharmacological and non-pharmacological strategies are effective in mitigating the persistent impacts of COVID-19. These strategies enhance immunity in post-COVID-19 patients by reducing cytokine release syndrome, improving T cell response, and increasing the circulation of activated natural killer and CD8 T cells in blood and tissues. This, in turn, alleviates symptoms such as fever, nausea, fatigue, muscle weakness, and pain. Vaccines, including inactivated viral, live attenuated viral, protein subunit, viral vectored, mRNA, DNA, and nanoparticle vaccines, significantly reduce the adverse long-term effects of the virus. However, no vaccine has been reported to provide lifetime protection against COVID-19. Consequently, protective measures such as physical distancing, mask usage, and hand hygiene remain essential strategies. This review offers a comprehensive understanding of the persistent effects of COVID-19 on individuals of varying ages, along with insights into diagnosis, treatment, vaccination, and future preventative measures against the spread of SARS-CoV-2.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2405.08226.pdf' target='_blank'>https://arxiv.org/pdf/2405.08226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asim Waqas, Aakash Tripathi, Sabeen Ahmed, Ashwin Mukund, Hamza Farooq, Matthew B. Schabath, Paul Stewart, Mia Naeini, Ghulam Rasool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08226">Self-Normalizing Foundation Model for Enhanced Multi-Omics Data Analysis in Oncology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-omics research has enhanced our understanding of cancer heterogeneity and progression. Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling more effective diagnosis, treatment, and prevention strategies. However, predicting patient outcomes through the integration of all available multi-omics data is still an under-study research direction. Here, we present SeNMo, a foundation model that has been trained on multi-omics data across 33 cancer types. SeNMo is particularly efficient in handling multi-omics data characterized by high-width and low-length attributes. We trained SeNMo for the task of overall survival of patients using pan-cancer multi-omics data involving 33 cancer sites from the GDC. The training multi-omics data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data. SeNMo was validated on two independent cohorts: Moffitt Cancer Center and CPTAC lung squamous cell carcinoma. We evaluated the model's performance in predicting patient's overall survival using the C-Index. SeNMo performed consistently well in the training regime, reflected by the validation C-Index of 0.76 on GDC's public data. In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set. The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort. SeNMo demonstrated robust performance on the classification task of predicting the primary cancer type of patients. SeNMo further demonstrated significant performance in predicting tertiary lymph structures from multi-omics data, showing generalizability across cancer types, molecular data types, and clinical endpoints.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2404.14970.pdf' target='_blank'>https://arxiv.org/pdf/2404.14970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rita T. Sousa, Heiko Paulheim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14970">Integrating Heterogeneous Gene Expression Data through Knowledge Graphs for Improving Diabetes Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetes is a worldwide health issue affecting millions of people. Machine learning methods have shown promising results in improving diabetes prediction, particularly through the analysis of diverse data types, namely gene expression data. While gene expression data can provide valuable insights, challenges arise from the fact that the sample sizes in expression datasets are usually limited, and the data from different datasets with different gene expressions cannot be easily combined.
  This work proposes a novel approach to address these challenges by integrating multiple gene expression datasets and domain-specific knowledge using knowledge graphs, a unique tool for biomedical data integration. KG embedding methods are then employed to generate vector representations, serving as inputs for a classifier. Experiments demonstrated the efficacy of our approach, revealing improvements in diabetes prediction when integrating multiple gene expression datasets and domain-specific knowledge about protein functions and interactions.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2404.07194.pdf' target='_blank'>https://arxiv.org/pdf/2404.07194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Sestak, Lisa Schneckenreiter, Johannes Brandstetter, Sepp Hochreiter, Andreas Mayr, GÃ¼nter Klambauer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07194">VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs. Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions. Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E(3)-equivariant predictions. Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction. However, the performance of GNNs at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets. In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme. The virtual nodes in these graphs are dedicated quantities to learn representations of binding sites, which leads to improved predictive performance. In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2403.06890.pdf' target='_blank'>https://arxiv.org/pdf/2403.06890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debarshi Kundu, Archisman Ghosh, Srinivasan Ekambaram, Jian Wang, Nikolay Dokholyan, Swaroop Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06890">Application of Quantum Tensor Networks for Protein Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2512.17815.pdf' target='_blank'>https://arxiv.org/pdf/2512.17815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyan Zhao, Yi-Ching Tang, Rivaaj Monsia, Victor J. Cantu, Ashwin Kumar Ramesh, Xiaozhong Liu, Zhiqiang An, Xiaoqian Jiang, Yejin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17815">Structure-Aware Antibody Design with Affinity-Optimized Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: The clinical efficacy of antibody therapeutics critically depends on high-affinity target engagement, yet laboratory affinity-maturation campaigns are slow and costly. In computational settings, most protein language models (PLMs) are not trained to favor high-affinity antibodies, and existing preference optimization approaches introduce substantial computational overhead without clear affinity gains. Therefore, this work proposes SimBinder-IF, which converts the inverse folding model ESM-IF into an antibody sequence generator by freezing its structure encoder and training only its decoder to prefer experimentally stronger binders through preference optimization. Results: On the 11-assay AbBiBench benchmark, SimBinder-IF achieves a 55 percent relative improvement in mean Spearman correlation between log-likelihood scores and experimentally measured binding affinity compared to vanilla ESM-IF (from 0.264 to 0.410). In zero-shot generalization across four unseen antigen-antibody complexes, the correlation improves by 156 percent (from 0.115 to 0.294). SimBinder-IF also outperforms baselines in top-10 precision for ten-fold or greater affinity improvements. A case study redesigning antibody F045-092 for A/California/04/2009 (pdmH1N1) shows that SimBinder-IF proposes variants with substantially lower predicted binding free energy changes than ESM-IF (mean Delta Delta G -75.16 vs -46.57). Notably, SimBinder-IF trains only about 18 percent of the parameters of the full ESM-IF model, highlighting its parameter efficiency for high-affinity antibody generation.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2512.09329.pdf' target='_blank'>https://arxiv.org/pdf/2512.09329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Tavakoli, Raswanth Murugan, Ozan Gokdemir, Arvind Ramanathan, Frances Arnold, Anima Anandkumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09329">Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2510.17716.pdf' target='_blank'>https://arxiv.org/pdf/2510.17716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suqiang Ma, Subhadeep Sengupta, Yao Lee, Beikang Gu, Xianyan Chen, Xianqiao Wang, Yang Liu, Mengjia Xu, Galit H. Frydman, He Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17716">Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Circulating blood cell clusters (CCCs) containing red blood cells (RBCs), white blood cells(WBCs), and platelets are significant biomarkers linked to conditions like thrombosis, infection, and inflammation. Flow cytometry, paired with fluorescence staining, is commonly used to analyze these cell clusters, revealing cell morphology and protein profiles. While computational approaches based on machine learning have advanced the automatic analysis of single-cell flow cytometry images, there is a lack of effort to build tools to automatically analyze images containing CCCs. Unlike single cells, cell clusters often exhibit irregular shapes and sizes. In addition, these cell clusters often consist of heterogeneous cell types, which require multi-channel staining to identify the specific cell types within the clusters. This study introduces a new computational framework for analyzing CCC images and identifying cell types within clusters. Our framework uses a two-step analysis strategy. First, it categorizes images into cell cluster and non-cluster groups by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms traditional convolutional neural networks (CNNs), Vision Transformers (ViT). Then, it identifies cell types by overlaying cluster contours with regions from multi-channel fluorescence stains, enhancing accuracy despite cell debris and staining artifacts. This approach achieved over 95% accuracy in both cluster classification and phenotype identification. In summary, our automated framework effectively analyzes CCC images from flow cytometry, leveraging both bright-field and fluorescence data. Initially tested on blood cells, it holds potential for broader applications, such as analyzing immune and tumor cell clusters, supporting cellular research across various diseases.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2510.05849.pdf' target='_blank'>https://arxiv.org/pdf/2510.05849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adhithyan Kalaivanan, Zheng Zhao, Jens Sjölund, Fredrik Lindsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05849">ESS-Flow: Training-free guidance of flow-based models as inference in source space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2509.05302.pdf' target='_blank'>https://arxiv.org/pdf/2509.05302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RaÃºl MiÃ±Ã¡n, Carles Perez-Lopez, Javier Iglesias, Ãlvaro Ciudad, Alexis Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05302">Sesame: Opening the door to protein pockets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a cornerstone of drug discovery, relying on high-resolution ligand-bound structures to achieve accurate predictions. However, obtaining these structures is often costly and time-intensive, limiting their availability. In contrast, ligand-free structures are more accessible but suffer from reduced docking performance due to pocket geometries being less suited for ligand accommodation in apo structures. Traditional methods for artificially inducing these conformations, such as molecular dynamics simulations, are computationally expensive. In this work, we introduce Sesame, a generative model designed to predict this conformational change efficiently. By generating geometries better suited for ligand accommodation at a fraction of the computational cost, Sesame aims to provide a scalable solution for improving virtual screening workflows.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2508.15321.pdf' target='_blank'>https://arxiv.org/pdf/2508.15321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Spirandelli, Arnur Nigmetov, Dmitriy Morozov, Myfanwy E. Evans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15321">Topological potentials guiding protein self-assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The simulated self-assembly of molecular building blocks into functional complexes is a key area of study in computational biology and materials science. Self-assembly simulations of proteins using physically-motivated potentials for non-polar interactions, can identify the biologically correct assembly as the energy-minimizing state. Short-range potentials, however, produce rugged energy landscapes, which lead to simulations becoming trapped in non-functional local minimizers. Successful self-assembly simulations depend on the physical realism of the driving potentials as well as their ability to efficiently explore the configuration space. We introduce a long-range topological potential, quantified via weighted total persistence, and combine it with the morphometric approach to solvation-free energy. This combination improves the assembly success rate in simulations of the tobacco mosaic virus dimer and other protein complexes by up to sixteen-fold compared with the morphometric model alone. It further enables successful simulation in systems that don't otherwise assemble during the examined timescales. Compared to previous topology-based work, which has been primarily descriptive, our approach uses topological measures as an active energetic bias that is independent of electrostatics or chemical specificity and depends only on atomic coordinates. Therefore, the method can, in principle, be applied to arbitrary systems where such coordinates are optimized. Integrating topological descriptions into an energy function offers a general strategy for overcoming kinetic barriers in molecular simulations, with potential applications in drug design, materials development, and the study of complex self-assembly processes.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2508.10841.pdf' target='_blank'>https://arxiv.org/pdf/2508.10841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viktor Zaverkin, Matheus Ferraz, Francesco Alesiani, Mathias Niepert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10841">Performance of universal machine-learned potentials with explicit long-range interactions in biomolecular simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal machine-learned potentials promise transferable accuracy across compositional and vibrational degrees of freedom, yet their application to biomolecular simulations remains underexplored. This work systematically evaluates equivariant message-passing architectures trained on the SPICE-v2 dataset with and without explicit long-range dispersion and electrostatics. We assess the impact of model size, training data composition, and electrostatic treatment across in- and out-of-distribution benchmark datasets, as well as molecular simulations of bulk liquid water, aqueous NaCl solutions, and biomolecules, including alanine tripeptide, the mini-protein Trp-cage, and Crambin. While larger models improve accuracy on benchmark datasets, this trend does not consistently extend to properties obtained from simulations. Predicted properties also depend on the composition of the training dataset. Long-range electrostatics show no systematic impact across systems. However, for Trp-cage, their inclusion yields increased conformational variability. Our results suggest that imbalanced datasets and immature evaluation practices currently challenge the applicability of universal machine-learned potentials to biomolecular simulations.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2507.14245.pdf' target='_blank'>https://arxiv.org/pdf/2507.14245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengjie Yu, Kenneth A. Dawson, Haiyun Yang, Shuya Liu, Yan Yan, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14245">A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2507.02883.pdf' target='_blank'>https://arxiv.org/pdf/2507.02883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Zeng, Tuo Wang, Adithya Kulkarni, Alexander Lu, Alexandra Ni, Phoebe Xing, Junhan Zhao, Siwei Chen, Dawei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02883">DISPROTBENCH: A Disorder-Aware, Task-Rich Benchmark for Evaluating Protein Structure Prediction in Realistic Biological Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein structure prediction have achieved near-atomic accuracy for well-folded proteins. However, current benchmarks inadequately assess model performance in biologically challenging contexts, especially those involving intrinsically disordered regions (IDRs), limiting their utility in applications such as drug discovery, disease variant interpretation, and protein interface design. We introduce DisProtBench, a comprehensive benchmark for evaluating protein structure prediction models (PSPMs) under structural disorder and complex biological conditions. DisProtBench spans three key axes: (1) Data complexity, covering disordered regions, G protein-coupled receptor (GPCR) ligand pairs, and multimeric complexes; (2) Task diversity, benchmarking twelve leading PSPMs across structure-based tasks with unified classification, regression, and interface metrics; and (3) Interpretability, via the DisProtBench Portal, which provides precomputed 3D structures and visual error analyses. Our results reveal significant variability in model robustness under disorder, with low-confidence regions linked to functional prediction failures. Notably, global accuracy metrics often fail to predict task performance in disordered settings, emphasizing the need for function-aware evaluation. DisProtBench establishes a reproducible, extensible, and biologically grounded framework for assessing next-generation PSPMs in realistic biomedical scenarios.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2506.04235.pdf' target='_blank'>https://arxiv.org/pdf/2506.04235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyan Zhao, Yi-Ching Tang, Akshita Singh, Victor J Cantu, KwanHo An, Junseok Lee, Adam E Stogsdill, Ashwin Kumar Ramesh, Zhiqiang An, Xiaoqian Jiang, Yejin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04235">Benchmark for Antibody Binding Affinity Maturation and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike existing antibody evaluation strategies that rely on antibody alone and its similarity to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench considers an antibody-antigen (Ab-Ag) complex as a functional unit and evaluates the potential of an antibody design binding to given antigen by measuring protein model's likelihood on the Ab-Ag complex. We first curate, standardize, and share 9 datasets containing 9 antigens (involving influenza, anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain mutated antibodies. Using these datasets, we systematically compare 14 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models. The correlation between model likelihood and experimental affinity values is used to evaluate model performance. Additionally, in a case study to increase binding affinity of antibody F045-092 to antigen influenza H1N1, we evaluate the generative power of the top-performing models by sampling a set of new antibodies binding to the antigen and ranking them based on structural integrity and biophysical properties of the Ab-Ag complex. As a result, structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2505.21873.pdf' target='_blank'>https://arxiv.org/pdf/2505.21873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Gao, Jun Li, Jing Hu, Shanzhuo Zhang, Kunrui Zhu, Yueyang Huang, Xiaonan Zhang, Xiaomin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21873">HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein binder design is central to therapeutics, diagnostics, and synthetic biology, yet practical deployment remains challenging due to fragmented workflows, high computational costs, and complex tool integration. We present HelixDesign-Binder, a production-grade, high-throughput platform built on HelixFold3 that automates the full binder design pipeline, from backbone generation and sequence design to structural evaluation and multi-dimensional scoring. By unifying these stages into a scalable and user-friendly system, HelixDesign-Binder enables efficient exploration of binder candidates with favorable structural, energetic, and physicochemical properties. The platform leverages Baidu Cloud's high-performance infrastructure to support large-scale design and incorporates advanced scoring metrics, including ipTM, predicted binding free energy, and interface hydrophobicity. Benchmarking across six protein targets demonstrates that HelixDesign-Binder reliably produces diverse and high-quality binders, some of which match or exceed validated designs in predicted binding affinity. HelixDesign-Binder is accessible via an interactive web interface in PaddleHelix platform, supporting both academic research and industrial applications in antibody and protein binder development.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2505.20251.pdf' target='_blank'>https://arxiv.org/pdf/2505.20251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophia Hager, Aleem Khan, Andrew Wang, Nicholas Andrews
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20251">Learning Extrapolative Sequence Transformations from Markov Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most successful applications of deep learning involve similar training and test conditions. However, tasks such as biological sequence design involve searching for sequences that improve desirable properties beyond previously known values, which requires novel hypotheses that \emph{extrapolate} beyond training data. In these settings, extrapolation may be achieved by using random search methods such as Markov chain Monte Carlo (MCMC), which, given an initial state, sample local transformations to approximate a target density that rewards states with the desired properties. However, even with a well-designed proposal, MCMC may struggle to explore large structured state spaces efficiently. Rather than relying on stochastic search, it would be desirable to have a model that greedily optimizes the properties of interest, successfully extrapolating in as few steps as possible. We propose to learn such a model from the Markov chains resulting from MCMC search. Specifically, our approach uses selected states from Markov chains as a source of training data for an autoregressive model, which is then able to efficiently generate novel sequences that extrapolate along the sequence-level properties of interest. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the autoregressive model can extrapolate as well or better than MCMC, but with the additional benefits of scalability and significantly higher sample efficiency.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2503.17361.pdf' target='_blank'>https://arxiv.org/pdf/2503.17361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17361">Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2503.15853.pdf' target='_blank'>https://arxiv.org/pdf/2503.15853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashkan Dehghan, PaweÅ PraÅat, FranÃ§ois ThÃ©berge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15853">Network Embedding Exploration Tool (NEExT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world and artificial systems and processes can be represented as graphs. Some examples of such systems include social networks, financial transactions, supply chains, and molecular structures. In many of these cases, one needs to consider a collection of graphs, rather than a single network. This could be a collection of distinct but related graphs, such as different protein structures or graphs resulting from dynamic processes on the same network. Examples of the latter include the evolution of social networks, community-induced graphs, or ego-nets around various nodes. A significant challenge commonly encountered is the absence of ground-truth labels for graphs or nodes, necessitating the use of unsupervised techniques to analyze such systems. Moreover, even when ground-truth labels are available, many existing graph machine learning methods depend on complex deep learning models, complicating model explainability and interpretability. To address some of these challenges, we have introduced NEExT (Network Embedding Exploration Tool) for embedding collections of graphs via user-defined node features. The advantages of the framework are twofold: (i) the ability to easily define your own interpretable node-based features in view of the task at hand, and (ii) fast embedding of graphs provided by the Vectorizers library. In this paper, we demonstrate the usefulness of NEExT on collections of synthetic and real-world graphs. For supervised tasks, we demonstrate that performance in graph classification tasks could be achieved similarly to other state-of-the-art techniques while maintaining model interpretability. Furthermore, our framework can also be used to generate high-quality embeddings in an unsupervised way, where target variables are not available.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2503.04362.pdf' target='_blank'>https://arxiv.org/pdf/2503.04362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Zhu, Mingyang Li, Junlong Liu, Kun Fu, Jiansheng Wu, Qiuyi Li, Mingze Yin, Jieping Ye, Jian Wu, Zheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04362">A Generalist Cross-Domain Molecular Learning Framework for Structure-Based Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug discovery (SBDD) is a systematic scientific process that develops new drugs by leveraging the detailed physical structure of the target protein. Recent advancements in pre-trained models for biomolecules have demonstrated remarkable success across various biochemical applications, including drug discovery and protein engineering. However, in most approaches, the pre-trained models primarily focus on the characteristics of either small molecules or proteins, without delving into their binding interactions which are essential cross-domain relationships pivotal to SBDD. To fill this gap, we propose a general-purpose foundation model named BIT (an abbreviation for Biomolecular Interaction Transformer), which is capable of encoding a range of biochemical entities, including small molecules, proteins, and protein-ligand complexes, as well as various data formats, encompassing both 2D and 3D structures. Specifically, we introduce Mixture-of-Domain-Experts (MoDE) to handle the biomolecules from diverse biochemical domains and Mixture-of-Structure-Experts (MoSE) to capture positional dependencies in the molecular structures. The proposed mixture-of-experts approach enables BIT to achieve both deep fusion and domain-specific encoding, effectively capturing fine-grained molecular interactions within protein-ligand complexes. Then, we perform cross-domain pre-training on the shared Transformer backbone via several unified self-supervised denoising tasks. Experimental results on various benchmarks demonstrate that BIT achieves exceptional performance in downstream tasks, including binding affinity prediction, structure-based virtual screening, and molecular property prediction.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2503.00975.pdf' target='_blank'>https://arxiv.org/pdf/2503.00975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanlue Li, Chenran Jiang, Ziqi Gao, Yu Liu, Chenyang Liu, Jiean Chen, Yong Huang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00975">Molecule Generation for Target Protein Binding with Hierarchical Consistency Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective generation of molecular structures, or new chemical entities, that bind to target proteins is crucial for lead identification and optimization in drug discovery. Despite advancements in atom- and motif-wise deep learning models for 3D molecular generation, current methods often struggle with validity and reliability. To address these issues, we develop the Atom-Motif Consistency Diffusion Model (AMDiff), utilizing a joint-training paradigm for multi-view learning. This model features a hierarchical diffusion architecture that integrates both atom- and motif-level views of molecules, allowing for comprehensive exploration of complementary information. By leveraging classifier-free guidance and incorporating binding site features as conditional inputs, AMDiff ensures robust molecule generation across diverse targets. Compared to existing approaches, AMDiff exhibits superior validity and novelty in generating molecules tailored to fit various protein pockets. Case studies targeting protein kinases, including Anaplastic Lymphoma Kinase (ALK) and Cyclin-dependent kinase 4 (CDK4), demonstrate the model's capability in structure-based de novo drug design. Overall, AMDiff bridges the gap between atom-view and motif-view drug discovery and speeds up the process of target-aware molecular generation.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2502.00529.pdf' target='_blank'>https://arxiv.org/pdf/2502.00529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arijit Khan, Xiangyu Ke, Yinghui Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00529">Graph Data Management and Graph Machine Learning: Synergies and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ubiquity of machine learning, particularly deep learning, applied to graphs is evident in applications ranging from cheminformatics (drug discovery) and bioinformatics (protein interaction prediction) to knowledge graph-based query answering, fraud detection, and social network analysis. Concurrently, graph data management deals with the research and development of effective, efficient, scalable, robust, and user-friendly systems and algorithms for storing, processing, and analyzing vast quantities of heterogeneous and complex graph data. Our survey provides a comprehensive overview of the synergies between graph data management and graph machine learning, illustrating how they intertwine and mutually reinforce each other across the entire spectrum of the graph data science and machine learning pipeline. Specifically, the survey highlights two crucial aspects: (1) How graph data management enhances graph machine learning, including contributions such as improved graph neural network performance through graph data cleaning, scalable graph embedding, efficient graph-based vector data management, robust graph neural networks, user-friendly explainability methods; and (2) how graph machine learning, in turn, aids in graph data management, with a focus on applications like query answering over knowledge graphs and various data science tasks. We discuss pertinent open problems and delineate crucial research directions.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2412.21154.pdf' target='_blank'>https://arxiv.org/pdf/2412.21154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Narayanan, James D. Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori Kabeli, Geemi Wellawatte, Sam Cox, Samuel G. Rodriques, Andrew D. White
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.21154">Aviary: training language agents on challenging scientific tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving complex real-world tasks requires cycles of actions and observations. This is particularly true in science, where tasks require many cycles of analysis, tool use, and experimentation. Language agents are promising for automating intellectual tasks in science because they can interact with tools via natural language or code. Yet their flexibility creates conceptual and practical challenges for software implementations, since agents may comprise non-standard components such as internal reasoning, planning, tool usage, as well as the inherent stochasticity of temperature-sampled language models. Here, we introduce Aviary, an extensible gymnasium for language agents. We formalize agents as policies solving language-grounded partially observable Markov decision processes, which we term language decision processes. We then implement five environments, including three challenging scientific environments: (1) manipulating DNA constructs for molecular cloning, (2) answering research questions by accessing scientific literature, and (3) engineering protein stability. These environments were selected for their focus on multi-step reasoning and their relevance to contemporary biology research. Finally, with online training and scaling inference-time compute, we show that language agents backed by open-source, non-frontier LLMs can match and exceed both frontier LLM agents and human experts on multiple tasks at up to 100x lower inference cost.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2412.17240.pdf' target='_blank'>https://arxiv.org/pdf/2412.17240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Zang, Lingfei Ren, Yue Li, Zhikang Wang, David Antony Selby, Zheng Wang, Sebastian Josef Vollmer, Hongzhi Yin, Jiangning Song, Junhang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17240">Rethinking Cancer Gene Identification through Graph Anomaly Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2411.15624.pdf' target='_blank'>https://arxiv.org/pdf/2411.15624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxin Zhao, Cong Ma, Mladen Kolar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15624">Trans-Glasso: A Transfer Learning Approach to Precision Matrix Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision matrix estimation is essential in various fields, yet it is challenging when samples for the target study are limited. Transfer learning can enhance estimation accuracy by leveraging data from related source studies. We propose Trans-Glasso, a two-step transfer learning method for precision matrix estimation. First, we obtain initial estimators using a multi-task learning objective that captures shared and unique features across studies. Then, we refine these estimators through differential network estimation to adjust for structural differences between the target and source precision matrices. Under the assumption that most entries of the target precision matrix are shared with source matrices, we derive non-asymptotic error bounds and show that Trans-Glasso achieves minimax optimality under certain conditions. Extensive simulations demonstrate Trans Glasso's superior performance compared to baseline methods, particularly in small-sample settings. We further validate Trans-Glasso in applications to gene networks across brain tissues and protein networks for various cancer subtypes, showcasing its effectiveness in biological contexts. Additionally, we derive the minimax optimal rate for differential network estimation, representing the first such guarantee in this area.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2411.11808.pdf' target='_blank'>https://arxiv.org/pdf/2411.11808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AdriÃ¡n Morales-Pastor, Raquel VÃ¡zquez-Reza, MiÅosz WieczÃ³r, ClÃ udia Valverde, Manel Gil-Sorribes, Bertran Miquel-Oliver, Ãlvaro Ciudad, Alexis Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11808">Character-level Tokenizations as Powerful Inductive Biases for RNA Foundational Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RNA is a vital biomolecule with numerous roles and functions within cells, and interest in targeting it for therapeutic purposes has grown significantly in recent years. However, fully understanding and predicting RNA behavior, particularly for applications in drug discovery, remains a challenge due to the complexity of RNA structures and interactions. While foundational models in biology have demonstrated success in modeling several biomolecules, especially proteins, achieving similar breakthroughs for RNA has proven more difficult. Current RNA models have yet to match the performance observed in the protein domain, leaving an important gap in computational biology. In this work, we present ChaRNABERT, a suite of sample and parameter-efficient RNA foundational models, that through a learnable tokenization process, are able to reach state-of-the-art performance on several tasks in established benchmarks. We extend its testing in relevant downstream tasks such as RNA-protein and aptamer-protein interaction prediction. Weights and inference code for ChaRNABERT-8M will be provided for academic research use. The other models will be available upon request.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2411.11061.pdf' target='_blank'>https://arxiv.org/pdf/2411.11061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoliang Luo, Michael Ramscar, Bradley C. Love
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11061">Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impressive performance of large language models (LLMs) has led to their consideration as models of human language processing. Instead, we suggest that the success of LLMs arises from the flexibility of the transformer learning architecture. To evaluate this conjecture, we trained LLMs on scientific texts that were either in a forward or backward format. Despite backward text being inconsistent with the structure of human languages, we found that LLMs performed equally well in either format on a neuroscience benchmark, eclipsing human expert performance for both forward and backward orders. Our results are consistent with the success of transformers across diverse domains, such as weather prediction and protein design. This widespread success is attributable to LLM's ability to extract predictive patterns from any sufficiently structured input. Given their generality, we suggest caution in interpreting LLM's success in linguistic tasks as evidence for human-like mechanisms.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2409.03773.pdf' target='_blank'>https://arxiv.org/pdf/2409.03773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rong Han, Xiaohong Liu, Tong Pan, Jing Xu, Xiaoyu Wang, Wuyang Lan, Zhenyu Li, Zixuan Wang, Jiangning Song, Guangyu Wang, Ting Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03773">CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately measuring protein-RNA binding affinity is crucial in many biological processes and drug design. Previous computational methods for protein-RNA binding affinity prediction rely on either sequence or structure features, unable to capture the binding mechanisms comprehensively. The recent emerging pre-trained language models trained on massive unsupervised sequences of protein and RNA have shown strong representation ability for various in-domain downstream tasks, including binding site prediction. However, applying different-domain language models collaboratively for complex-level tasks remains unexplored. In this paper, we propose CoPRA to bridge pre-trained language models from different biological domains via Complex structure for Protein-RNA binding Affinity prediction. We demonstrate for the first time that cross-biological modal language models can collaborate to improve binding affinity prediction. We propose a Co-Former to combine the cross-modal sequence and structure information and a bi-scope pre-training strategy for improving Co-Former's interaction understanding. Meanwhile, we build the largest protein-RNA binding affinity dataset PRA310 for performance evaluation. We also test our model on a public dataset for mutation effect prediction. CoPRA reaches state-of-the-art performance on all the datasets. We provide extensive analyses and verify that CoPRA can (1) accurately predict the protein-RNA binding affinity; (2) understand the binding affinity change caused by mutations; and (3) benefit from scaling data and model size.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2408.16975.pdf' target='_blank'>https://arxiv.org/pdf/2408.16975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihang Liu, Shanzhuo Zhang, Yang Xue, Xianbin Ye, Kunrui Zhu, Yuxin Li, Yang Liu, Jie Gao, Wenlai Zhao, Hongkun Yu, Zhihua Wu, Xiaonan Zhang, Xiaomin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16975">Technical Report of HelixFold3 for Biomolecular Structure Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The AlphaFold series has transformed protein structure prediction with remarkable accuracy, often matching experimental methods. AlphaFold2, AlphaFold-Multimer, and the latest AlphaFold3 represent significant strides in predicting single protein chains, protein complexes, and biomolecular structures. While AlphaFold2 and AlphaFold-Multimer are open-sourced, facilitating rapid and reliable predictions, AlphaFold3 remains partially accessible through a limited online server and has not been open-sourced, restricting further development. To address these challenges, the PaddleHelix team is developing HelixFold3, aiming to replicate AlphaFold3's capabilities. Leveraging insights from previous models and extensive datasets, HelixFold3 achieves accuracy comparable to AlphaFold3 in predicting the structures of the conventional ligands, nucleic acids, and proteins. The initial release of HelixFold3 is available as open source on GitHub for academic research, promising to advance biomolecular research and accelerate discoveries. The latest version will be continuously updated on the HelixFold3 web server, providing both interactive visualization and API access.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2408.00892.pdf' target='_blank'>https://arxiv.org/pdf/2408.00892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thuong Le Hoai Pham, Jillur Rahman Saurav, Aisosa A. Omere, Calvin J. Heyl, Mohammad Sadegh Nasr, Cody Tyler Reynolds, Jai Prakash Yadav Veerla, Helen H Shang, Justyn Jaworski, Alison Ravenscraft, Joseph Anthony Buonomo, Jacob M. Luber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00892">Peptide Sequencing Via Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a protein language model for determining the complete sequence of a peptide based on measurement of a limited set of amino acids. To date, protein sequencing relies on mass spectrometry, with some novel edman degregation based platforms able to sequence non-native peptides. Current protein sequencing techniques face limitations in accurately identifying all amino acids, hindering comprehensive proteome analysis. Our method simulates partial sequencing data by selectively masking amino acids that are experimentally difficult to identify in protein sequences from the UniRef database. This targeted masking mimics real-world sequencing limitations. We then modify and finetune a ProtBert derived transformer-based model, for a new downstream task predicting these masked residues, providing an approximation of the complete sequence. Evaluating on three bacterial Escherichia species, we achieve per-amino-acid accuracy up to 90.5% when only four amino acids ([KCYM]) are known. Structural assessment using AlphaFold and TM-score validates the biological relevance of our predictions. The model also demonstrates potential for evolutionary analysis through cross-species performance. This integration of simulated experimental constraints with computational predictions offers a promising avenue for enhancing protein sequence analysis, potentially accelerating advancements in proteomics and structural biology by providing a probabilistic reconstruction of the complete protein sequence from limited experimental data.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2407.10362.pdf' target='_blank'>https://arxiv.org/pdf/2407.10362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, Samuel G. Rodriques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10362">LAB-Bench: Measuring Capabilities of Language Models for Biology Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is widespread optimism that frontier Large Language Models (LLMs) and LLM-augmented systems have the potential to rapidly accelerate scientific discovery across disciplines. Today, many benchmarks exist to measure LLM knowledge and reasoning on textbook-style science questions, but few if any benchmarks are designed to evaluate language model performance on practical tasks required for scientific research, such as literature search, protocol planning, and data analysis. As a step toward building such benchmarks, we introduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of over 2,400 multiple choice questions for evaluating AI systems on a range of practical biology research capabilities, including recall and reasoning over literature, interpretation of figures, access and navigation of databases, and comprehension and manipulation of DNA and protein sequences. Importantly, in contrast to previous scientific benchmarks, we expect that an AI system that can achieve consistently high scores on the more difficult LAB-Bench tasks would serve as a useful assistant for researchers in areas such as literature search and molecular cloning. As an initial assessment of the emergent scientific task capabilities of frontier language models, we measure performance of several against our benchmark and report results compared to human expert biology researchers. We will continue to update and expand LAB-Bench over time, and expect it to serve as a useful tool in the development of automated research systems going forward. A public subset of LAB-Bench is available for use at the following URL: https://huggingface.co/datasets/futurehouse/lab-bench
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2407.09274.pdf' target='_blank'>https://arxiv.org/pdf/2407.09274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Chen, Tianhao Chen, Chenggang Xie, Yang Xue, Xiaonan Zhang, Jingbo Zhou, Xiaomin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09274">Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein Generation with the Large Multimodal Model HelixProtX</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are fundamental components of biological systems and can be represented through various modalities, including sequences, structures, and textual descriptions. Despite the advances in deep learning and scientific large language models (LLMs) for protein research, current methodologies predominantly focus on limited specialized tasks -- often predicting one protein modality from another. These approaches restrict the understanding and generation of multimodal protein data. In contrast, large multimodal models have demonstrated potential capabilities in generating any-to-any content like text, images, and videos, thus enriching user interactions across various domains. Integrating these multimodal model technologies into protein research offers significant promise by potentially transforming how proteins are studied. To this end, we introduce HelixProtX, a system built upon the large multimodal model, aiming to offer a comprehensive solution to protein research by supporting any-to-any protein modality generation. Unlike existing methods, it allows for the transformation of any input protein modality into any desired protein modality. The experimental results affirm the advanced capabilities of HelixProtX, not only in generating functional descriptions from amino acid sequences but also in executing critical tasks such as designing protein sequences and structures from textual descriptions. Preliminary findings indicate that HelixProtX consistently achieves superior accuracy across a range of protein-related tasks, outperforming existing state-of-the-art models. By integrating multimodal large models into protein research, HelixProtX opens new avenues for understanding protein biology, thereby promising to accelerate scientific discovery.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2406.07249.pdf' target='_blank'>https://arxiv.org/pdf/2406.07249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaiza Serrano, Ãlvaro Ciudad, Alexis Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07249">Are Protein Language Models Compute Optimal?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored. By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget. Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field. Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently. Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass. This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2404.10260.pdf' target='_blank'>https://arxiv.org/pdf/2404.10260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaomin Fang, Jie Gao, Jing Hu, Lihang Liu, Yang Xue, Xiaonan Zhang, Kunrui Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10260">HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field. This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short. Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles. In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance. HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions. Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, greatly surpassing AlphaFold 3. HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version. Researchers can conveniently access and utilize this service for their development needs.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2404.06481.pdf' target='_blank'>https://arxiv.org/pdf/2404.06481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RaÃºl MiÃ±Ã¡n, Javier Gallardo, Ãlvaro Ciudad, Alexis Molina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06481">GeoDirDock: Guiding Docking Along Geodesic Paths</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions. GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom. Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions. We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism. Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions. Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2511.10244.pdf' target='_blank'>https://arxiv.org/pdf/2511.10244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Schilling, Akshat Dubey, Georges Hattab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10244">PepTriX: A Framework for Explainable Peptide Analysis through Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptide classification tasks, such as predicting toxicity and HIV inhibition, are fundamental to bioinformatics and drug discovery. Traditional approaches rely heavily on handcrafted encodings of one-dimensional (1D) peptide sequences, which can limit generalizability across tasks and datasets. Recently, protein language models (PLMs), such as ESM-2 and ESMFold, have demonstrated strong predictive performance. However, they face two critical challenges. First, fine-tuning is computationally costly. Second, their complex latent representations hinder interpretability for domain experts. Additionally, many frameworks have been developed for specific types of peptide classification, lacking generalization. These limitations restrict the ability to connect model predictions to biologically relevant motifs and structural properties. To address these limitations, we present PepTriX, a novel framework that integrates one dimensional (1D) sequence embeddings and three-dimensional (3D) structural features via a graph attention network enhanced with contrastive training and cross-modal co-attention. PepTriX automatically adapts to diverse datasets, producing task-specific peptide vectors while retaining biological plausibility. After evaluation by domain experts, we found that PepTriX performs remarkably well across multiple peptide classification tasks and provides interpretable insights into the structural and biophysical motifs that drive predictions. Thus, PepTriX offers both predictive robustness and interpretable validation, bridging the gap between performance-driven peptide-level models (PLMs) and domain-level understanding in peptide research.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2511.04040.pdf' target='_blank'>https://arxiv.org/pdf/2511.04040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoling Luo, Peng Chen, Chengliang Liu, Xiaopeng Jin, Jie Wen, Yumeng Liu, Junsong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04040">Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal protein features play a crucial role in protein function prediction. However, these features encompass a wide range of information, ranging from structural data and sequence features to protein attributes and interaction networks, making it challenging to decipher their complex interconnections. In this work, we propose a multimodal protein function prediction method (DSRPGO) by utilizing dynamic selection and reconstructive pre-training mechanisms. To acquire complex protein information, we introduce reconstructive pre-training to mine more fine-grained information with low semantic levels. Moreover, we put forward the Bidirectional Interaction Module (BInM) to facilitate interactive learning among multimodal features. Additionally, to address the difficulty of hierarchical multi-label classification in this task, a Dynamic Selection Module (DSM) is designed to select the feature representation that is most conducive to current protein function prediction. Our proposed DSRPGO model improves significantly in BPO, MFO, and CCO on human datasets, thereby outperforming other benchmark models.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2510.23273.pdf' target='_blank'>https://arxiv.org/pdf/2510.23273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23273">A Novel Framework for Multi-Modal Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate protein function prediction requires integrating heterogeneous intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts (e.g., protein-protein interactions and GO term annotations). However, two key challenges hinder effective fusion: (i) cross-modal distributional mismatch among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy relational graphs of extrinsic data that degrade GNN-based information aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding (DAMPE), a unified framework that addresses these through two core mechanisms. First, we propose Optimal Transport (OT)-based representation alignment that establishes correspondence between intrinsic embedding spaces of different modalities, effectively mitigating cross-modal heterogeneity. Second, we develop a Conditional Graph Generation (CGG)-based information fusion method, where a condition encoder fuses the aligned intrinsic embeddings to provide informative cues for graph reconstruction. Meanwhile, our theoretical analysis implies that the CGG objective drives this condition encoder to absorb graph-aware knowledge into its produced protein representations. Empirically, DAMPE outperforms or matches state-of-the-art methods such as DPFunc on standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains 0.004-0.007 pp. Ablation studies further show that OT-based alignment contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for robust multi-modal protein representation learning, substantially enhancing protein function prediction.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2510.12842.pdf' target='_blank'>https://arxiv.org/pdf/2510.12842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qiang, Chengyue Gong, Xinshi Chen, Yuxuan Zhang, Wenzhi Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12842">Protenix-Mini+: efficient structure prediction model with scalable pairformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lightweight inference is critical for biomolecular structure prediction and downstream tasks, enabling efficient real-world deployment and inference-time scaling for large-scale applications. While AF3 and its variants (e.g., Protenix, Chai-1) have advanced structure prediction results, they suffer from critical limitations: high inference latency and cubic time complexity with respect to token count, both of which restrict scalability for large biomolecular complexes. To address the core challenge of balancing model efficiency and prediction accuracy, we introduce three key innovations: (1) compressing non-scalable operations to mitigate cubic time complexity, (2) removing redundant blocks across modules to reduce unnecessary overhead, and (3) adopting a few-step sampler for the atom diffusion module to accelerate inference. Building on these design principles, we develop Protenix-Mini+, a highly lightweight and scalable variant of the Protenix model. Within an acceptable range of performance degradation, it substantially improves computational efficiency. For example, in the case of low-homology single-chain proteins, Protenix-Mini+ experiences an intra-protein LDDT drop of approximately 3% relative to the full Protenix model -- an acceptable performance trade-off given its substantially 90%+ improved computational efficiency.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2510.06413.pdf' target='_blank'>https://arxiv.org/pdf/2510.06413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Zhang, Yuxin Yang, Feixiong Chen, Cheng-Chang Lu, Nima Saeidi, Samuel L. Volchenboum, Junhan Zhao, Siwei Chen, Weiwen Jiang, Qiang Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06413">A Hybrid Quantum-AI Framework for Protein Structure Prediction on NISQ Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variational quantum algorithms provide a direct, physics-based approach to protein structure prediction, but their accuracy is limited by the coarse resolution of the energy landscapes generated on current noisy devices. We propose a hybrid framework that combines quantum computation with deep learning, formulating structure prediction as a problem of energy fusion. Candidate conformations are obtained through the Variational Quantum Eigensolver (VQE) executed on IBM's 127-qubit superconducting processor, which defines a global yet low-resolution quantum energy surface. To refine these basins, secondary structure probabilities and dihedral angle distributions predicted by the NSP3 neural network are incorporated as statistical potentials. These additional terms sharpen the valleys of the quantum landscape, resulting in a fused energy function that enhances effective resolution and better distinguishes native-like structures. Evaluation on 375 conformations from 75 protein fragments shows consistent improvements over AlphaFold3, ColabFold, and quantum-only predictions, achieving a mean RMSD of 4.9 Å with statistical significance (p < 0.001). The findings demonstrate that energy fusion offers a systematic method for combining data-driven models with quantum algorithms, improving the practical applicability of near-term quantum computing to molecular and structural biology.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2510.00774.pdf' target='_blank'>https://arxiv.org/pdf/2510.00774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eoin Quinn, Marco Carobene, Jean Quentin, Sebastien Boyer, Miguel Arbesú, Oliver Bent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00774">GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2509.25955.pdf' target='_blank'>https://arxiv.org/pdf/2509.25955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mason Minot, Gisbert Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25955">AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneously optimizing multiple, frequently conflicting, molecular properties is a key bottleneck in the development of novel therapeutics. Although a promising approach, the efficacy of multi-task learning is often compromised by destructive gradient interference, especially in the data-scarce regimes common to drug discovery. To address this, we propose AIM, an optimization framework that learns a dynamic policy to mediate gradient conflicts. The policy is trained jointly with the main network using a novel augmented objective composed of dense, differentiable regularizers. This objective guides the policy to produce updates that are geometrically stable and dynamically efficient, prioritizing progress on the most challenging tasks. We demonstrate that AIM achieves statistically significant improvements over multi-task baselines on subsets of the QM9 and targeted protein degraders benchmarks, with its advantage being most pronounced in data-scarce regimes. Beyond performance, AIM's key contribution is its interpretability; the learned policy matrix serves as a diagnostic tool for analyzing inter-task relationships. This combination of data-efficient performance and diagnostic insight highlights the potential of adaptive optimizers to accelerate scientific discovery by creating more robust and insightful models for multi-property molecular design.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2509.21689.pdf' target='_blank'>https://arxiv.org/pdf/2509.21689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Walton, Darin Tsui, Aryan Musharaf, Amirali Aghazadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21689">SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughput protein screening. Speculative decoding accelerates generation by employing a lightweight draft model to sample tokens, which a larger target model then verifies and refines. Yet, in protein sequence generation, draft models are typically agnostic to the structural and functional constraints of the target protein, leading to biologically implausible outputs and a shift in the likelihood distribution of generated sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a novel framework that incorporates biological, structural, and functional priors using k-mer motifs extracted from multiple sequence alignments. By scoring candidate sequences in parallel and selecting those most consistent with known biological patterns, SpecMER significantly improves sequence plausibility while retaining the efficiency of speculative decoding. SpecMER achieves 24-32% speedup over standard autoregressive decoding, along with higher acceptance rates and improved sequence likelihoods.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2509.13294.pdf' target='_blank'>https://arxiv.org/pdf/2509.13294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Allan dos Santos Costa, Manvitha Ponnapati, Dana Rubin, Tess Smidt, Joseph Jacobson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13294">Accelerating Protein Molecular Dynamics Simulation with DeepJump</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unraveling the dynamical motions of biomolecules is essential for bridging their structure and function, yet it remains a major computational challenge. Molecular dynamics (MD) simulation provides a detailed depiction of biomolecular motion, but its high-resolution temporal evolution comes at significant computational cost, limiting its applicability to timescales of biological relevance. Deep learning approaches have emerged as promising solutions to overcome these computational limitations by learning to predict long-timescale dynamics. However, generalizable kinetics models for proteins remain largely unexplored, and the fundamental limits of achievable acceleration while preserving dynamical accuracy are poorly understood. In this work, we fill this gap with DeepJump, an Euclidean-Equivariant Flow Matching-based model for predicting protein conformational dynamics across multiple temporal scales. We train DeepJump on trajectories of the diverse proteins of mdCATH, systematically studying our model's performance in generalizing to long-term dynamics of fast-folding proteins and characterizing the trade-off between computational acceleration and prediction accuracy. We demonstrate the application of DeepJump to ab initio folding, showcasing prediction of folding pathways and native states. Our results demonstrate that DeepJump achieves significant $\approx$1000$\times$ computational acceleration while effectively recovering long-timescale dynamics, providing a stepping stone for enabling routine simulation of proteins.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2509.13216.pdf' target='_blank'>https://arxiv.org/pdf/2509.13216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebecca Manuela Neeser, Ilia Igashov, Arne Schneuing, Michael Bronstein, Philippe Schwaller, Bruno Correia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13216">Flow-Based Fragment Identification via Binding Site-Specific Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fragment-based drug design is a promising strategy leveraging the binding of small chemical moieties that can efficiently guide drug discovery. The initial step of fragment identification remains challenging, as fragments often bind weakly and non-specifically. We developed a protein-fragment encoder that relies on a contrastive learning approach to map both molecular fragments and protein surfaces in a shared latent space. The encoder captures interaction-relevant features and allows to perform virtual screening as well as generative design with our new method LatentFrag. In LatentFrag, fragment embeddings and positions are generated conditioned on the protein surface while being chemically realistic by construction. Our expressive fragment and protein representations allow location of protein-fragment interaction sites with high sensitivity and we observe state-of-the-art fragment recovery rates when sampling from the learned distribution of latent fragment embeddings. Our generative method outperforms common methods such as virtual screening at a fraction of its computational cost providing a valuable starting point for fragment hit discovery. We further show the practical utility of LatentFrag and extend the workflow to full ligand design tasks. Together, these approaches contribute to advancing fragment identification and provide valuable tools for fragment-based drug discovery.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2508.18567.pdf' target='_blank'>https://arxiv.org/pdf/2508.18567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darin Tsui, Kunal Talreja, Amirali Aghazadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18567">Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein function from amino acid sequence remains a central challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided protein design when only small amounts of assay-labeled sequence-function data are available. Protein language models (pLMs) have advanced the field by providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have enabled decomposition of these embeddings into interpretable latent variables that capture structural and functional features. However, the effectiveness of SAEs for low-$N$ function prediction and protein design has not been systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2 embeddings across diverse fitness extrapolation and protein engineering tasks. We show that SAEs, with as few as 24 sequences, consistently outperform or compete with their ESM2 baselines in fitness prediction, indicating that their sparse latent space encodes compact and biologically meaningful representations that generalize more effectively from limited data. Moreover, steering predictive latents exploits biological motifs in pLM representations, yielding top-fitness variants in 83% of cases compared to designing with ESM2 alone.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2508.17815.pdf' target='_blank'>https://arxiv.org/pdf/2508.17815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arne Schneuing, Ilia Igashov, Adrian W. Dobbelstein, Thomas Castiglione, Michael Bronstein, Bruno Correia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17815">Multi-domain Distribution Learning for De Novo Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2508.02674.pdf' target='_blank'>https://arxiv.org/pdf/2508.02674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tamir Bendory, Dan Edidin, Josh Katz, Shay Kreymer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02674">Orbit recovery for spherical functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Orbit recovery is a central problem in both mathematics and applied sciences, with important applications to structural biology. This paper focuses on recovering generic orbits of functions on ${\mathbb R}^{n}$ and the sphere $S^{n-1}$ under the rotation action of $SO(n)$. Specifically, we demonstrate that invariants of degree three (called the bispectrum) suffice to recover generic orbits of functions in finite-dimensional approximations of $L^2({\mathbb R}^n)$ obtained by band-limiting the spherical component and discretizing the radial direction. In particular, our main result explicitly bounds the number of samples in the radial direction required for recovery from the degree three invariants. From an application perspective, the most important case is $SO(3)$, which arises in many scientific fields, and in particular, plays a central role in leading structural biology applications such as cryo-electron tomography and cryo-electron microscopy. Our result for $SO(3)$ states that considering three spherical shells (i.e., samples in the radial direction) is sufficient to recover generic orbits, which verifies an implicit conjecture made in a paper of Bandeira et al. Our proof technique provides an explicit, computationally efficient algorithm to recover the signal by successively solving systems of linear equations. We implemented this algorithm and demonstrated its effectiveness on two protein structures.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2508.00837.pdf' target='_blank'>https://arxiv.org/pdf/2508.00837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Zhang, Yuxin Yang, Cheng-Chang Lu, Weiwen Jiang, Feixiong Cheng, Bo Fang, Qiang Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00837">QDockBank: A Dataset for Ligand Docking on Protein Fragments Predicted on Utility-Level Quantum Computers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction is a core challenge in computational biology, particularly for fragments within ligand-binding regions, where accurate modeling is still difficult. Quantum computing offers a novel first-principles modeling paradigm, but its application is currently limited by hardware constraints, high computational cost, and the lack of a standardized benchmarking dataset. In this work, we present QDockBank-the first large-scale protein fragment structure dataset generated entirely using utility-level quantum computers, specifically designed for protein-ligand docking tasks. QDockBank comprises 55 protein fragments extracted from ligand-binding pockets. The dataset was generated through tens of hours of execution on superconducting quantum processors, making it the first quantum-based protein structure dataset with a total computational cost exceeding one million USD. Experimental evaluations demonstrate that structures predicted by QDockBank outperform those predicted by AlphaFold2 and AlphaFold3 in terms of both RMSD and docking affinity scores. QDockBank serves as a new benchmark for evaluating quantum-based protein structure prediction.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2507.19755.pdf' target='_blank'>https://arxiv.org/pdf/2507.19755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Zhang, Shiheng Chen, Runze Yang, Zhisheng Wei, Wei Zhang, Lei Wang, Zhanzhi Liu, Fengshan Zhang, Jing Wu, Xiaoyong Pan, Hongbin Shen, Longbing Cao, Zhaohong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19755">Modeling enzyme temperature stability from sequence segment perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing enzymes with desired thermal properties is crucial for a wide range of industrial and research applications, and determining temperature stability is an essential step in this process. Experimental determination of thermal parameters is labor-intensive, time-consuming, and costly. Moreover, existing computational approaches are often hindered by limited data availability and imbalanced distributions. To address these challenges, we introduce a curated temperature stability dataset designed for model development and benchmarking in enzyme thermal modeling. Leveraging this dataset, we present the \textit{Segment Transformer}, a novel deep learning framework that enables efficient and accurate prediction of enzyme temperature stability. The model achieves state-of-the-art performance with an RMSE of 24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33, respectively. These results highlight the effectiveness of incorporating segment-level representations, grounded in the biological observation that different regions of a protein sequence contribute unequally to thermal behavior. As a proof of concept, we applied the Segment Transformer to guide the engineering of a cutinase enzyme. Experimental validation demonstrated a 1.64-fold improvement in relative activity following heat treatment, achieved through only 17 mutations and without compromising catalytic function.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2507.15226.pdf' target='_blank'>https://arxiv.org/pdf/2507.15226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changguo Jia, Yi Zhan, Tianqi Zhao, Hengzhi Ye, Minghui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15226">Code Clone Detection via an AlphaFold-Inspired Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code clone detection, which aims to identify functionally equivalent code fragments, plays a critical role in software maintenance and vulnerability analysis. Substantial methods have been proposed to detect code clones, but they fall short in capturing code semantics or relying on language-specific analyzers. Inspired by the remarkable success of AlphaFold in predicting three-dimensional protein structures from protein sequences, in this paper, we leverage AlphaFold for code clone detection based on the insight that protein sequences and token sequences share a common linear sequential structure. In particular, we propose AlphaCC, which represents code fragments as token sequences to ensure multi-language applicability and adapts AlphaFold's sequence-to-structure modeling capability to infer code semantics. The pipeline of AlphaCC goes through three steps. First, AlphaCC transforms each input code fragment into a token sequence and, motivated by AlphaFold's use of multiple sequence alignment (MSA) to enhance contextual understanding, constructs an MSA from lexically similar token sequences. Second, AlphaCC adopts a modified attention-based encoder based on AlphaFold to model dependencies within and across token sequences. Finally, unlike AlphaFold's protein structure prediction task, AlphaCC computes similarity scores between token sequences through a late interaction strategy and performs binary classification to determine code clone pairs. Comprehensive evaluations on three language-diverse datasets demonstrate AlphaCC's applicability across multiple programming languages. On two semantic clone detection datasets, it consistently outperforms all baselines, showing strong semantic understanding. Moreover, AlphaCC maintains competitive efficiency, enabling practical usage in large-scale clone detection tasks.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2507.03174.pdf' target='_blank'>https://arxiv.org/pdf/2507.03174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunrui Qiu, Richard John, Lukas Herron, Pratyush Tiwary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03174">Latent Thermodynamic Flows: Unified Representation Learning and Generative Modeling of Temperature-Dependent Behaviors from Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate characterization of the equilibrium distributions of complex molecular systems and their dependence on environmental factors such as temperature is essential for understanding thermodynamic properties and transition mechanisms. Projecting these distributions onto meaningful low-dimensional representations enables interpretability and downstream analysis. Recent advances in generative AI, particularly flow models such as Normalizing Flows (NFs), have shown promise in modeling such distributions, but their scope is limited without tailored representation learning. In this work, we introduce Latent Thermodynamic Flows (LaTF), an end-to-end framework that tightly integrates representation learning and generative modeling. LaTF unifies the State Predictive Information Bottleneck (SPIB) with NFs to simultaneously learn low-dimensional latent representations, referred to as Collective Variables (CVs), classify metastable states, and generate equilibrium distributions across temperatures beyond the training data. The two components of representation learning and generative modeling are optimized jointly, ensuring that the learned latent features capture the system's slow, important degrees of freedom while the generative model accurately reproduces the system's equilibrium behavior. We demonstrate LaTF's effectiveness across diverse systems, including a model potential, the Chignolin protein, and cluster of Lennard Jones particles, with thorough evaluations and benchmarking using multiple metrics and extensive simulations. Finally, we apply LaTF to a RNA tetraloop system, where despite using simulation data from only two temperatures, LaTF reconstructs the temperature-dependent structural ensemble and melting behavior, consistent with experimental and prior extensive computational results.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2506.22677.pdf' target='_blank'>https://arxiv.org/pdf/2506.22677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Zhang, Yuxin Yang, William Martin, Kingsten Lin, Zixu Wang, Cheng-Chang Lu, Weiwen Jiang, Ruth Nussinov, Joseph Loscalzo, Qiang Guan, Feixiong Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22677">Prediction of Protein Three-dimensional Structures via a Hardware-Executable Quantum Computing Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein active site structures remains a central challenge in structural biology, particularly for short and flexible peptide fragments where conventional methods often fail. Here, we present a quantum computing framework specifically developed for utility-level quantum processors to address this problem. Starting from an amino acid sequence, we formulate the structure prediction task as a ground-state energy minimization problem using the Variational Quantum Eigensolver (VQE). Amino acid connectivity is encoded on a tetrahedral lattice model, and structural constraints-including steric, geometric, and chirality terms-are mapped into a problem-specific Hamiltonian expressed as sparse Pauli operators. The optimization is executed via a two-stage architecture separating energy estimation and measurement decoding, allowing noise mitigation under realistic quantum device conditions. We evaluate the framework on 23 randomly selected real protein fragments from the PDBbind dataset, as well as 7 real fragments from proteins with therapeutic potential, and run the experiments on the IBM-Cleveland Clinic quantum processor. Structural predictions are benchmarked against AlphaFold3 (AF3) using identical postprocessing and docking procedures. Our quantum method outperformed AF3 in both RMSD (Root-Mean-Square Deviation) and docking efficacy. This work demonstrates, for the first time, a complete end-to-end pipeline for biologically relevant structure prediction on real quantum hardware, highlighting its engineering feasibility and practical advantage over existing classical and deep learning approaches.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2506.10908.pdf' target='_blank'>https://arxiv.org/pdf/2506.10908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emmanuel J. CandÃ¨s, Andrew Ilyas, Tijana Zrnic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10908">Probably Approximately Correct Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obtaining high-quality labeled datasets is often costly, requiring either extensive human annotation or expensive experiments. We propose a method that supplements such "expert" labels with AI predictions from pre-trained models to construct labeled datasets more cost-effectively. Our approach results in probably approximately correct labels: with high probability, the overall labeling error is small. This solution enables rigorous yet efficient dataset curation using modern AI models. We demonstrate the benefits of the methodology through text annotation with large language models, image labeling with pre-trained vision models, and protein folding analysis with AlphaFold.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2506.04490.pdf' target='_blank'>https://arxiv.org/pdf/2506.04490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishwanth Raghu, Axel Levy, Gordon Wetzstein, Ellen D. Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04490">Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure prediction models are now capable of generating accurate 3D structural hypotheses from sequence alone. However, they routinely fail to capture the conformational diversity of dynamic biomolecular complexes, often requiring heuristic MSA subsampling approaches for generating alternative states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a powerful tool for imaging near-native structural heterogeneity, but is challenged by arduous pipelines to go from raw experimental data to atomic models. Here, we bridge the gap between these modalities, combining cryo-EM density maps with the rich sequence and biophysical priors learned by protein structure prediction models. Our method, CryoBoltz, guides the sampling trajectory of a pretrained protein structure prediction model using both global and local structural constraints derived from density maps, driving predictions towards conformational states consistent with the experimental data. We demonstrate that this flexible yet powerful inference-time approach allows us to build atomic models into heterogeneous cryo-EM maps across a variety of dynamic biomolecular systems including transporters and antibodies.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2505.12511.pdf' target='_blank'>https://arxiv.org/pdf/2505.12511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanting Li, Jiyue Jiang, Zikang Wang, Ziqian Lin, Dongchen He, Yuheng Shan, Yanruisheng Shao, Jiayi Li, Xiangyu Shi, Jiuming Wang, Yanyu Chen, Yimin Fan, Han Li, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12511">DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse Protein Folding (IPF) is a critical subtask in the field of protein design, aiming to engineer amino acid sequences capable of folding correctly into a specified three-dimensional (3D) conformation. Although substantial progress has been achieved in recent years, existing methods generally rely on either backbone coordinates or molecular surface features alone, which restricts their ability to fully capture the complex chemical and geometric constraints necessary for precise sequence prediction. To address this limitation, we present DS-ProGen, a dual-structure deep language model for functional protein design, which integrates both backbone geometry and surface-level representations. By incorporating backbone coordinates as well as surface chemical and geometric descriptors into a next-amino-acid prediction paradigm, DS-ProGen is able to generate functionally relevant and structurally stable sequences while satisfying both global and local conformational constraints. On the PRIDE dataset, DS-ProGen attains the current state-of-the-art recovery rate of 61.47%, demonstrating the synergistic advantage of multi-modal structural encoding in protein design. Furthermore, DS-ProGen excels in predicting interactions with a variety of biological partners, including ligands, ions, and RNA, confirming its robust functional retention capabilities.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2505.05874.pdf' target='_blank'>https://arxiv.org/pdf/2505.05874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anjie Qiao, Hao Zhang, Qianmu Yuan, Qirui Deng, Jingtian Su, Weifeng Huang, Huihao Zhou, Guo-Bo Li, Zhen Wang, Jinping Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05874">A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating molecules that bind to specific protein targets via diffusion models has shown good promise for structure-based drug design and molecule optimization. Especially, the diffusion models with binding interaction guidance enables molecule generation with high affinity through forming favorable interaction within protein pocket. However, the generated molecules may not form interactions with the highly conserved residues, which are important for protein functions and bioactivities of the ligands. Herein, we developed a new 3D target-aware diffusion model DiffDecip, which explicitly incorporates the protein-ligand binding interactions and evolutionary conservation information of protein residues into both diffusion and sampling process, for molecule optimization through scaffold decoration. The model performance revealed that DiffDecip outperforms baseline model DiffDec on molecule optimization towards higher affinity through forming more non-covalent interactions with highly conserved residues in the protein pocket.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2505.04967.pdf' target='_blank'>https://arxiv.org/pdf/2505.04967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Ni, Ziqi Deng, Lin Mu, Lei Zhang, Wenjian Luo, Yiwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04967">Community and hyperedge inference in multiple hypergraphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hypergraphs, capable of representing high-order interactions via hyperedges, have become a powerful tool for modeling real-world biological and social systems. Inherent relationships within these real-world systems, such as the encoding relationship between genes and their protein products, drive the establishment of interconnections between multiple hypergraphs. Here, we demonstrate how to utilize those interconnections between multiple hypergraphs to synthesize integrated information from multiple higher-order systems, thereby enhancing understanding of underlying structures. We propose a model based on the stochastic block model, which integrates information from multiple hypergraphs to reveal latent high-order structures. Real-world hyperedges exhibit preferential attachment, where certain nodes dominate hyperedge formation. To characterize this phenomenon, our model introduces hyperedge internal degree to quantify nodes' contributions to hyperedge formation. This model is capable of mining communities, predicting missing hyperedges of arbitrary sizes within hypergraphs, and inferring inter-hypergraph edges between hypergraphs. We apply our model to high-order datasets to evaluate its performance. Experimental results demonstrate strong performance of our model in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Moreover, we show that our model enables analysis of multiple hypergraphs of different types and supports the analysis of a single hypergraph in the absence of inter-hypergraph edges. Our work provides a practical and flexible tool for analyzing multiple hypergraphs, greatly advancing the understanding of the organization in real-world high-order systems.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2504.21065.pdf' target='_blank'>https://arxiv.org/pdf/2504.21065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anjie Qiao, Junjie Xie, Weifeng Huang, Hao Zhang, Jiahua Rao, Shuangjia Zheng, Yuedong Yang, Zhen Wang, Guo-Bo Li, Jinping Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21065">A 3D pocket-aware and affinity-guided diffusion model for lead optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular optimization, aimed at improving binding affinity or other molecular properties, is a crucial task in drug discovery that often relies on the expertise of medicinal chemists. Recently, deep learning-based 3D generative models showed promise in enhancing the efficiency of molecular optimization. However, these models often struggle to adequately consider binding affinities with protein targets during lead optimization. Herein, we propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop, to optimize molecules with enhanced binding affinity. The model explicitly incorporates the knowledge of protein-ligand binding affinity to guide the denoising sampling for molecule generation with high affinity. The comprehensive evaluations indicated that Diffleop outperforms baseline models across multiple metrics, especially in terms of binding affinity.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2504.06806.pdf' target='_blank'>https://arxiv.org/pdf/2504.06806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Rossi, Guido Barducci, Tiziana Sanavia, Paola Turina, Emidio Capriotti, Piero Fariselli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06806">Mass Balance Approximation of Unfolding Improves Potential-Like Methods for Protein Stability Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of protein stability changes following single-point mutations plays a pivotal role in computational biology, particularly in areas like drug discovery, enzyme reengineering, and genetic disease analysis. Although deep-learning strategies have pushed the field forward, their use in standard workflows remains limited due to resource demands. Conversely, potential-like methods are fast, intuitive, and efficient. Yet, these typically estimate Gibbs free energy shifts without considering the free-energy variations in the unfolded protein state, an omission that may breach mass balance and diminish accuracy. This study shows that incorporating a mass-balance correction (MBC) to account for the unfolded state significantly enhances these methods. While many machine learning models partially model this balance, our analysis suggests that a refined representation of the unfolded state may improve the predictive performance.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2504.01973.pdf' target='_blank'>https://arxiv.org/pdf/2504.01973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Brunken, Sebastien Boyer, Mustafa Omar, Martin Maarand, Olivier Peltre, Solal Attias, Bakary N'tji Diallo, Anastasia Markina, Olaf Othersen, Oliver Bent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01973">Universally applicable and tunable graph-based coarse-graining for Machine learning force fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coarse-grained (CG) force field methods for molecular systems are a crucial tool to simulate large biological macromolecules and are therefore essential for characterisations of biomolecular systems. While state-of-the-art deep learning (DL)-based models for all-atom force fields have improved immensely over recent years, we observe and analyse significant limitations of the currently available approaches for DL-based CG simulations. In this work, we present the first transferable DL-based CG force field approach (i.e., not specific to only one narrowly defined system type) applicable to a wide range of biosystems. To achieve this, our CG algorithm does not rely on hard-coded rules and is tuned to output coarse-grained systems optimised for minimal statistical noise in the ground truth CG forces, which results in significant improvement of model training. Our force field model is also the first CG variant that is based on the MACE architecture and is trained on a custom dataset created by a new approach based on the fragmentation of large biosystems covering protein, RNA and lipid chemistry. We demonstrate that our model can be applied in molecular dynamics simulations to obtain stable and qualitatively accurate trajectories for a variety of systems, while also discussing cases for which we observe limited reliability.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2503.08295.pdf' target='_blank'>https://arxiv.org/pdf/2503.08295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umberto Borso, Davide Paglieri, Jude Wells, Tim RocktÃ¤schel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08295">Preference-Based Alignment of Discrete Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved state-of-the-art performance across multiple domains, with recent advancements extending their applicability to discrete data. However, aligning discrete diffusion models with task-specific preferences remains challenging, particularly in scenarios where explicit reward functions are unavailable. In this work, we introduce Discrete Diffusion DPO (D2-DPO), the first adaptation of Direct Preference Optimization (DPO) to discrete diffusion models formulated as continuous-time Markov chains. Our approach derives a novel loss function that directly fine-tunes the generative process using preference data while preserving fidelity to a reference distribution. We validate D2-DPO on a structured binary sequence generation task, demonstrating that the method effectively aligns model outputs with preferences while maintaining structural validity. Our results highlight that D2-DPO enables controlled fine-tuning without requiring explicit reward models, making it a practical alternative to reinforcement learning-based approaches. Future research will explore extending D2-DPO to more complex generative tasks, including language modeling and protein sequence generation, as well as investigating alternative noise schedules, such as uniform noising, to enhance flexibility across different applications.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2503.04490.pdf' target='_blank'>https://arxiv.org/pdf/2503.04490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wang, Zikang Wang, Jiyue Jiang, Pengan Chen, Xiangyu Shi, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04490">Large Language Models in Bioinformatics: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2503.04135.pdf' target='_blank'>https://arxiv.org/pdf/2503.04135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyue Jiang, Zikang Wang, Yuheng Shan, Heyan Chai, Jiayi Li, Zixian Ma, Xinrui Zhang, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04135">Biological Sequence with Language Model Prompting: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language models (LLMs) have emerged as powerful tools for addressing challenges across diverse domains. Notably, recent studies have demonstrated that large language models significantly enhance the efficiency of biomolecular analysis and synthesis, attracting widespread attention from academics and medicine. In this paper, we systematically investigate the application of prompt-based methods with LLMs to biological sequences, including DNA, RNA, proteins, and drug discovery tasks. Specifically, we focus on how prompt engineering enables LLMs to tackle domain-specific problems, such as promoter sequence prediction, protein structure modeling, and drug-target binding affinity prediction, often with limited labeled data. Furthermore, our discussion highlights the transformative potential of prompting in bioinformatics while addressing key challenges such as data scarcity, multimodal fusion, and computational resource limitations. Our aim is for this paper to function both as a foundational primer for newcomers and a catalyst for continued innovation within this dynamic field of study.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2503.00648.pdf' target='_blank'>https://arxiv.org/pdf/2503.00648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gian Marco Visani, Michael N. Pun, Anastasia A. Minervina, Philip Bradley, Paul Thomas, Armita Nourmohammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00648">T-cell receptor specificity landscape revealed through de novo peptide design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>T-cells play a key role in adaptive immunity by mounting specific responses against diverse pathogens. An effective binding between T-cell receptors (TCRs) and pathogen-derived peptides presented on Major Histocompatibility Complexes (MHCs) mediate an immune response. However, predicting these interactions remains challenging due to limited functional data on T-cell reactivities. Here, we introduce a computational approach to predict TCR interactions with peptides presented on MHC class I alleles, and to design novel immunogenic peptides for specified TCR-MHC complexes. Our method leverages HERMES, a structure-based, physics-guided machine learning model trained on the protein universe to predict amino acid preferences based on local structural environments. Despite no direct training on TCR-pMHC data, the implicit physical reasoning in HERMES enables us to make accurate predictions of both TCR-pMHC binding affinities and T-cell activities across diverse viral epitopes and cancer neoantigens, achieving up to 0.72 correlation with experimental data. Leveraging our TCR recognition model, we develop a computational protocol for de novo design of immunogenic peptides. Through experimental validation in three TCR-MHC systems targeting viral and cancer peptides, we demonstrate that our designs -- with up to five substitutions from the native sequence -- activate T-cells at success rates of up to 50%. Lastly, we use our generative framework to quantify the diversity of the peptide recognition landscape for various TCR-MHC complexes, offering key insights into T-cell specificity in both humans and mice. Our approach provides a platform for immunogenic peptide and neoantigen design, as well as for evaluating TCR specificity, offering a computational framework to inform design of engineered T-cell therapies and vaccines.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2502.19173.pdf' target='_blank'>https://arxiv.org/pdf/2502.19173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gregory W. Kyro, Tianyin Qiu, Victor S. Batista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19173">A Model-Centric Review of Deep Learning for Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has transformed protein design, enabling accurate structure prediction, sequence optimization, and de novo protein generation. Advances in single-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold, and others have achieved near-experimental accuracy, inspiring successive work extended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold All-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as ProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone design beyond natural evolution-based limitations. More recently, joint sequence-structure co-design models, including ESM3, have integrated both modalities into a unified framework, resulting in improved designability. Despite these advances, challenges still exist pertaining to modeling sequence-structure-function relationships and ensuring robust generalization beyond the regions of protein space spanned by the training data. Future advances will likely focus on joint sequence-structure-function co-design frameworks that are able to model the fitness landscape more effectively than models that treat these modalities independently. Current capabilities, coupled with the dizzying rate of progress, suggest that the field will soon enable rapid, rational design of proteins with tailored structures and functions that transcend the limitations imposed by natural evolution. In this review, we discuss the current capabilities of deep learning methods for protein design, focusing on some of the most revolutionary and capable models with respect to their functionality and the applications that they enable, leading up to the current challenges of the field and the optimal path forward.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2502.05230.pdf' target='_blank'>https://arxiv.org/pdf/2502.05230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Etienne Goffinet, Sen Yan, Fabrizio Gabellieri, Laurence Jennings, Lydia Gkoura, Filippo Castiglione, Ryan Young, Idir Malki, Ankita Singh, Thomas Launey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05230">DiffNMR2: NMR Guided Sampling Acquisition Through Diffusion Model Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nuclear Magnetic Resonance (NMR) spectrometry uses electro-frequency pulses to probe the resonance of a compound's nucleus, which is then analyzed to determine its structure. The acquisition time of high-resolution NMR spectra remains a significant bottleneck, especially for complex biological samples such as proteins. In this study, we propose a novel and efficient sub-sampling strategy based on a diffusion model trained on protein NMR data. Our method iteratively reconstructs under-sampled spectra while using model uncertainty to guide subsequent sampling, significantly reducing acquisition time. Compared to state-of-the-art strategies, our approach improves reconstruction accuracy by 52.9\%, reduces hallucinated peaks by 55.6%, and requires 60% less time in complex NMR experiments. This advancement holds promise for many applications, from drug discovery to materials science, where rapid and high-resolution spectral analysis is critical.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2501.12365.pdf' target='_blank'>https://arxiv.org/pdf/2501.12365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darin Tsui, Kunal Talreja, Amirali Aghazadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12365">Efficient Algorithm for Sparse Fourier Transform of Generalized $q$-ary Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computing the Fourier transform of a $q$-ary function $f:\mathbb{Z}_{q}^n\rightarrow \mathbb{R}$, which maps $q$-ary sequences to real numbers, is an important problem in mathematics with wide-ranging applications in biology, signal processing, and machine learning. Previous studies have shown that, under the sparsity assumption, the Fourier transform can be computed efficiently using fast and sample-efficient algorithms. However, in most practical settings, the function is defined over a more general space -- the space of generalized $q$-ary sequences $\mathbb{Z}_{q_1} \times \mathbb{Z}_{q_2} \times \cdots \times \mathbb{Z}_{q_n}$ -- where each $\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. Herein, we develop GFast, a coding theoretic algorithm that computes the $S$-sparse Fourier transform of $f$ with a sample complexity of $O(Sn)$, computational complexity of $O(Sn \log N)$, and a failure probability that approaches zero as $N=\prod_{i=1}^n q_i \rightarrow \infty$ with $S = N^Î´$ for some $0 \leq Î´< 1$. We show that a noise-robust version of GFast computes the transform with a sample complexity of $O(Sn^2)$ and computational complexity of $O(Sn^2 \log N)$ under the same high probability guarantees. Additionally, we demonstrate that GFast computes the sparse Fourier transform of generalized $q$-ary functions $8\times$ faster using $16\times$ fewer samples on synthetic experiments, and enables explaining real-world heart disease diagnosis and protein fitness models using up to $13\times$ fewer samples compared to existing Fourier algorithms applied to the most efficient parameterization of the models as $q$-ary functions.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2501.09274.pdf' target='_blank'>https://arxiv.org/pdf/2501.09274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09274">Large Language Model is Secretly a Protein Sequence Optimizer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the protein sequence engineering problem, which aims to find protein sequences with high fitness levels, starting from a given wild-type sequence. Directed evolution has been a dominating paradigm in this field which has an iterative process to generate variants and select via experimental feedback. We demonstrate large language models (LLMs), despite being trained on massive texts, are secretly protein sequence optimizers. With a directed evolutionary method, LLM can perform protein engineering through Pareto and experiment-budget constrained optimization, demonstrating success on both synthetic and experimental fitness landscapes.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2412.09420.pdf' target='_blank'>https://arxiv.org/pdf/2412.09420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Levy, Rishwanth Raghu, David Shustin, Adele Rui-Yang Peng, Huan Li, Oliver Biggs Clarke, Gordon Wetzstein, Ellen D. Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09420">Mixture of neural fields for heterogeneous reconstruction in cryo-EM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryo-electron microscopy (cryo-EM) is an experimental technique for protein structure determination that images an ensemble of macromolecules in near-physiological contexts. While recent advances enable the reconstruction of dynamic conformations of a single biomolecular complex, current methods do not adequately model samples with mixed conformational and compositional heterogeneity. In particular, datasets containing mixtures of multiple proteins require the joint inference of structure, pose, compositional class, and conformational states for 3D reconstruction. Here, we present Hydra, an approach that models both conformational and compositional heterogeneity fully ab initio by parameterizing structures as arising from one of K neural fields. We employ a new likelihood-based loss function and demonstrate the effectiveness of our approach on synthetic datasets composed of mixtures of proteins with large degrees of conformational variability. We additionally demonstrate Hydra on an experimental dataset of a cellular lysate containing a mixture of different protein complexes. Hydra expands the expressivity of heterogeneous reconstruction methods and thus broadens the scope of cryo-EM to increasingly complex samples.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2410.21683.pdf' target='_blank'>https://arxiv.org/pdf/2410.21683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Pengmei, Zhengyuan Shen, Zichen Wang, Marcus Collins, Huzefa Rangwala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21683">Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling and Zero-Shot Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing transferable descriptors for conformation representation of molecular and biological systems finds numerous applications in drug discovery, learning-based molecular dynamics, and protein mechanism analysis. Geometric graph neural networks (Geom-GNNs) with all-atom information have transformed atomistic simulations by serving as a general learnable geometric descriptors for downstream tasks including prediction of interatomic potential and molecular properties. However, common practices involve supervising Geom-GNNs on specific downstream tasks, which suffer from the lack of high-quality data and inaccurate labels leading to poor generalization and performance degradation on out-of-distribution (OOD) scenarios. In this work, we explored the possibility of using pre-trained Geom-GNNs as transferable and highly effective geometric descriptors for improved generalization. To explore their representation power, we studied the scaling behaviors of Geom-GNNs under self-supervised pre-training, supervised and unsupervised learning setups. We find that the expressive power of different architectures can differ on the pre-training task. Interestingly, Geom-GNNs do not follow the power-law scaling on the pre-training task, and universally lack predictable scaling behavior on the supervised tasks with quantum chemical labels important for screening and design of novel molecules. More importantly, we demonstrate how all-atom graph embedding can be organically combined with other neural architectures to enhance the expressive power. Meanwhile, the low-dimensional projection of the latent space shows excellent agreement with conventional geometrical descriptors.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2410.19236.pdf' target='_blank'>https://arxiv.org/pdf/2410.19236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darin Tsui, Aryan Musharaf, Yigit Efe Erginbas, Justin Singh Kang, Amirali Aghazadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19236">SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2410.09667.pdf' target='_blank'>https://arxiv.org/pdf/2410.09667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Allan dos Santos Costa, Ilan Mitnikov, Franco Pellegrini, Ameya Daigavane, Mario Geiger, Zhonglin Cao, Karsten Kreis, Tess Smidt, Emine Kucukbenli, Joseph Jacobson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09667">EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2409.18201.pdf' target='_blank'>https://arxiv.org/pdf/2409.18201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Borisiak, Gian Marco Visani, Armita Nourmohammad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18201">Loop-Diffusion: an equivariant diffusion model for designing and scoring protein loops</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein functional characteristics from structure remains a central problem in protein science, with broad implications from understanding the mechanisms of disease to designing novel therapeutics. Unfortunately, current machine learning methods are limited by scarce and biased experimental data, and physics-based methods are either too slow to be useful, or too simplified to be accurate. In this work, we present Loop-Diffusion, an energy based diffusion model which leverages a dataset of general protein loops from the entire protein universe to learn an energy function that generalizes to functional prediction tasks. We evaluate Loop-Diffusion's performance on scoring TCR-pMHC interfaces and demonstrate state-of-the-art results in recognizing binding-enhancing mutations.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2409.04737.pdf' target='_blank'>https://arxiv.org/pdf/2409.04737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrimon Mukherjee, Madhusudan Ghosh, Partha Basuchowdhuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04737">CrysAtom: Distributed Representation of Atoms for Crystal Property Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Application of artificial intelligence (AI) has been ubiquitous in the growth of research in the areas of basic sciences. Frequent use of machine learning (ML) and deep learning (DL) based methodologies by researchers has resulted in significant advancements in the last decade. These techniques led to notable performance enhancements in different tasks such as protein structure prediction, drug-target binding affinity prediction, and molecular property prediction. In material science literature, it is well-known that crystalline materials exhibit topological structures. Such topological structures may be represented as graphs and utilization of graph neural network (GNN) based approaches could help encoding them into an augmented representation space. Primarily, such frameworks adopt supervised learning techniques targeted towards downstream property prediction tasks on the basis of electronic properties (formation energy, bandgap, total energy, etc.) and crystalline structures. Generally, such type of frameworks rely highly on the handcrafted atom feature representations along with the structural representations. In this paper, we propose an unsupervised framework namely, CrysAtom, using untagged crystal data to generate dense vector representation of atoms, which can be utilized in existing GNN-based property predictor models to accurately predict important properties of crystals. Empirical results show that our dense representation embeds chemical properties of atoms and enhance the performance of the baseline property predictor models significantly.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2409.03118.pdf' target='_blank'>https://arxiv.org/pdf/2409.03118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratyush Tiwary, Lukas Herron, Richard John, Suemin Lee, Disha Sanwal, Ruiyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03118">Generative artificial intelligence for computational chemistry: a roadmap to predicting emergent phenomena</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent surge in Generative Artificial Intelligence (AI) has introduced exciting possibilities for computational chemistry. Generative AI methods have made significant progress in sampling molecular structures across chemical species, developing force fields, and speeding up simulations. This Perspective offers a structured overview, beginning with the fundamental theoretical concepts in both Generative AI and computational chemistry. It then covers widely used Generative AI methods, including autoencoders, generative adversarial networks, reinforcement learning, flow models and language models, and highlights their selected applications in diverse areas including force field development, and protein/RNA structure prediction. A key focus is on the challenges these methods face before they become truly predictive, particularly in predicting emergent chemical phenomena. We believe that the ultimate goal of a simulation method or theory is to predict phenomena not seen before, and that Generative AI should be subject to these same standards before it is deemed useful for chemistry. We suggest that to overcome these challenges, future AI models need to integrate core chemical principles, especially from statistical mechanics.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2406.09003.pdf' target='_blank'>https://arxiv.org/pdf/2406.09003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lincan Cai, Shuang Li, Wenxuan Ma, Jingxuan Kang, Binhui Xie, Zixun Sun, Chengwei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09003">Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pretrained models have proven immensely valuable in handling data-intensive modalities like text and image. However, fine-tuning these models for certain specialized modalities, such as protein sequence and cosmic ray, poses challenges due to the significant modality discrepancy and scarcity of labeled data. In this paper, we propose an end-to-end method, PaRe, to enhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained model to various target modalities. PaRe employs a gating mechanism to select key patches from both source and target data. Through a modality-agnostic Patch Replacement scheme, these patches are preserved and combined to construct data-rich intermediate modalities ranging from easy to hard. By gradually intermediate modality generation, we can not only effectively bridge the modality gap to enhance stability and transferability of cross-modal fine-tuning, but also address the challenge of limited data in the target modality by leveraging enriched intermediate modality data. Compared with hand-designed, general-purpose, task-specific, and state-of-the-art cross-modal fine-tuning approaches, PaRe demonstrates superior performance across three challenging benchmarks, encompassing more than ten modalities.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2405.08699.pdf' target='_blank'>https://arxiv.org/pdf/2405.08699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Li, Wei Zhang, Qinghao Zhang, Xuegong Zhang, Xiaowo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08699">Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2405.06836.pdf' target='_blank'>https://arxiv.org/pdf/2405.06836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salma J. Ahmed, Emad A. Mohammed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06836">Improving Targeted Molecule Generation through Language Model Fine-Tuning Via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing new drugs is laborious and costly, demanding extensive time investment. In this paper, we introduce a de-novo drug design strategy, which harnesses the capabilities of language models to devise targeted drugs for specific proteins. Employing a Reinforcement Learning (RL) framework utilizing Proximal Policy Optimization (PPO), we refine the model to acquire a policy for generating drugs tailored to protein targets. The proposed method integrates a composite reward function, combining considerations of drug-target interaction and molecular validity. Following RL fine-tuning, the proposed method demonstrates promising outcomes, yielding notable improvements in molecular validity, interaction efficacy, and critical chemical properties, achieving 65.37 for Quantitative Estimation of Drug-likeness (QED), 321.55 for Molecular Weight (MW), and 4.47 for Octanol-Water Partition Coefficient (logP), respectively. Furthermore, out of the generated drugs, only 0.041% do not exhibit novelty.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2405.05349.pdf' target='_blank'>https://arxiv.org/pdf/2405.05349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yassine Chemingui, Aryan Deshwal, Trong Nghia Hoang, Janardhan Rao Doppa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05349">Offline Model-Based Optimization via Policy-Guided Gradient Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline optimization is an emerging problem in many experimental engineering domains including protein, drug or aircraft design, where online experimentation to collect evaluation data is too expensive or dangerous. To avoid that, one has to optimize an unknown function given only its offline evaluation at a fixed set of inputs. A naive solution to this problem is to learn a surrogate model of the unknown function and optimize this surrogate instead. However, such a naive optimizer is prone to erroneous overestimation of the surrogate (possibly due to over-fitting on a biased sample of function evaluation) on inputs outside the offline dataset. Prior approaches addressing this challenge have primarily focused on learning robust surrogate models. However, their search strategies are derived from the surrogate model rather than the actual offline data. To fill this important gap, we introduce a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem. Our proposed policy-guided gradient search approach explicitly learns the best policy for a given surrogate model created from the offline data. Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2405.02374.pdf' target='_blank'>https://arxiv.org/pdf/2405.02374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arturo Fiorellini-Bernardis, Sebastien Boyer, Christoph Brunken, Bakary Diallo, Karim Beguir, Nicolas Lopez-Carranza, Oliver Bent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02374">Protein binding affinity prediction under multiple substitutions applying eGNNs on Residue and Atomic graphs combined with Language model information: eGRAL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) play a crucial role in numerous biological processes. Developing methods that predict binding affinity changes under substitution mutations is fundamental for modelling and re-engineering biological systems. Deep learning is increasingly recognized as a powerful tool capable of bridging the gap between in-silico predictions and in-vitro observations. With this contribution, we propose eGRAL, a novel SE(3) equivariant graph neural network (eGNN) architecture designed for predicting binding affinity changes from multiple amino acid substitutions in protein complexes. eGRAL leverages residue, atomic and evolutionary scales, thanks to features extracted from protein large language models. To address the limited availability of large-scale affinity assays with structural information, we generate a simulated dataset comprising approximately 500,000 data points. Our model is pre-trained on this dataset, then fine-tuned and tested on experimental data.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2404.04810.pdf' target='_blank'>https://arxiv.org/pdf/2404.04810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Song, Rongzhi Dong, Lai Wei, Qin Li, Jianjun Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04810">AlphaCrystal-II: Distance matrix based crystal structure prediction using deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational prediction of stable crystal structures has a profound impact on the large-scale discovery of novel functional materials. However, predicting the crystal structure solely from a material's composition or formula is a promising yet challenging task, as traditional ab initio crystal structure prediction (CSP) methods rely on time-consuming global searches and first-principles free energy calculations. Inspired by the recent success of deep learning approaches in protein structure prediction, which utilize pairwise amino acid interactions to describe 3D structures, we present AlphaCrystal-II, a novel knowledge-based solution that exploits the abundant inter-atomic interaction patterns found in existing known crystal structures. AlphaCrystal-II predicts the atomic distance matrix of a target crystal material and employs this matrix to reconstruct its 3D crystal structure. By leveraging the wealth of inter-atomic relationships of known crystal structures, our approach demonstrates remarkable effectiveness and reliability in structure prediction through comprehensive experiments. This work highlights the potential of data-driven methods in accelerating the discovery and design of new materials with tailored properties.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2404.00551.pdf' target='_blank'>https://arxiv.org/pdf/2404.00551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gao, Jian Huang, Yuling Jiao, Shurong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00551">Convergence of Continuous Normalizing Flows for Learning Probability Distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the error due to velocity estimation, the discretization error, and the early stopping error. A key step in our analysis involves establishing the regularity properties of the velocity field and its estimator for CNFs constructed with linear interpolation. This necessitates the development of uniform error bounds with Lipschitz regularity control of deep ReLU networks that approximate the Lipschitz function class, which could be of independent interest. Our nonparametric convergence analysis offers theoretical guarantees for using CNFs to learn probability distributions from a finite random sample.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2403.14915.pdf' target='_blank'>https://arxiv.org/pdf/2403.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anqi Dong, Can Chen, Tryphon T. Georgiou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14915">Network Learning with Directional Sign Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex systems can be effectively modeled via graphs that encode networked interactions, where relations between entities or nodes are often quantified by signed edge weights, e.g., promotion/inhibition in gene regulatory networks, or encoding political of friendship differences in social networks. However, it is often the case that only an aggregate consequence of such edge weights that characterize relations may be directly observable, as in protein expression of in gene regulatory networks. Thus, learning edge weights poses a significant challenge that is further exacerbated for intricate and large-scale networks. In this article, we address a model problem to determine the strength of sign-indefinite relations that explain marginal distributions that constitute our data. To this end, we develop a paradigm akin to that of the SchrÃ¶dinger bridge problem and an efficient Sinkhorn type algorithm (more properly, SchrÃ¶dinger-Fortet-Sinkhorn algorithm) that allows fast convergence to parameters that minimize a relative entropy/likelihood criterion between the sought signed adjacency matrix and a prior. The formalism that we present represents a novel generalization of the earlier SchrÃ¶dinger formalism in that marginal computations may incorporate weights that model directionality in underlying relations, and further, that it can be extended to high-order networks -- the SchrÃ¶dinger-Fortet-Sinkhorn algorithm that we derive is applicable all the same and allows geometric convergence to a sought sign-indefinite adjacency matrix or tensor, for high-order networks. We demonstrate our framework with synthetic and real-world examples.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2402.06079.pdf' target='_blank'>https://arxiv.org/pdf/2402.06079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehui Li, Yuhao Ni, William A V Beardall, Guoxuan Xia, Akashaditya Das, Guy-Bart Stan, Yiren Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06079">DiscDiff: Latent Diffusion Model for DNA Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting `round errors' inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2402.03675.pdf' target='_blank'>https://arxiv.org/pdf/2402.03675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenqing Hua, Connor Coley, Guy Wolf, Doina Precup, Shuangjia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03675">Effective Protein-Protein Interaction Exploration with PPIretrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2401.15122.pdf' target='_blank'>https://arxiv.org/pdf/2401.15122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Liu, Weitao Du, Hannan Xu, Yanjing Li, Zhuoxinran Li, Vignesh Bhethanabotla, Divin Yan, Christian Borgs, Anima Anandkumar, Hongyu Guo, Jennifer Chayes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15122">A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by utilizing machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations in protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) the BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory under Newtonian mechanics. For the experiment, we design ten single-trajectory and three multi-trajectory binding simulation tasks. We demonstrate the efficiency and effectiveness of NeuralMD, achieving over 1K$\times$ speedup compared to standard numerical MD simulations. NeuralMD also outperforms all other ML approaches, achieving up to 15$\times$ reduction in reconstruction error and 70% increase in validity. Additionally, we qualitatively illustrate that the oscillations in the predicted trajectories align more closely with ground-truth dynamics than those of other machine-learning methods. We believe NeuralMD paves the foundation for a new research paradigm in simulating protein-ligand dynamics.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2512.08772.pdf' target='_blank'>https://arxiv.org/pdf/2512.08772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamsini Ramanathan, Roman Bushuiev, Matouš Soldát, Jirí Kohout, Téo Hebra, Joshua David Smith, Josef Sivic, Tomáš Pluskal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08772">De novo generation of functional terpene synthases using TpsGPT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2512.06520.pdf' target='_blank'>https://arxiv.org/pdf/2512.06520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Pengmei, Spencer C. Guo, Chatipat Lorpaiboon, Aaron R. Dinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06520">Hierarchical geometric deep learning enables scalable analysis of molecular dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2512.05386.pdf' target='_blank'>https://arxiv.org/pdf/2512.05386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Kopko, David Graber, Saltuk Mustafa Eyrilmez, Stanislav Mazurenko, David Bednar, Jiri Sedlar, Josef Sivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05386">Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2511.22311.pdf' target='_blank'>https://arxiv.org/pdf/2511.22311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fiona Y. Wang, Di Sheng Lee, David L. Kaplan, Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22311">Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2511.10440.pdf' target='_blank'>https://arxiv.org/pdf/2511.10440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Pan, Evan Dramko, Mitchell D. Miller, Anastasios Kyrillidis, George N. Phillips
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10440">Completion of partial structures using Patterson maps with the CrysFormer machine learning model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2510.26611.pdf' target='_blank'>https://arxiv.org/pdf/2510.26611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter Benner, Boris N. Khoromskij, Venera Khoromskaia, Matthias Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26611">Fast tensor-based electrostatic energy calculations in the perspective of protein-ligand docking problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose and justify a new approach for fast calculation of the electrostatic interaction energy of clusters of charged particles in constrained energy minimization in the framework of rigid protein-ligand docking. Our ``blind search'' docking technique is based on the low-rank range-separated (RS) tensor-based representation of the free-space electrostatic potential of the biomolecule represented on large $n\times n\times n$ 3D grid. We show that both the collective electrostatic potential of a complex protein-ligand system and the respective electrostatic interaction energy can be calculated by tensor techniques in $O(n)$-complexity, such that the numerical cost for energy calculation only mildly (logarithmically) depends on the number of particles in the system. Moreover, tensor representation of the electrostatic potential enables usage of large 3D Cartesian grids (of the order of $n^3 \sim 10^{12}$), which could allow the accurate modeling of complexes with several large proteins. In our approach selection of the correct geometric pose predictions in the localized posing process is based on the control of van der Waals distance between the target molecular clusters. Here, we confine ourselves by constrained minimization of the energy functional by using only fast tensor-based free-space electrostatic energy recalculation for various rotations and translations of both clusters. Numerical tests of the electrostatic energy-based ``protein-ligand docking'' algorithm applied to synthetic and realistic input data present a proof of concept for rather complex particle configurations. The method may be used in the framework of the traditional stochastic or deterministic posing/docking techniques.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2510.16674.pdf' target='_blank'>https://arxiv.org/pdf/2510.16674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azam Shirali, Giri Narasimhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16674">Evaluating protein binding interfaces with PUMBA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein docking tools help in studying interactions between proteins, and are essential for drug, vaccine, and therapeutic development. However, the accuracy of a docking tool depends on a robust scoring function that can reliably differentiate between native and non-native complexes. PIsToN is a state-of-the-art deep learning-based scoring function that uses Vision Transformers in its architecture. Recently, the Mamba architecture has demonstrated exceptional performance in both natural language processing and computer vision, often outperforming Transformer-based models in their domains. In this study, we introduce PUMBA (Protein-protein interface evaluation with Vision Mamba), which improves PIsToN by replacing its Vision Transformer backbone with Vision Mamba. This change allows us to leverage Mamba's efficient long-range sequence modeling for sequences of image patches. As a result, the model's ability to capture both global and local patterns in protein-protein interface features is significantly improved. Evaluation on several widely-used, large-scale public datasets demonstrates that PUMBA consistently outperforms its original Transformer-based predecessor, PIsToN.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2510.11750.pdf' target='_blank'>https://arxiv.org/pdf/2510.11750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sazan Mahbub, Souvik Kundu, Eric P. Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11750">PRISM: Enhancing Protein Inverse Folding through Fine-Grained Retrieval on Structure-Sequence Multimodal Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein sequences that fold into a target three-dimensional structure, known as the inverse folding problem, is central to protein engineering but remains challenging due to the vast sequence space and the importance of local structural constraints. Existing deep learning approaches achieve strong recovery rates, yet they lack explicit mechanisms to reuse fine-grained structure-sequence patterns that are conserved across natural proteins. We present PRISM, a multimodal retrieval-augmented generation framework for inverse folding that retrieves fine-grained representations of potential motifs from known proteins and integrates them with a hybrid self-cross attention decoder. PRISM is formulated as a latent-variable probabilistic model and implemented with an efficient approximation, combining theoretical grounding with practical scalability. Across five benchmarks (CATH-4.2, TS50, TS500, CAMEO 2022, and the PDB date split), PRISM establishes new state of the art in both perplexity and amino acid recovery, while also improving foldability metrics (RMSD, TM-score, pLDDT), demonstrating that fine-grained multimodal retrieval is a powerful and efficient paradigm for protein sequence design.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2510.11188.pdf' target='_blank'>https://arxiv.org/pdf/2510.11188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Chen, Zuchao Li, Mengqi Gao, Yufeng Zhang, Chak Tou Leong, Haoyang Li, Jiaqi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11188">Protein as a Second Language for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the "Protein-as-Second-Language" framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence-question-answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process, we curate a bilingual corpus of 79,926 protein-QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4, achieving up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2510.03811.pdf' target='_blank'>https://arxiv.org/pdf/2510.03811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aya Laajil, Abduragim Shtanchaev, Sajan Muhammad, Eric Moulines, Salem Lahlou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03811">Curriculum-Augmented GFlowNets For mRNA Sequence Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing mRNA sequences is a major challenge in developing next-generation therapeutics, since it involves exploring a vast space of possible nucleotide combinations while optimizing sequence properties like stability, translation efficiency, and protein expression. While Generative Flow Networks are promising for this task, their training is hindered by sparse, long-horizon rewards and multi-objective trade-offs. We propose Curriculum-Augmented GFlowNets (CAGFN), which integrate curriculum learning with multi-objective GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based curriculum that progressively adapts the maximum sequence length guiding exploration from easier to harder subproblems. We also provide a new mRNA design environment for GFlowNets which, given a target protein sequence and a combination of biological objectives, allows for the training of models that generate plausible mRNA candidates. This provides a biologically motivated setting for applying and advancing GFlowNets in therapeutic sequence design. On different mRNA design tasks, CAGFN improves Pareto performance and biological plausibility, while maintaining diversity. Moreover, CAGFN reaches higher-quality solutions faster than a GFlowNet trained with random sequence sampling (no curriculum), and enables generalization to out-of-distribution sequences.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2510.02578.pdf' target='_blank'>https://arxiv.org/pdf/2510.02578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Cremer, Tuan Le, Mohammad M. Ghahremanpour, Emilia Sługocka, Filipe Menezes, Djork-Arné Clevert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02578">FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D ligand generation with joint binding affinity prediction and confidence estimation. The model supports de novo generation, pharmacophore-conditional sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50, pKi, pKd, pEC50). Training combines large-scale ligand libraries with mixed-fidelity protein-ligand complexes, followed by refinement on curated co-crystal datasets and parameter-efficient finetuning for project-specific adaptation. FLOWR:root achieves state-of-the-art performance in unconditional 3D molecule generation and pocket-conditional ligand design, producing geometrically realistic, low-strain structures. The integrated affinity prediction module demonstrates superior accuracy on the SPINDR test set and outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with substantial speed advantages. As a foundation model, FLOWR:root requires finetuning on project-specific datasets to account for unseen structure-activity landscapes, yielding strong correlation with experimental data. Joint generation and affinity prediction enable inference-time scaling through importance sampling, steering molecular design toward higher-affinity compounds. Case studies validate this: selective CK2$α$ ligand generation against CLK3 shows significant correlation between predicted and quantum-mechanical binding energies, while ER$α$ and TYK2 scaffold elaboration demonstrates strong agreement with QM calculations. By integrating structure-aware generation, affinity estimation, and property-guided sampling, FLOWR:root provides a comprehensive foundation for structure-based drug design spanning hit identification through lead optimization.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2509.21556.pdf' target='_blank'>https://arxiv.org/pdf/2509.21556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bianca Datta, Markus J. Buehler, Yvonne Chow, Kristina Gligoric, Dan Jurafsky, David L. Kaplan, Rodrigo Ledesma-Amaro, Giorgia Del Missier, Lisa Neidhardt, Karim Pichara, Benjamin Sanchez-Lengeling, Miek Schlangen, Skyler R. St. Pierre, Ilias Tagkopoulos, Anna Thomas, Nicholas J. Watson, Ellen Kuhl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21556">AI for Sustainable Future Foods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global food systems must deliver nutritious and sustainable foods while sharply reducing environmental impact. Yet, food innovation remains slow, empirical, and fragmented. Artificial intelligence (AI) now offers a transformative path with the potential to link molecular composition to functional performance, bridge chemical structure to sensory outcomes, and accelerate cross-disciplinary innovation across the entire production pipeline. Here we outline AI for Food as an emerging discipline that integrates ingredient design, formulation development, fermentation and production, texture analysis, sensory properties, manufacturing, and recipe generation. Early successes demonstrate how AI can predict protein performance, map molecules to flavor, and tailor consumer experiences. But significant challenges remain: lack of standardization, scarce multimodal data, cultural and nutritional diversity, and low consumer confidence. We propose three priorities to unlock the field: treating food as a programmable biomaterial, building self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By embedding AI responsibly into the food innovation cycle, we can accelerate the transition to sustainable protein systems and chart a predictive, design-driven science of food for our own health and the health of our planet.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2509.12976.pdf' target='_blank'>https://arxiv.org/pdf/2509.12976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taher Yacoub, Camille Depenveiller, Atsushi Tatsuma, Tin Barisin, Eugen Rusakov, Udo Gobel, Yuxu Peng, Shiqiang Deng, Yuki Kagaya, Joon Hong Park, Daisuke Kihara, Marco Guerra, Giorgio Palmieri, Andrea Ranieri, Ulderico Fugacci, Silvia Biasotti, Ruiwen He, Halim Benhabiles, Adnane Cabani, Karim Hammoudi, Haotian Li, Hao Huang, Chunyan Li, Alireza Tehrani, Fanwang Meng, Farnaz Heidar-Zadeh, Tuan-Anh Yang, Matthieu Montes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12976">SHREC 2025: Protein surface shape retrieval including electrostatic potential</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This SHREC 2025 track dedicated to protein surface shape retrieval involved 9 participating teams. We evaluated the performance in retrieval of 15 proposed methods on a large dataset of 11,555 protein surfaces with calculated electrostatic potential (a key molecular surface descriptor). The performance in retrieval of the proposed methods was evaluated through different metrics (Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best retrieval performance was achieved by the proposed methods that used the electrostatic potential complementary to molecular surface shape. This observation was also valid for classes with limited data which highlights the importance of taking into account additional molecular surface descriptors.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2508.12212.pdf' target='_blank'>https://arxiv.org/pdf/2508.12212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanliu Fan, Zicheng Ma, Jun Gao, Nan Yu, Jun Zhang, Ziqiang Cao, Yi Qin Gao, Guohong Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12212">ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein large language models, such as ProtTeX, represent both side-chain amino acids and backbone structure as discrete token sequences of residue length. While this design enables unified modeling of multimodal protein information, it suffers from two major limitations: (1) The concatenation of sequence and structure tokens approximately doubles the protein length and breaks the intrinsic residue-level alignment between modalities. (2) Constrained by the training corpus and limited context window, ProtTeX is typically trained on single-protein inputs, rendering it incompatible with in-context learning (ICL) and thus limiting its generalization capability. To address these issues, we propose ProtTeX-CC, a lightweight two-stage compression framework designed to enhance ProtTeX under few-shot settings. We first design a joint embedding compression mechanism that fuses sequence and structure representations at the residue level, effectively reducing the protein input length by half without sacrificing performance. Then we propose a self-compression module that aggregates each full demonstration into the latent space of the last few linguistic tokens, reducing the average demonstration length from 751 tokens to less than 16 tokens. Compared to the original ProtTeX, our self-compression approach achieves a compression ratio of approximately 93.68% in the total prompt length under the 16-shot setting. Without modifying the backbone model, ProtTeX-CC introduces only a small number of additional parameters through PEFT-based tuning in the joint embedding compression stage and a single trainable projection layer in the self-compression stage. Extensive experiments on protein function prediction show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and generalizes well to the out-of-domain dataset with a performance gain of 11%.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2507.12574.pdf' target='_blank'>https://arxiv.org/pdf/2507.12574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Deng, Spencer S. Ericksen, Anthony Gitter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12574">Assay2Mol: large language model-based drug design using BioAssay context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific databases aggregate vast amounts of quantitative data alongside descriptive text. In biochemistry, molecule screening assays evaluate candidate molecules' functional responses against disease targets. Unstructured text that describes the biological mechanisms through which these targets operate, experimental screening protocols, and other attributes of assays offer rich information for drug discovery campaigns but has been untapped because of that unstructured format. We present Assay2Mol, a large language model-based workflow that can capitalize on the vast existing biochemical screening assays for early-stage drug discovery. Assay2Mol retrieves existing assay records involving targets similar to the new target and generates candidate molecules using in-context learning with the retrieved assay screening data. Assay2Mol outperforms recent machine learning approaches that generate candidate ligand molecules for target protein structures, while also promoting more synthesizable molecule generation.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2507.07390.pdf' target='_blank'>https://arxiv.org/pdf/2507.07390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seonghyun Park, Kiyoung Seong, Soojung Yang, Rafael Gómez-Bombarelli, Sungsoo Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07390">Learning Collective Variables for Enhanced Sampling from BioEmu with Time-Lagged Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics is crucial for understanding molecular systems but its applicability is often limited by the vast timescales of rare events like protein folding. Enhanced sampling techniques overcome this by accelerating the simulation along key reaction pathways, which are defined by collective variables (CVs). However, identifying effective CVs that capture the slow, macroscopic dynamics of a system remains a major bottleneck. This work proposes a novel framework coined BioEmu-CV that learns these essential CVs automatically from BioEmu, a recently proposed foundation model for generating protein equilibrium samples. In particular, we re-purpose BioEmu to learn time-lagged generation conditioned on the learned CV, i.e., predict the distribution of molecular states after a certain amount of time. This training process promotes the CV to encode only the slow, long-term information while disregarding fast, random fluctuations. We validate our learned CV on fast-folding proteins with two key applications: (1) estimating free energy differences using on-the-fly probability enhanced sampling and (2) sampling transition paths with steered molecular dynamics. Our empirical study also serves as a new systematic and comprehensive benchmark for MLCVs on fast-folding proteins larger than Alanine Dipeptide.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2507.02902.pdf' target='_blank'>https://arxiv.org/pdf/2507.02902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhang, Mingyuan Zhou, Wesley Tansey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02902">Controllable diffusion-based generation for multi-channel biological data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial profiling technologies in biology, such as imaging mass cytometry (IMC) and spatial transcriptomics (ST), generate high-dimensional, multi-channel data with strong spatial alignment and complex inter-channel relationships. Generative modeling of such data requires jointly capturing intra- and inter-channel structure, while also generalizing across arbitrary combinations of observed and missing channels for practical application. Existing diffusion-based models generally assume low-dimensional inputs (e.g., RGB images) and rely on simple conditioning mechanisms that break spatial correspondence and ignore inter-channel dependencies. This work proposes a unified diffusion framework for controllable generation over structured and spatial biological data. Our model contains two key innovations: (1) a hierarchical feature injection mechanism that enables multi-resolution conditioning on spatially aligned channels, and (2) a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. To support flexible conditioning and generalization to arbitrary subsets of observed channels, we train the model using a random masking strategy, enabling it to reconstruct missing channels from any combination of inputs. We demonstrate state-of-the-art performance across both spatial and non-spatial prediction tasks, including protein imputation in IMC and gene-to-protein prediction in single-cell datasets, and show strong generalization to unseen conditional configurations.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2506.05596.pdf' target='_blank'>https://arxiv.org/pdf/2506.05596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jes Frellsen, Maher M. Kassem, Tone Bengtsen, Lars Olsen, Kresten Lindorff-Larsen, Jesper Ferkinghoff-Borg, Wouter Boomsma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05596">Zero-shot protein stability prediction by inverse folding models: a free energy interpretation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse folding models have proven to be highly effective zero-shot predictors of protein stability. Despite this success, the link between the amino acid preferences of an inverse folding model and the free-energy considerations underlying thermodynamic stability remains incompletely understood. A better understanding would be of interest not only from a theoretical perspective, but also potentially provide the basis for stronger zero-shot stability prediction. In this paper, we take steps to clarify the free-energy foundations of inverse folding models. Our derivation reveals the standard practice of likelihood ratios as a simplistic approximation and suggests several paths towards better estimates of the relative stability. We empirically assess these approaches and demonstrate that considerable gains in zero-shot performance can be achieved with fairly simple means.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2505.13375.pdf' target='_blank'>https://arxiv.org/pdf/2505.13375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Kolloff, Tobias HÃ¶ppe, Emmanouil Angelis, Mathias Jacob Schreiner, Stefan Bauer, Andrea Dittadi, Simon Olsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13375">Minimum-Excess-Work Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a regularization framework inspired by thermodynamic work for guiding pre-trained probability flow generative models (e.g., continuous normalizing flows or diffusion models) by minimizing excess work, a concept rooted in statistical mechanics and with strong conceptual connections to optimal transport. Our approach enables efficient guidance in sparse-data regimes common to scientific applications, where only limited target samples or partial density constraints are available. We introduce two strategies: Path Guidance for sampling rare transition states by concentrating probability mass on user-defined subsets, and Observable Guidance for aligning generated distributions with experimental observables while preserving entropy. We demonstrate the framework's versatility on a coarse-grained protein model, guiding it to sample transition configurations between folded/unfolded states and correct systematic biases using experimental data. The method bridges thermodynamic principles with modern generative architectures, offering a principled, efficient, and physics-inspired alternative to standard fine-tuning in data-scarce domains. Empirical results highlight improved sample efficiency and bias reduction, underscoring its applicability to molecular simulations and beyond.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2505.02639.pdf' target='_blank'>https://arxiv.org/pdf/2505.02639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Qingrui Liu, Hongxin Xiang, Daojian Zeng, Xiangxiang Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02639">Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2504.15508.pdf' target='_blank'>https://arxiv.org/pdf/2504.15508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxiong Li, Beining Zhang, Mingzhen Li, Siyu Hu, Jinzhe Zeng, Lijun Liu, Guojun Yuan, Zhan Wang, Guangming Tan, Weile Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15508">Scaling Neural-Network-Based Molecular Dynamics with Long-Range Electrostatic Interactions to 51 Nanoseconds per Day</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural network-based molecular dynamics (NNMD) simulations incorporating long-range electrostatic interactions have significantly extended the applicability to heterogeneous and ionic systems, enabling effective modeling critical physical phenomena such as protein folding and dipolar surface and maintaining ab initio accuracy. However, neural network inference and long-range force computation remain the major bottlenecks, severely limiting simulation speed. In this paper, we target DPLR, a state-of-the-art NNMD package that supports long-range electrostatics, and propose a set of comprehensive optimizations to enhance computational efficiency. We introduce (1) a hardware-offloaded FFT method to reduce the communication overhead; (2) an overlapping strategy that hides long-range force computations using a single core per node, and (3) a ring-based load balancing method that enables atom-level task evenly redistribution with minimal communication overhead. Experimental results on the Fugaku supercomputer show that our work achieves a 37x performance improvement, reaching a maximum simulation speed of 51 ns/day.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2503.08179.pdf' target='_blank'>https://arxiv.org/pdf/2503.08179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zicheng Ma, Chuanliu Fan, Zhicong Wang, Zhenyu Chen, Xiaohan Lin, Yanheng Li, Shihao Feng, Jun Zhang, Ziqiang Cao, Yi Qin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08179">ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models have made remarkable progress in the field of molecular science, particularly in understanding and generating functional small molecules. This success is largely attributed to the effectiveness of molecular tokenization strategies. In protein science, the amino acid sequence serves as the sole tokenizer for LLMs. However, many fundamental challenges in protein science are inherently structure-dependent. The absence of structure-aware tokens significantly limits the capabilities of LLMs for comprehensive biomolecular comprehension and multimodal generation. To address these challenges, we introduce a novel framework, ProtTeX, which tokenizes the protein sequences, structures, and textual information into a unified discrete space. This innovative approach enables joint training of the LLM exclusively through the Next-Token Prediction paradigm, facilitating multimodal protein reasoning and generation. ProtTeX enables general LLMs to perceive and process protein structures through sequential text input, leverage structural information as intermediate reasoning components, and generate or manipulate structures via sequential text output. Experiments demonstrate that our model achieves significant improvements in protein function prediction, outperforming the state-of-the-art domain expert model with a twofold increase in accuracy. Our framework enables high-quality conformational generation and customizable protein design. For the first time, we demonstrate that by adopting the standard training and inference pipelines from the LLM domain, ProtTeX empowers decoder-only LLMs to effectively address diverse spectrum of protein-related tasks.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2503.00143.pdf' target='_blank'>https://arxiv.org/pdf/2503.00143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Pan, Evan Dramko, Mitchell D. Miller, George N. Phillips, Anastasios Kyrillidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00143">RecCrysFormer: Refined Protein Structural Prediction from 3D Patterson Maps via Recycling Training Runs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining protein structures at an atomic level remains a significant challenge in structural biology. We introduce $\texttt{RecCrysFormer}$, a hybrid model that exploits the strengths of transformers with the aim of integrating experimental and ML approaches to protein structure determination from crystallographic data. $\texttt{RecCrysFormer}$ leverages Patterson maps and incorporates known standardized partial structures of amino acid residues to directly predict electron density maps, which are essential for constructing detailed atomic models through crystallographic refinement processes. $\texttt{RecCrysFormer}$ benefits from a ``recycling'' training regimen that iteratively incorporates results from crystallographic refinements and previous training runs as additional inputs in the form of template maps. Using a preliminary dataset of synthetic peptide fragments based on Protein Data Bank, $\texttt{RecCrysFormer}$ achieves good accuracy in structural predictions and shows robustness against variations in crystal parameters, such as unit cell dimensions and angles.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2502.04468.pdf' target='_blank'>https://arxiv.org/pdf/2502.04468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Denker, Shreyas Padhy, Francisco Vargas, Johannes Hertrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04468">Iterative Importance Fine-tuning of Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling, inverse problems and reward fine-tuning for text-to-image diffusion models.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2412.19945.pdf' target='_blank'>https://arxiv.org/pdf/2412.19945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beyza E. Ortlek, Ozgur B. Akan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19945">Modeling and Analysis of SCFA-Driven Vagus Nerve Signaling in the Gut-Brain Axis via Molecular Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular communication (MC) is a bio-inspired communication paradigm that utilizes molecules to transfer information and offers a robust framework for understanding biological signaling systems. This paper introduces a novel end-to-end MC framework for short-chain fatty acid (SCFA)-driven vagus nerve signaling within the gut-brain axis (GBA) to enhance our understanding of gut-brain communication mechanisms. SCFA molecules, produced by gut microbiota, serve as important biomarkers in physiological and psychological processes, including neurodegenerative and mental health disorders. The developed end-to-end model integrates SCFA binding to vagal afferent fibers, G protein-coupled receptor (GPCR)-mediated calcium signaling, and Hodgkin-Huxley-based action potential generation into a comprehensive vagus nerve signaling mechanism through GBA. Information-theoretic metrics such as mutual information and delay are used to evaluate the efficiency of this SCFA-driven signaling pathway model. Simulations demonstrate how molecular inputs translate into neural outputs, highlighting critical aspects that govern gut-brain communication. In this work, the integration of SCFA-driven signaling into the MC framework provides a novel perspective on gut-brain communication and paves the way for the development of innovative therapeutic advancements targeting neurological and psychiatric disorders.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2411.18648.pdf' target='_blank'>https://arxiv.org/pdf/2411.18648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Lin, Mingjie Li, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18648">MADE: Graph Backdoor Defense with Masked Unlearning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have garnered significant attention from researchers due to their outstanding performance in handling graph-related tasks, such as social network analysis, protein design, and so on. Despite their widespread application, recent research has demonstrated that GNNs are vulnerable to backdoor attacks, implemented by injecting triggers into the training datasets. Trained on the poisoned data, GNNs will predict target labels when attaching trigger patterns to inputs. This vulnerability poses significant security risks for applications of GNNs in sensitive domains, such as drug discovery. While there has been extensive research into backdoor defenses for images, strategies to safeguard GNNs against such attacks remain underdeveloped. Furthermore, we point out that conventional backdoor defense methods designed for images cannot work well when directly implemented on graph data. In this paper, we first analyze the key difference between image backdoor and graph backdoor attacks. Then we tackle the graph defense problem by presenting a novel approach called MADE, which devises an adversarial mask generation mechanism that selectively preserves clean sub-graphs and further leverages masks on edge weights to eliminate the influence of triggers effectively. Extensive experiments across various graph classification tasks demonstrate the effectiveness of MADE in significantly reducing the attack success rate (ASR) while maintaining a high classification accuracy.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2411.02109.pdf' target='_blank'>https://arxiv.org/pdf/2411.02109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anton Bushuiev, Roman Bushuiev, Nikola Zadorozhny, Raman Samusevich, Hannes StÃ¤rk, Jiri Sedlar, TomÃ¡Å¡ Pluskal, Josef Sivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02109">Training on test proteins improves fitness, structure, and function prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data scarcity and distribution shifts often hinder the ability of machine learning models to generalize when applied to proteins and other biological data. Self-supervised pre-training on large datasets is a common method to enhance generalization. However, striving to perform well on all possible proteins can limit model's capacity to excel on any specific one, even though practitioners are often most interested in accurate predictions for the individual protein they study. To address this limitation, we propose an orthogonal approach to achieve generalization. Building on the prevalence of self-supervised pre-training, we introduce a method for self-supervised fine-tuning at test time, allowing models to adapt to the test protein of interest on the fly and without requiring any additional data. We study our test-time training (TTT) method through the lens of perplexity minimization and show that it consistently enhances generalization across different models, their scales, and datasets. Notably, our method leads to new state-of-the-art results on the standard benchmark for protein fitness prediction, improves protein structure prediction for challenging targets, and enhances function prediction accuracy.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2410.20182.pdf' target='_blank'>https://arxiv.org/pdf/2410.20182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Deng, Spencer S. Ericksen, Anthony Gitter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20182">Chemical Language Model Linker: blending text and molecules with modular adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of large language models and multi-modal models has enabled the appealing idea of generating novel molecules from text descriptions. Generative modeling would shift the paradigm from relying on large-scale chemical screening to find molecules with desired properties to directly generating those molecules. However, multi-modal models combining text and molecules are often trained from scratch, without leveraging existing high-quality pretrained models. Training from scratch consumes more computational resources and prohibits model scaling. In contrast, we propose a lightweight adapter-based strategy named Chemical Language Model Linker (ChemLML). ChemLML blends the two single domain models and obtains conditional molecular generation from text descriptions while still operating in the specialized embedding spaces of the molecular domain. ChemLML can tailor diverse pretrained text models for molecule generation by training relatively few adapter parameters. We find that the choice of molecular representation used within ChemLML, SMILES versus SELFIES, has a strong influence on conditional molecular generation performance. SMILES is often preferable despite not guaranteeing valid molecules. We raise issues in using the entire PubChem dataset of molecules and their associated descriptions for evaluating molecule generation and provide a filtered version of the dataset as a generation test set. To demonstrate how ChemLML could be used in practice, we generate candidate protein inhibitors and use docking to assess their quality and also generate candidate membrane permeable molecules.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2410.07968.pdf' target='_blank'>https://arxiv.org/pdf/2410.07968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Yiquan Wang, Longji Xu, Yuhua Dong, Tin-Yeh Huang, Xiang Li, Jia Deng, Rui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07968">Octopus Inspired Optimization (OIO): A Hierarchical Framework for Navigating Protein Fitness Landscapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the vast, rugged, and multi-modal fitness landscapes of protein sequences presents a formidable challenge for computational protein engineering, often trapping algorithms in suboptimal solutions. Existing methods struggle with the exploration-exploitation dilemma, failing to synergize global search with deep local refinement. To overcome this critical barrier, we introduce the Octopus Inspired Optimization (OIO), a novel hierarchical metaheuristic that mimics the octopus's unique neural architecture of centralized control and decentralized, parallel execution. OIO's "individual-tentacle-sucker" framework provides an intrinsic unification of global exploration and parallelized local exploitation, making it structurally ideal for complex combinatorial problems. We validated OIO's efficacy through a rigorous three-tiered experimental framework. On a real-world Green Fluorescent Protein (GFP) design benchmark, OIO surpassed a comprehensive suite of 15 competing metaheuristics, including 7 classic algorithms and 8 state-of-the-art methods from the past two years, delivering performance comparable only to a specialized local search algorithm. This success is explained by its fundamental strengths: OIO ranked first on the NK-Landscape benchmark, confirming its architectural suitability for protein-like fitness landscapes, and also ranked first on the gold-standard CEC2022 benchmark, demonstrating the raw power and efficiency of its optimization engine. OIO establishes a new, nature-inspired paradigm for protein engineering, offering a robust and powerful tool with significant potential for advancing therapeutic and enzymatic design.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2409.09057.pdf' target='_blank'>https://arxiv.org/pdf/2409.09057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Aledhari, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09057">Gene and RNA Editing: Methods, Enabling Technologies, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gene and RNA editing methods, technologies, and applications are emerging as innovative forms of therapy and medicine, offering more efficient implementation compared to traditional pharmaceutical treatments. Current trends emphasize the urgent need for advanced methods and technologies to detect public health threats, including diseases and viral agents. Gene and RNA editing techniques enhance the ability to identify, modify, and ameliorate the effects of genetic diseases, disorders, and disabilities. Viral detection and identification methods present numerous opportunities for enabling technologies, such as CRISPR, applicable to both RNA and gene editing through the use of specific Cas proteins. This article explores the distinctions and benefits of RNA and gene editing processes, emphasizing their contributions to the future of medical treatment. CRISPR technology, particularly its adaptation via the Cas13 protein for RNA editing, is a significant advancement in gene editing. The article will delve into RNA and gene editing methodologies, focusing on techniques that alter and modify genetic coding. A-to-I and C-to-U editing are currently the most predominant methods of RNA modification. CRISPR stands out as the most cost-effective and customizable technology for both RNA and gene editing. Unlike permanent changes induced by cutting an individual's DNA genetic code, RNA editing offers temporary modifications by altering nucleoside bases in RNA strands, which can then attach to DNA strands as temporary modifiers.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2407.21028.pdf' target='_blank'>https://arxiv.org/pdf/2407.21028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NataÅ¡a Tagasovska, Ji Won Park, Matthieu Kirchmeyer, Nathan C. Frey, Andrew Martin Watkins, Aya Abdelsalam Ismail, Arian Rokkum Jamasb, Edith Lee, Tyler Bryson, Stephen Ra, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21028">Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) has demonstrated significant promise in accelerating drug design. Active ML-guided optimization of therapeutic molecules typically relies on a surrogate model predicting the target property of interest. The model predictions are used to determine which designs to evaluate in the lab, and the model is updated on the new measurements to inform the next cycle of decisions. A key challenge is that the experimental feedback from each cycle inspires changes in the candidate proposal or experimental protocol for the next cycle, which lead to distribution shifts. To promote robustness to these shifts, we must account for them explicitly in the model training. We apply domain generalization (DG) methods to classify the stability of interactions between an antibody and antigen across five domains defined by design cycles. Our results suggest that foundational models and ensembling improve predictive performance on out-of-distribution domains. We publicly release our codebase extending the DG benchmark ``DomainBed,'' and the associated dataset of antibody sequences and structures emulating distribution shifts across design cycles.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2407.11548.pdf' target='_blank'>https://arxiv.org/pdf/2407.11548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wu, Xiao Yi, Yang Tan, Huiqun Yu, Guisheng Fan, Gaowei Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11548">A PLMs based protein retrieval framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein retrieval, which targets the deconstruction of the relationship between sequences, structures and functions, empowers the advancing of biology. Basic Local Alignment Search Tool (BLAST), a sequence-similarity-based algorithm, has proved the efficiency of this field. Despite the existing tools for protein retrieval, they prioritize sequence similarity and probably overlook proteins that are dissimilar but share homology or functionality. In order to tackle this problem, we propose a novel protein retrieval framework that mitigates the bias towards sequence similarity. Our framework initiatively harnesses protein language models (PLMs) to embed protein sequences within a high-dimensional feature space, thereby enhancing the representation capacity for subsequent analysis. Subsequently, an accelerated indexed vector database is constructed to facilitate expedited access and retrieval of dense vectors. Extensive experiments demonstrate that our framework can equally retrieve both similar and dissimilar proteins. Moreover, this approach enables the identification of proteins that conventional methods fail to uncover. This framework will effectively assist in protein mining and empower the development of biology.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2406.18330.pdf' target='_blank'>https://arxiv.org/pdf/2406.18330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Halfon, Eyal Rozenberg, Ehud Rivlin, Daniel Freedman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18330">Molecular Diffusion Models with Virtual Receptors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning approaches to Structure-Based Drug Design (SBDD) have proven quite fertile over the last few years. In particular, diffusion-based approaches to SBDD have shown great promise. We present a technique which expands on this diffusion approach in two crucial ways. First, we address the size disparity between the drug molecule and the target/receptor, which makes learning more challenging and inference slower. We do so through the notion of a Virtual Receptor, which is a compressed version of the receptor; it is learned so as to preserve key aspects of the structural information of the original receptor, while respecting the relevant group equivariance. Second, we incorporate a protein language embedding used originally in the context of protein folding. We experimentally demonstrate the contributions of both the virtual receptors and the protein embeddings: in practice, they lead to both better performance, as well as significantly faster computations.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2406.05347.pdf' target='_blank'>https://arxiv.org/pdf/2406.05347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Chen, Zhilei Bei, Xingyi Cheng, Pan Li, Jie Tang, Le Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05347">MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high quality MSA. Although various methods have been proposed to generate virtual MSA under these conditions, they fall short in comprehensively capturing the intricate coevolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pretraining in the low MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model complex evolutionary patterns. Endowed by this, its flexible 1D MSA decoding framework facilitates zero or few shot learning. Moreover, we demonstrate that leveraging the feedback from AlphaFold2 can further enhance the model capacity via Rejective Fine tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful virtual MSA to enhance the structure prediction accuracy. The transfer learning capabilities also highlight its great potential for facilitating other protein tasks.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2406.04052.pdf' target='_blank'>https://arxiv.org/pdf/2406.04052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Liu, David Ruhe, Patrick ForrÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04052">Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either consider mostly scalar information such as distances and angles or have a very high computational complexity. In this work, we test a few novel message passing graph neural networks (GNNs) based on Clifford multivectors, structured similarly to other prevalent equivariant models in geometric deep learning. Our approach leverages efficient invariant scalar features while simultaneously performing expressive learning on multivector representations, particularly through the use of the equivariant geometric product operator. By integrating these elements, our methods outperform established efficient baseline models on an N-Body simulation task and protein denoising task while maintaining a high efficiency. In particular, we push the state-of-the-art error on the N-body dataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent methods. Our implementation is available on Github.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2406.00812.pdf' target='_blank'>https://arxiv.org/pdf/2406.00812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueming Lyu, Kim Yong Tan, Yew Soon Ong, Ivor W. Tsang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00812">Covariance-Adaptive Sequential Black-box Optimization for Diffusion Targeted Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc. However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging. To address this issue, we first formulate the fine-tuning of the targeted reserve-time stochastic differential equation (SDE) associated with a pre-trained diffusion model as a sequential black-box optimization problem. Furthermore, we propose a novel covariance-adaptive sequential optimization algorithm to optimize cumulative black-box scores under unknown transition dynamics. Theoretically, we prove a $O(\frac{d^2}{\sqrt{T}})$ convergence rate for cumulative convex functions without smooth and strongly convex assumptions. Empirically, experiments on both numerical test problems and target-guided 3D-molecule generation tasks show the superior performance of our method in achieving better target scores.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2405.14925.pdf' target='_blank'>https://arxiv.org/pdf/2405.14925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Cremer, Tuan Le, Frank NoÃ©, Djork-ArnÃ© Clevert, Kristof T. SchÃ¼tt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14925">PILOT: Equivariant diffusion for pocket conditioned de novo ligand generation with multi-objective guidance via importance sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of ligands that both are tailored to a given protein pocket and exhibit a range of desired chemical properties is a major challenge in structure-based drug design. Here, we propose an in-silico approach for the $\textit{de novo}$ generation of 3D ligand structures using the equivariant diffusion model PILOT, combining pocket conditioning with a large-scale pre-training and property guidance. Its multi-objective trajectory-based importance sampling strategy is designed to direct the model towards molecules that not only exhibit desired characteristics such as increased binding affinity for a given protein pocket but also maintains high synthetic accessibility. This ensures the practicality of sampled molecules, thus maximizing their potential for the drug discovery pipeline. PILOT significantly outperforms existing methods across various metrics on the common benchmark dataset CrossDocked2020. Moreover, we employ PILOT to generate novel ligands for unseen protein pockets from the Kinodata-3D dataset, which encompasses a substantial portion of the human kinome. The generated structures exhibit predicted $IC_{50}$ values indicative of potent biological activity, which highlights the potential of PILOT as a powerful tool for structure-based drug design.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2404.17452.pdf' target='_blank'>https://arxiv.org/pdf/2404.17452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Michael, Simon Bartels, Miguel GonzÃ¡lez-Duque, Yevgen Zainchkovskyy, Jes Frellsen, SÃ¸ren Hauberg, Wouter Boomsma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17452">A Continuous Relaxation for Discrete Bayesian Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To optimize efficiently over discrete data and with only few available target observations is a challenge in Bayesian optimization. We propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable. We consider in particular the optimization domain where very few observations and strict budgets exist; motivated by optimizing protein sequences for expensive to evaluate bio-chemical properties. The advantages of our approach are two-fold: the problem is treated in the continuous setting, and available prior knowledge over sequences can be incorporated directly. More specifically, we utilize available and learned distributions over the problem domain for a weighting of the Hellinger distance which yields a covariance function. We show that the resulting acquisition function can be optimized with both continuous or discrete optimization algorithms and empirically assess our method on two bio-chemical sequence optimization tasks.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2404.10457.pdf' target='_blank'>https://arxiv.org/pdf/2404.10457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anton Bushuiev, Roman Bushuiev, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, Josef Sivic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10457">Revealing data leakage in protein interaction benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been remarkable progress in machine learning for protein-protein interactions. However, prior work has predominantly focused on improving learning algorithms, with less attention paid to evaluation strategies and data preparation. Here, we demonstrate that further development of machine learning methods may be hindered by the quality of existing train-test splits. Specifically, we find that commonly used splitting strategies for protein complexes, based on protein sequence or metadata similarity, introduce major data leakage. This may result in overoptimistic evaluation of generalization, as well as unfair benchmarking of the models, biased towards assessing their overfitting capacity rather than practical utility. To overcome the data leakage, we recommend constructing data splits based on 3D structural similarity of protein-protein interfaces and suggest corresponding algorithms. We believe that addressing the data leakage problem is critical for further progress in this research area.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2403.01158.pdf' target='_blank'>https://arxiv.org/pdf/2403.01158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungwon Kim, D. ChangMo Yang, Soohaeng Yoo Willow, Chang Woo Myung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01158">A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the pivotal role of oxygen-containing organic compounds in serving as an energy source for living organisms and contributing to protein formation is crucial in the field of biochemistry. This study addresses the challenge of comprehending protein-protein interactions (PPI) and developing predicitive models for proteins and organic compounds, with a specific focus on quantifying their binding affinity. Here, we introduce the active Bayesian Committee Machine (BCM) potential, specifically designed to predict oxygen-containing organic compounds within eight groups of CHO. The BCM potential adopts a committee-based approach to tackle scalability issues associated with kernel regressors, particularly when dealing with large datasets. Its adaptable structure allows for efficient and cost-effective expansion, maintaing both transferability and scalability. Through systematic benchmarking, we position the sparse BCM potential as a promising contender in the pursuit of a universal machine learning potential.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2402.11729.pdf' target='_blank'>https://arxiv.org/pdf/2402.11729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gautam Machiraju, Alexander Derry, Arjun Desai, Neel Guha, Amir-Hossein Karimi, James Zou, Russ Altman, Christopher RÃ©, Parag Mallick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11729">Prospector Heads: Generalized Feature Attribution for Large Models & Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for ML models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based attribution methods that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 26.3 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for ML models in complex domains.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2401.02882.pdf' target='_blank'>https://arxiv.org/pdf/2401.02882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jai Prakash Veerla, Partha Sai Guttikonda, Amir Hajighasemi, Jillur Rahman Saurav, Aarti Darji, Cody T. Reynolds, Mohamed Mohamed, Mohammad S. Nasr, Helen H. Shang, Jacob M. Luber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02882">SpatialVisVR: An Immersive, Multiplexed Medical Image Viewer With Contextual Similar-Patient Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In contemporary pathology, multiplexed immunofluorescence (mIF) and multiplex immunohistochemistry (mIHC) present both significant opportunities and challenges. These methodologies shed light on intricate tumor microenvironment interactions, emphasizing the need for intuitive visualization tools to analyze vast biological datasets effectively. As electronic health records (EHR) proliferate and physicians face increasing information overload, the integration of advanced technologies becomes imperative. SpatialVisVR emerges as a versatile VR platform tailored for comparing medical images, with adaptability for data privacy on embedded hardware. Clinicians can capture pathology slides in real-time via mobile devices, leveraging SpatialVisVR's deep learning algorithm to match and display similar mIF images. This interface supports the manipulation of up to 100 multiplexed protein channels, thereby assisting in immuno-oncology decision-making. Ultimately, SpatialVisVR aims to streamline diagnostic processes, advocating for a comprehensive and efficient approach to immuno-oncology research and treatment.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2512.10309.pdf' target='_blank'>https://arxiv.org/pdf/2512.10309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Weng, Xinyi Zhu, Jing Liu, Linyuan Lü, Pan Zhang, Ying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10309">Tracking large chemical reaction networks and rare events by neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chemical reaction networks are widely used to model stochastic dynamics in chemical kinetics, systems biology and epidemiology. Solving the chemical master equation that governs these systems poses a significant challenge due to the large state space exponentially growing with system sizes. The development of autoregressive neural networks offers a flexible framework for this problem; however, its efficiency is limited especially for high-dimensional systems and in scenarios with rare events. Here, we push the frontier of neural-network approach by exploiting faster optimizations such as natural gradient descent and time-dependent variational principle, achieving a 5- to 22-fold speedup, and by leveraging enhanced-sampling strategies to capture rare events. We demonstrate reduced computational cost and higher accuracy over the previous neural-network method in challenging reaction networks, including the mitogen-activated protein kinase (MAPK) cascade network, the hitherto largest biological network handled by the previous approaches of solving the chemical master equation. We further apply the approach to spatially extended reaction-diffusion systems, the Schlögl model with rare events, on two-dimensional lattices, beyond the recent tensor-network approach that handles one-dimensional lattices. The present approach thus enables efficient modeling of chemical reaction networks in general.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2512.09566.pdf' target='_blank'>https://arxiv.org/pdf/2512.09566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Ji, Zhangfan Yang, Dong Xu, Ruibin Bai, Jianqiang Li, Tingjun Hou, Zexuan Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09566">Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2512.09365.pdf' target='_blank'>https://arxiv.org/pdf/2512.09365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Qin, Zhengquan Luo, Guy Tadmor, Changyou Chen, David Zeevi, Zhiqiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09365">KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2512.02031.pdf' target='_blank'>https://arxiv.org/pdf/2512.02031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Mahmood, Pedro O. Pinheiro, Richard Bonneau, Saeed Saremi, Vishnu Sresht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02031">Pharmacophore-based design by learning on voxel grids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce \emph{VoxCap}, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: \emph{de-novo} design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse \textit{de-novo} hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2512.00126.pdf' target='_blank'>https://arxiv.org/pdf/2512.00126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Han, Tianfan Fu, Wu-Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00126">RadDiff: Retrieval-Augmented Denoising Diffusion for Protein Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein inverse folding, the design of an amino acid sequence based on a target 3D structure, is a fundamental problem of computational protein engineering. Existing methods either generate sequences without leveraging external knowledge or relying on protein language models (PLMs). The former omits the evolutionary information stored in protein databases, while the latter is parameter-inefficient and inflexible to adapt to ever-growing protein data. To overcome the above drawbacks, in this paper we propose a novel method, called retrieval-augmented denoising diffusion (RadDiff), for protein inverse folding. Given the target protein backbone, RadDiff uses a hierarchical search strategy to efficiently retrieve structurally similar proteins from large protein databases. The retrieved structures are then aligned residue-by-residue to the target to construct a position-specific amino acid profile, which serves as an evolutionary-informed prior that conditions the denoising process. A lightweight integration module is further designed to incorporate this prior effectively. Experimental results on the CATH, PDB, and TS50 datasets show that RadDiff consistently outperforms existing methods, improving sequence recovery rate by up to 19%. Experimental results also demonstrate that RadDiff generates highly foldable sequences and scales effectively with database size.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2510.24732.pdf' target='_blank'>https://arxiv.org/pdf/2510.24732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhua Chen, Simon Mathis, Charles Harris, Kieran Didi, Pietro Lio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24732">Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling techniques such as Diffusion and Flow Matching have achieved significant successes in generating designable and diverse protein backbones. However, many current models are computationally expensive, requiring hundreds or even thousands of function evaluations (NFEs) to yield samples of acceptable quality, which can become a bottleneck in practical design campaigns that often generate $10^4\ -\ 10^6$ designs per target. In image generation, Rectified Flows (ReFlow) can significantly reduce the required NFEs for a given target quality, but their application in protein backbone generation has been less studied. We apply ReFlow to improve the low NFE performance of pretrained SE(3) flow matching models for protein backbone generation and systematically study ReFlow design choices in the context of protein generation in data curation, training and inference time settings. In particular, we (1) show that ReFlow in the protein domain is particularly sensitive to the choice of coupling generation and annealing, (2) demonstrate how useful design choices for ReFlow in the image domain do not directly translate to better performance on proteins, and (3) make improvements to ReFlow methodology for proteins.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2510.17826.pdf' target='_blank'>https://arxiv.org/pdf/2510.17826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carles Navarro, Mariona Torrens, Philipp Thölke, Stefan Doerr, Gianni De Fabritiis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17826">Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2510.17786.pdf' target='_blank'>https://arxiv.org/pdf/2510.17786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Stecklov, Noah El Rimawi-Fine, Mathieu Blanchette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17786">Inference-Time Compute Scaling For Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Allocating extra computation at inference time has recently improved sample quality in large language models and diffusion-based image generation. In parallel, Flow Matching (FM) has gained traction in language, vision, and scientific domains, but inference-time scaling methods for it remain under-explored. Concurrently, Kim et al., 2025 approach this problem but replace the linear interpolant with a non-linear variance-preserving (VP) interpolant at inference, sacrificing FM's efficient and straight sampling. Additionally, inference-time compute scaling for flow matching has only been applied to visual tasks, like image generation. We introduce novel inference-time scaling procedures for FM that preserve the linear interpolant during sampling. Evaluations of our method on image generation, and for the first time (to the best of our knowledge), unconditional protein generation, show that I) sample quality consistently improves as inference compute increases, and II) flow matching inference-time scaling can be applied to scientific domains.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2510.14139.pdf' target='_blank'>https://arxiv.org/pdf/2510.14139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Islam Akef Ebeid, Haoteng Tang, Pengfei Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14139">Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduction Accurate prediction of protein-protein interactions (PPIs) is crucial for understanding cellular functions and advancing drug development. Existing in-silico methods use direct sequence embeddings from Protein Language Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein structures. This study explores less computationally intensive alternatives. We introduce a novel framework for downstream PPI prediction through link prediction. Methods We introduce a two-stage graph representation learning framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach models a protein's primary structure as a hierarchy of globally inferred n-gram graphs. In these graphs, residue transition probabilities define edge weights. Each edge connects a pair of residues in a directed graph. The probabilities are aggregated from a large corpus of sequences. Second, we propose DirectGCN, a custom directed graph convolutional neural network. This model features a unique convolutional layer. It processes information through separate path-specific transformations: incoming, outgoing, and undirected. A shared transformation is also applied. These paths are combined via a learnable gating mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level embeddings. These embeddings are pooled via attention to generate protein-level embeddings for prediction. Results We first established the efficacy of DirectGCN on standard node classification benchmarks. Its performance matches established methods on general datasets. The model excels at complex, directed graphs with dense, heterophilic structures. When applied to PPI prediction, the full ProtGram-DirectGCN framework delivers robust predictive power. This strong performance holds even with limited training data.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2510.01428.pdf' target='_blank'>https://arxiv.org/pdf/2510.01428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Huei Tsou, Michal Ozery-Flato, Ella Barkan, Diwakar Mahajan, Ben Shapira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01428">BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) and biomedical foundation models (BioFMs) have achieved strong results in biological text reasoning, molecular modeling, and single-cell analysis, yet they remain siloed in disjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE (Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage approach that adapts pretrained BioFMs as modality encoders and aligns them with LLMs through lightweight, modality-specific projection layers. The approach first aligns each modality to a shared LLM space through independently trained projections, allowing them to interoperate naturally, and then applies standard instruction tuning with multi-modal data to bring them together for downstream reasoning. By unifying raw biomedical data with knowledge embedded in LLMs, the approach enables zero-shot annotation, cross-modal question answering, and interactive, explainable dialogue. Across tasks spanning cell-type annotation, molecular description, and protein function reasoning, compact BIOVERSE configurations surpass larger LLM baselines while enabling richer, generative outputs than existing BioFMs, establishing a foundation for principled multi-modal biomedical reasoning.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2510.01184.pdf' target='_blank'>https://arxiv.org/pdf/2510.01184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Xu, Yu Wu, Sungjae Park, Zhizhuo Zhou, Shubham Tulsiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01184">Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a mechanism to steer the sampling diversity of denoising diffusion and flow matching models, allowing users to sample from a sharper or broader distribution than the training distribution. We build on the observation that these models leverage (learned) score functions of noisy data distributions for sampling and show that rescaling these allows one to effectively control a `local' sampling temperature. Notably, this approach does not require any finetuning or alterations to training strategy, and can be applied to any off-the-shelf model and is compatible with both deterministic and stochastic samplers. We first validate our framework on toy 2D data, and then demonstrate its application for diffusion models trained across five disparate tasks -- image generation, pose estimation, depth prediction, robot manipulation, and protein design. We find that across these tasks, our approach allows sampling from sharper (or flatter) distributions, yielding performance gains e.g., depth prediction models benefit from sampling more likely depth estimates, whereas image generation models perform better when sampling a slightly flatter distribution. Project page: https://temporalscorerescaling.github.io
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2509.24886.pdf' target='_blank'>https://arxiv.org/pdf/2509.24886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ya-Wei Eileen Lin, Ron Levie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24886">Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Canonicalization is a widely used strategy in equivariant machine learning, enforcing symmetry in neural networks by mapping each input to a standard form. Yet, it often introduces discontinuities that can affect stability during training, limit generalization, and complicate universal approximation theorems. In this paper, we address this by introducing \emph{adaptive canonicalization}, a general framework in which the canonicalization depends both on the input and the network. Specifically, we present the adaptive canonicalization based on prior maximization, where the standard form of the input is chosen to maximize the predictive confidence of the network. We prove that this construction yields continuous and symmetry-respecting models that admit universal approximation properties. We propose two applications of our setting: (i) resolving eigenbasis ambiguities in spectral graph neural networks, and (ii) handling rotational symmetries in point clouds. We empirically validate our methods on molecular and protein classification, as well as point cloud classification tasks. Our adaptive canonicalization outperforms the three other common solutions to equivariant machine learning: data augmentation, standard canonicalization, and equivariant architectures.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2509.13476.pdf' target='_blank'>https://arxiv.org/pdf/2509.13476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Masud Rana, Farjana Tasnim Mukta, Duc D. Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13476">A Geometric Graph-Based Deep Learning Model for Drug-Target Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In structure-based drug design, accurately estimating the binding affinity between a candidate ligand and its protein receptor is a central challenge. Recent advances in artificial intelligence, particularly deep learning, have demonstrated superior performance over traditional empirical and physics-based methods for this task, enabled by the growing availability of structural and experimental affinity data. In this work, we introduce DeepGGL, a deep convolutional neural network that integrates residual connections and an attention mechanism within a geometric graph learning framework. By leveraging multiscale weighted colored bipartite subgraphs, DeepGGL effectively captures fine-grained atom-level interactions in protein-ligand complexes across multiple scales. We benchmarked DeepGGL against established models on CASF-2013 and CASF-2016, where it achieved state-of-the-art performance with significant improvements across diverse evaluation metrics. To further assess robustness and generalization, we tested the model on the CSAR-NRC-HiQ dataset and the PDBbind v2019 holdout set. DeepGGL consistently maintained high predictive accuracy, highlighting its adaptability and reliability for binding affinity prediction in structure-based drug discovery.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2509.07983.pdf' target='_blank'>https://arxiv.org/pdf/2509.07983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long-Kai Huang, Rongyi Zhu, Bing He, Jianhua Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07983">Steering Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein Language Models (PLMs), pre-trained on extensive evolutionary data from natural proteins, have emerged as indispensable tools for protein design. While powerful, PLMs often struggle to produce proteins with precisely specified functionalities or properties due to inherent challenges in controlling their outputs. In this work, we investigate the potential of Activation Steering, a technique originally developed for controlling text generation in Large Language Models (LLMs), to direct PLMs toward generating protein sequences with targeted properties. We propose a simple yet effective method that employs activation editing to steer PLM outputs, and extend this approach to protein optimization through a novel editing site identification module. Through comprehensive experiments on lysozyme-like sequence generation and optimization, we demonstrate that our methods can be seamlessly integrated into both auto-encoding and autoregressive PLMs without requiring additional training. These results highlight a promising direction for precise protein engineering using foundation models.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2509.03885.pdf' target='_blank'>https://arxiv.org/pdf/2509.03885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Wang, Arian Jamasb, Mustafa Hajij, Alex Morehead, Luke Braithwaite, Pietro LiÃ²
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03885">Topotein: Topological Deep Learning for Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein representation learning (PRL) is crucial for understanding structure-function relationships, yet current sequence- and graph-based methods fail to capture the hierarchical organization inherent in protein structures. We introduce Topotein, a comprehensive framework that applies topological deep learning to PRL through the novel Protein Combinatorial Complex (PCC) and Topology-Complete Perceptron Network (TCPNet). Our PCC represents proteins at multiple hierarchical levels -- from residues to secondary structures to complete proteins -- while preserving geometric information at each level. TCPNet employs SE(3)-equivariant message passing across these hierarchical structures, enabling more effective capture of multi-scale structural patterns. Through extensive experiments on four PRL tasks, TCPNet consistently outperforms state-of-the-art geometric graph neural networks. Our approach demonstrates particular strength in tasks such as fold classification which require understanding of secondary structure arrangements, validating the importance of hierarchical topological features for protein analysis.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2509.03425.pdf' target='_blank'>https://arxiv.org/pdf/2509.03425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuc Pham, Viet Thanh Duy Nguyen, Truong-Son Hy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03425">LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate identification of interactions between protein residues and ligand functional groups is essential to understand molecular recognition and guide rational drug design. Existing deep learning approaches for protein-ligand interpretability often rely on 3D structural input or use distance-based contact labels, limiting both their applicability and biological relevance. We introduce LINKER, the first sequence-based model to predict residue-functional group interactions in terms of biologically defined interaction types, using only protein sequences and the ligand SMILES as input. LINKER is trained with structure-supervised attention, where interaction labels are derived from 3D protein-ligand complexes via functional group-based motif extraction. By abstracting ligand structures into functional groups, the model focuses on chemically meaningful substructures while predicting interaction types rather than mere spatial proximity. Crucially, LINKER requires only sequence-level input at inference time, enabling large-scale application in settings where structural data is unavailable. Experiments on the LP-PDBBind benchmark demonstrate that structure-informed supervision over functional group abstractions yields interaction predictions closely aligned with ground-truth biochemical annotations.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2509.02196.pdf' target='_blank'>https://arxiv.org/pdf/2509.02196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Sengar, Jiying Zhang, Pierre Vandergheynst, Patrick Barth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02196">Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating the long-timescale dynamics of biomolecules is a central challenge in computational science. While enhanced sampling methods can accelerate these simulations, they rely on pre-defined collective variables that are often difficult to identify. A recent generative model, LD-FPG, demonstrated that this problem could be bypassed by learning to sample the static equilibrium ensemble as all-atom deformations from a reference structure, establishing a powerful method for all-atom ensemble generation. However, while this approach successfully captures a system's probable conformations, it does not model the temporal evolution between them. We introduce the Graph Latent Dynamics Propagator (GLDP), a modular component for simulating dynamics within the learned latent space of LD-FPG. We then compare three classes of propagators: (i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and (iii) autoregressive neural networks. Within a unified encoder-propagator-decoder framework, we evaluate long-horizon stability, backbone and side-chain ensemble fidelity, and functional free-energy landscapes. Autoregressive neural networks deliver the most robust long rollouts; score-guided Langevin best recovers side-chain thermodynamics when the score is well learned; and Koopman provides an interpretable, lightweight baseline that tends to damp fluctuations. These results clarify the trade-offs among propagators and offer practical guidance for latent-space simulators of all-atom protein dynamics.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2508.10775.pdf' target='_blank'>https://arxiv.org/pdf/2508.10775.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Xu, Zhangfan Yang, Jenna Xinyi Yao, Shuangbao Song, Zexuan Zhu, Junkai Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10775">IBEX: Information-Bottleneck-EXplored Coarse-to-Fine Molecular Generation under Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional generative models increasingly drive structure-based drug discovery, yet it remains constrained by the scarce publicly available protein-ligand complexes. Under such data scarcity, almost all existing pipelines struggle to learn transferable geometric priors and consequently overfit to training-set biases. As such, we present IBEX, an Information-Bottleneck-EXplored coarse-to-fine pipeline to tackle the chronic shortage of protein-ligand complex data in structure-based drug design. Specifically, we use PAC-Bayesian information-bottleneck theory to quantify the information density of each sample. This analysis reveals how different masking strategies affect generalization and indicates that, compared with conventional de novo generation, the constrained Scaffold Hopping task endows the model with greater effective capacity and improved transfer performance. IBEX retains the original TargetDiff architecture and hyperparameters for training to generate molecules compatible with the binding pocket; it then applies an L-BFGS optimization step to finely refine each conformation by optimizing five physics-based terms and adjusting six translational and rotational degrees of freedom in under one second. With only these modifications, IBEX raises the zero-shot docking success rate on CBGBench CrossDocked2020-based from 53% to 64%, improves the mean Vina score from $-7.41 kcal mol^{-1}$ to $-8.07 kcal mol^{-1}$, and achieves the best median Vina energy in 57 of 100 pockets versus 3 for the original TargetDiff. IBEX also increases the QED by 25%, achieves state-of-the-art validity and diversity, and markedly reduces extrapolation error.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2507.20426.pdf' target='_blank'>https://arxiv.org/pdf/2507.20426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samiul Based Shuvo, Tasnia Binte Mamun, U Rajendra Acharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20426">ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that combines a residual learning-based encoder with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly from raw protein sequences. Our architecture incorporates dilated convolutions within residual blocks to mitigate vanishing gradient issues and extract rich sequence features, while capsule layers with dynamic routing capture hierarchical and spatial relationships within the learned feature space. We conducted comprehensive ablation studies comparing global and local embeddings from ProteinBERT and conventional one-hot encoding. Results show that ProteinBERT embeddings substantially outperform other representations on large datasets. Although one-hot encoding showed marginal advantages on smaller datasets, such as PDB186, it struggled to scale effectively. Extensive evaluations on four pairs of publicly available benchmark datasets demonstrate that our model consistently outperforms current state-of-the-art methods. It achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2% and 83.3%, while maintaining competitive performance on larger datasets such as PDB20000. Notably, the model maintains a well balanced sensitivity and specificity across datasets. These results demonstrate the efficacy and generalizability of integrating global protein representations with advanced deep learning architectures for reliable and scalable DBP prediction in diverse genomic contexts.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2507.20130.pdf' target='_blank'>https://arxiv.org/pdf/2507.20130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi He, Ailun Wang, Zhi Wang, Yu Liu, Xingyuan Xu, Wen Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20130">Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative models, particularly diffusion and auto-regressive models, have revolutionized fields like computer vision and natural language processing. However, their application to structure-based drug design (SBDD) remains limited due to critical data constraints. To address the limitation of training data for models targeting SBDD tasks, we propose an evolutionary framework named MEVO, which bridges the gap between billion-scale small molecule dataset and the scarce protein-ligand complex dataset, and effectively increase the abundance of training data for generative SBDD models. MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule representation in latent space, a diffusion model for pharmacophore-guided molecule generation, and a pocket-aware evolutionary strategy for molecule optimization with physics-based scoring function. This framework efficiently generate high-affinity binders for various protein targets, validated with predicted binding affinities using free energy perturbation (FEP) methods. In addition, we showcase the capability of MEVO in designing potent inhibitors to KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with similar affinity to the known highly active inhibitor evaluated by FEP calculations. With high versatility and generalizability, MEVO offers an effective and data-efficient model for various tasks in structure-based ligand design.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2507.07201.pdf' target='_blank'>https://arxiv.org/pdf/2507.07201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Xu, Zhangfan Yang, Sisi Yuan, Jenna Xinyi Yao, Jiangqiang Li, Junkai Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07201">MODA: A Unified 3D Diffusion Framework for Multi-Task Target-Aware Molecular Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional molecular generators based on diffusion models can now reach near-crystallographic accuracy, yet they remain fragmented across tasks. SMILES-only inputs, two-stage pretrain-finetune pipelines, and one-task-one-model practices hinder stereochemical fidelity, task alignment, and zero-shot transfer. We introduce MODA, a diffusion framework that unifies fragment growing, linker design, scaffold hopping, and side-chain decoration with a Bayesian mask scheduler. During training, a contiguous spatial fragment is masked and then denoised in one pass, enabling the model to learn shared geometric and chemical priors across tasks. Multi-task training yields a universal backbone that surpasses six diffusion baselines and three training paradigms on substructure, chemical property, interaction, and geometry. Model-C reduces ligand-protein clashes and substructure divergences while maintaining Lipinski compliance, whereas Model-B preserves similarity but trails in novelty and binding affinity. Zero-shot de novo design and lead-optimisation tests confirm stable negative Vina scores and high improvement rates without force-field refinement. These results demonstrate that a single-stage multi-task diffusion routine can replace two-stage workflows for structure-based molecular design.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2506.17064.pdf' target='_blank'>https://arxiv.org/pdf/2506.17064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Sengar, Ali Hariri, Daniel Probst, Patrick Barth, Pierre Vandergheynst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17064">Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2506.14793.pdf' target='_blank'>https://arxiv.org/pdf/2506.14793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Ravuri, Neil D. Lawrence
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14793">Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein Language Models (PLMs) such as ESM2 have been shown to be capable of zero-shot prediction of critical scalar properties of proteins (fitness). In this work, we show that injecting a dropout layer at inference time between a PLM's featurizer/embedding layer and its transformer, and averaging its output akin to Monte-Carlo dropout increases zero-shot performance on a subset of the ProteinGym dataset. This is the case even when the model was not trained with dropouts to begin with, and does not require retraining or finetuning of the PLM. A dropout of 0.1 seems performant across all models.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2506.14488.pdf' target='_blank'>https://arxiv.org/pdf/2506.14488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Xu, Zhangfan Yang, Ka-chun Wong, Zexuan Zhu, Jiangqiang Li, Junkai Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14488">Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2505.11023.pdf' target='_blank'>https://arxiv.org/pdf/2505.11023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>KutalmÄ±Å CoÅkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan LÃ¼dtke, Martin Becker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11023">Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2504.16941.pdf' target='_blank'>https://arxiv.org/pdf/2504.16941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria Lamine, Abdelatif Hafid, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16941">Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study presents a novel mathematical model derived from cohomology, leveraging the KEEL-proven theorem that establishes cohomology as tautological, generated by boundary classes of curves with fixed dual graphs. Simplicial complexes are constructed using skew-commutative graded algebra, and the structure theorem is applied to connect distinct homologies, enabling precise interpretations of the resulting geometric forms. The proposed model is utilized for protein structure analysis and prediction, with a specific application to the Flagellar Motor structure. This approach offers new insights into the geometric and algebraic foundations of biological macromolecular modeling, highlighting its potential for advancement in structural biology.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2504.02698.pdf' target='_blank'>https://arxiv.org/pdf/2504.02698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengrui XU, Tianchi Lu, Zikun Wang, Jixiu Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02698">SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interaction (PPI) prediction plays a pivotal role in deciphering cellular functions and disease mechanisms. To address the limitations of traditional experimental methods and existing computational approaches in cross-modal feature fusion and false-negative suppression, we propose SCMPPI-a novel supervised contrastive multimodal framework. By effectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with network topology (Node2Vec embeddings) and incorporating an enhanced contrastive learning strategy with negative sample filtering, SCMPPI achieves superior prediction performance. Extensive experiments on eight benchmark datasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%), along with excellent cross-species generalization (AUC>99%). Successful applications in CD9 networks, Wnt pathway analysis, and cancer-specific networks further highlight its potential for disease target discovery, establishing SCMPPI as a powerful tool for multimodal biological data analysis.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2503.18551.pdf' target='_blank'>https://arxiv.org/pdf/2503.18551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eoin Quinn, Ghassene Jebali, Maxime Seince, Oliver Bent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18551">Discriminative protein sequence modelling with Latent Space Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore a framework for protein sequence representation learning that decomposes the task between manifold learning and distributional modelling. Specifically we present a Latent Space Diffusion architecture which combines a protein sequence autoencoder with a denoising diffusion model operating on its latent space. We obtain a one-parameter family of learned representations from the diffusion model, along with the autoencoder's latent representation. We propose and evaluate two autoencoder architectures: a homogeneous model forcing amino acids of the same type to be identically distributed in the latent space, and an inhomogeneous model employing a noise-based variant of masking. As a baseline we take a latent space learned by masked language modelling, and evaluate discriminative capability on a range of protein property prediction tasks. Our finding is twofold: the diffusion models trained on both our proposed variants display higher discriminative power than the one trained on the masked language model baseline, none of the diffusion representations achieve the performance of the masked language model embeddings themselves.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2503.16659.pdf' target='_blank'>https://arxiv.org/pdf/2503.16659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet Thanh Duy Nguyen, Truong-Son Hy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16659">Advances in Protein Representation Learning: Methods, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are complex biomolecules that play a central role in various biological processes, making them critical targets for breakthroughs in molecular biology, medical research, and drug discovery. Deciphering their intricate, hierarchical structures, and diverse functions is essential for advancing our understanding of life at the molecular level. Protein Representation Learning (PRL) has emerged as a transformative approach, enabling the extraction of meaningful computational representations from protein data to address these challenges. In this paper, we provide a comprehensive review of PRL research, categorizing methodologies into five key areas: feature-based, sequence-based, structure-based, multimodal, and complex-based approaches. To support researchers in this rapidly evolving field, we introduce widely used databases for protein sequences, structures, and functions, which serve as essential resources for model development and evaluation. We also explore the diverse applications of these approaches in multiple domains, demonstrating their broad impact. Finally, we discuss pressing technical challenges and outline future directions to advance PRL, offering insights to inspire continued innovation in this foundational field.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2503.04680.pdf' target='_blank'>https://arxiv.org/pdf/2503.04680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Barron, Maksim E. Eren, Duc P. Truong, Cynthia Matuszek, James Wendelberger, Mary F. Dorn, Boian Alexandrov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04680">Matrix Factorization for Inferring Associations and Missing Links</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Missing link prediction is a method for network analysis, with applications in recommender systems, biology, social sciences, cybersecurity, information retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs. Missing link prediction identifies unseen but potentially existing connections in a network by analyzing the observed patterns and relationships. In proliferation detection, this supports efforts to identify and characterize attempts by state and non-state actors to acquire nuclear weapons or associated technology - a notoriously challenging but vital mission for global security. Dimensionality reduction techniques like Non-Negative Matrix Factorization (NMF) and Logistic Matrix Factorization (LMF) are effective but require selection of the matrix rank parameter, that is, of the number of hidden features, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk), Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along with ensemble variants incorporating logistic factorization, for link prediction. Our methods integrate automatic model determination for rank estimation by evaluating stability and accuracy using a modified bootstrap methodology and uncertainty quantification (UQ), assessing prediction reliability under random perturbations. We incorporate Otsu threshold selection and k-means clustering for Boolean matrix factorization, comparing them to coordinate descent-based Boolean thresholding. Our experiments highlight the impact of rank k selection, evaluate model performance under varying test-set sizes, and demonstrate the benefits of UQ for reliable predictions using abstention. We validate our methods on three synthetic datasets (Boolean and uniformly distributed) and benchmark them against LMF and symmetric LMF (symLMF) on five real-world protein-protein interaction networks, showcasing an improved prediction performance.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2502.15855.pdf' target='_blank'>https://arxiv.org/pdf/2502.15855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dengdeng Huang, Shikui Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15855">THFlow: A Temporally Hierarchical Flow Matching Framework for 3D Peptide Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models provide a promising approach to de novo 3D peptide design. Most of them jointly model the distributions of peptide's position, orientation, and conformation, attempting to simultaneously converge to the target pocket. However, in the early stage of docking, optimizing conformation-only modalities such as rotation and torsion can be physically meaningless, as the peptide is initialized far from the protein pocket and no interaction field is present. We define this problem as the multimodal temporal inconsistency problem and claim it is a key factor contributing to low binding affinity in generated peptides. To address this challenge, we propose THFlow, a novel flow matching-based multimodal generative model that explicitly models the temporal hierarchy between peptide position and conformation. It employs a polynomial based conditional flow to accelerate positional convergence early on, and later aligns it with rotation and torsion for coordinated conformation refinement under the emerging interaction field. Additionally, we incorporate interaction-related features, such as polarity, to further enhance the model's understanding of peptide-protein binding. Extensive experiments demonstrate that THFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering an effective and accurate solution for advancing peptide-based therapeutic development.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2502.00001.pdf' target='_blank'>https://arxiv.org/pdf/2502.00001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rownak Hossain Chowdhury, Mostafizur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00001">Accelerating PageRank Algorithmic Tasks with a new Programmable Hardware Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing the growing demands of artificial intelligence (AI) and data analytics requires new computing approaches. In this paper, we propose a reconfigurable hardware accelerator designed specifically for AI and data-intensive applications. Our architecture features a messaging-based intelligent computing scheme that allows for dynamic programming at runtime using a minimal instruction set. To assess our hardware's effectiveness, we conducted a case study in TSMC 28nm technology node. The simulation-based study involved analyzing a protein network using the computationally demanding PageRank algorithm. The results demonstrate that our hardware can analyze a 5,000-node protein network in just 213.6 milliseconds over 100 iterations. These outcomes signify the potential of our design to achieve cutting-edge performance in next-generation AI applications.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2501.01811.pdf' target='_blank'>https://arxiv.org/pdf/2501.01811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesc SabanÃ©s Zariquiey, Stephen E. Farr, Stefan Doerr, Gianni De Fabritiis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01811">QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations Using Neural Network Potentials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding affinities is crucial in drug discovery, particularly during hit-to-lead and lead optimization phases, however, limitations in ligand force fields continue to impact prediction accuracy. In this work, we validate relative binding free energy (RBFE) accuracy using neural network potentials (NNPs) for the ligands. We utilize a novel NNP model, AceFF 1.0, based on the TensorNet architecture for small molecules that broadens the applicability to diverse drug-like compounds, including all important chemical elements and supporting charged molecules. Using established benchmarks, we show overall improved accuracy and correlation in binding affinity predictions compared with GAFF2 for molecular mechanics and ANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4. We also show that we can run the NNP simulations at 2 fs timestep, at least two times larger than previous NNP models, providing significant speed gains. The results show promise for further evolutions of free energy calculations using NNPs while demonstrating its practical use already with the current generation. The code and NNP model are publicly available for research use.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2412.17799.pdf' target='_blank'>https://arxiv.org/pdf/2412.17799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip Isola, David Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17799">Automating the Search for Artificial Life with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2411.11265.pdf' target='_blank'>https://arxiv.org/pdf/2411.11265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh V. T. Tran, Nhat Khang Ngo, Viet Anh Nguyen, Truong Son Hy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11265">GROOT: Effective Design of Biological Sequences with Limited Experimental Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent space optimization (LSO) is a powerful method for designing discrete, high-dimensional biological sequences that maximize expensive black-box functions, such as wet lab experiments. This is accomplished by learning a latent space from available data and using a surrogate model to guide optimization algorithms toward optimal outputs. However, existing methods struggle when labeled data is limited, as training the surrogate model with few labeled data points can lead to subpar outputs, offering no advantage over the training data itself. We address this challenge by introducing GROOT, a Graph-based Latent Smoothing for Biological Sequence Optimization. In particular, GROOT generates pseudo-labels for neighbors sampled around the training latent embeddings. These pseudo-labels are then refined and smoothed by Label Propagation. Additionally, we theoretically and empirically justify our approach, demonstrate GROOT's ability to extrapolate to regions beyond the training set while maintaining reliability within an upper bound of their expected distances from the training regions. We evaluate GROOT on various biological sequence design tasks, including protein optimization (GFP and AAV) and three tasks with exact oracles from Design-Bench. The results demonstrate that GROOT equalizes and surpasses existing methods without requiring access to black-box oracles or vast amounts of labeled data, highlighting its practicality and effectiveness. We release our code at https://anonymous.4open.science/r/GROOT-D554
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2407.13023.pdf' target='_blank'>https://arxiv.org/pdf/2407.13023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Abdi, Marko Djukanovic, Hesam Tahmasebi Boldaji, Hadis Salehi, Aleksandar Kartelj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13023">A Three-Stage Algorithm for the Closest String Problem on Artificial and Real Gene Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Closest String Problem is an NP-hard problem that aims to find a string that has the minimum distance from all sequences that belong to the given set of strings. Its applications can be found in coding theory, computational biology, and designing degenerated primers, among others. There are efficient exact algorithms that have reached high-quality solutions for binary sequences. However, there is still room for improvement concerning the quality of solutions over DNA and protein sequences. In this paper, we introduce a three-stage algorithm that comprises the following process: first, we apply a novel alphabet pruning method to reduce the search space for effectively finding promising search regions. Second, a variant of beam search to find a heuristic solution is employed. This method utilizes a newly developed guiding function based on an expected distance heuristic score of partial solutions. Last, we introduce a local search to improve the quality of the solution obtained from the beam search. Furthermore, due to the lack of real-world benchmarks, two real-world datasets are introduced to verify the robustness of the method. The extensive experimental results show that the proposed method outperforms the previous approaches from the literature.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2407.11942.pdf' target='_blank'>https://arxiv.org/pdf/2407.11942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Klarner, Tim G. J. Rudner, Garrett M. Morris, Charlotte M. Deane, Yee Whye Teh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11942">Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have the potential to accelerate key steps in the discovery of novel molecular therapeutics and materials. Diffusion models have recently emerged as a powerful approach, excelling at unconditional sample generation and, with data-driven guidance, conditional generation within their training domain. Reliably sampling from high-value regions beyond the training data, however, remains an open challenge -- with current methods predominantly focusing on modifying the diffusion process itself. In this paper, we develop context-guided diffusion (CGD), a simple plug-and-play method that leverages unlabeled data and smoothness constraints to improve the out-of-distribution generalization of guided diffusion models. We demonstrate that this approach leads to substantial performance gains across various settings, including continuous, discrete, and graph-structured diffusion processes with applications across drug discovery, materials science, and protein design.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2407.11057.pdf' target='_blank'>https://arxiv.org/pdf/2407.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungyeon Choi, Sangmin Seo, Sanghyun Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11057">SPIN: SE(3)-Invariant Physics Informed Network for Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding affinity is crucial for rapid and efficient drug development. Recently, the importance of predicting binding affinity has led to increased attention on research that models the three-dimensional structure of protein-ligand complexes using graph neural networks to predict binding affinity. However, traditional methods often fail to accurately model the complex's spatial information or rely solely on geometric features, neglecting the principles of protein-ligand binding. This can lead to overfitting, resulting in models that perform poorly on independent datasets and ultimately reducing their usefulness in real drug development. To address this issue, we propose SPIN, a model designed to achieve superior generalization by incorporating various inductive biases applicable to this task, beyond merely training on empirical data from datasets. For prediction, we defined two types of inductive biases: a geometric perspective that maintains consistent binding affinity predictions regardless of the complexs rotations and translations, and a physicochemical perspective that necessitates minimal binding free energy along their reaction coordinate for effective protein-ligand binding. These prior knowledge inputs enable the SPIN to outperform comparative models in benchmark sets such as CASF-2016 and CSAR HiQ. Furthermore, we demonstrated the practicality of our model through virtual screening experiments and validated the reliability and potential of our proposed model based on experiments assessing its interpretability.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2407.04493.pdf' target='_blank'>https://arxiv.org/pdf/2407.04493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinghua Yao, Yuangang Pan, Jing Li, Ivor Tsang, Xin Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04493">PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in the realm of deep generative models focus on generating samples that satisfy multiple desired properties. However, prevalent approaches optimize these property functions independently, thus omitting the trade-offs among them. In addition, the property optimization is often improperly integrated into the generative models, resulting in an unnecessary compromise on generation quality (i.e., the quality of generated samples). To address these issues, we formulate a constrained optimization problem. It seeks to optimize generation quality while ensuring that generated samples reside at the Pareto front of multiple property objectives. Such a formulation enables the generation of samples that cannot be further improved simultaneously on the conflicting property functions and preserves good quality of generated samples. Building upon this formulation, we introduce the PaRetO-gUided Diffusion model (PROUD), wherein the gradients in the denoising process are dynamically adjusted to enhance generation quality while the generated samples adhere to Pareto optimality. Experimental evaluations on image generation and protein generation tasks demonstrate that our PROUD consistently maintains superior generation quality while approaching Pareto optimality across multiple property functions compared to various baselines.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2406.10867.pdf' target='_blank'>https://arxiv.org/pdf/2406.10867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Grayson Lee, Tony Shen, Martin Ester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10867">Geometric-informed GFlowNets for Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of cost involved with drug discovery and current speed of which they are discover, underscore the need for more efficient structure-based drug design (SBDD) methods. We employ Generative Flow Networks (GFlowNets), to effectively explore the vast combinatorial space of drug-like molecules, which traditional virtual screening methods fail to cover. We introduce a novel modification to the GFlowNet framework by incorporating trigonometrically consistent embeddings, previously utilized in tasks involving protein conformation and protein-ligand interactions, to enhance the model's ability to generate molecules tailored to specific protein pockets. We have modified the existing protein conditioning used by GFlowNets, blending geometric information from both protein and ligand embeddings to achieve more geometrically consistent embeddings. Experiments conducted using CrossDocked2020 demonstrated an improvement in the binding affinity between generated molecules and protein pockets for both single and multi-objective tasks, compared to previous work. Additionally, we propose future work aimed at further increasing the geometric information captured in protein-ligand interactions.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2406.01781.pdf' target='_blank'>https://arxiv.org/pdf/2406.01781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01781">DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2403.12636.pdf' target='_blank'>https://arxiv.org/pdf/2403.12636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Bischoff, Alana Darcher, Michael Deistler, Richard Gao, Franziska Gerken, Manuel Gloeckler, Lisa Haxel, Jaivardhan Kapoor, Janne K Lappalainen, Jakob H Macke, Guy Moss, Matthijs Pals, Felix Pei, Rachel Rapp, A Erdem SaÄtekin, Cornelius SchrÃ¶der, Auguste Schulz, Zinovia Stefanidi, Shoji Toyota, Linda Ulmer, Julius Vetter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12636">A Practical Guide to Sample-based Statistical Distances for Evaluating Generative Models in Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular sample-based statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (FrÃ©chet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision-making and a model generating medical images. We showcase that distinct distances can give different results on similar data. Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2402.05140.pdf' target='_blank'>https://arxiv.org/pdf/2402.05140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, Nicolo Fusi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05140">Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2402.01542.pdf' target='_blank'>https://arxiv.org/pdf/2402.01542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soojung Yang, Juno Nam, Johannes C. B. Dietschreit, Rafael GÃ³mez-Bombarelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01542">Learning Collective Variables with Synthetic Data Augmentation through Physics-Inspired Geodesic Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In molecular dynamics simulations, rare events, such as protein folding, are typically studied using enhanced sampling techniques, most of which are based on the definition of a collective variable (CV) along which acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. This new data can be used to improve the accuracy of classifier-based methods. Alternatively, a regression-based learning scheme for CV models can be adopted by leveraging the interpolation progress parameter.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2512.03750.pdf' target='_blank'>https://arxiv.org/pdf/2512.03750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sathya Edamadaka, Soojung Yang, Ju Li, Rafael Gómez-Bombarelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03750">Universally Converging Representations of Matter Across Scientific Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2511.15067.pdf' target='_blank'>https://arxiv.org/pdf/2511.15067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zisong Wang, Xuanyu Wang, Hang Chen, Haizhou Wang, Yuxin Chen, Yihang Xu, Yunhe Yuan, Lihuan Luo, Xitong Ling, Xiaoping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15067">Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2511.01795.pdf' target='_blank'>https://arxiv.org/pdf/2511.01795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Nobis, Maximilian Springenberg, Arina Belova, Rembert Daems, Christoph Knochenhauer, Manfred Opper, Tolga Birdal, Wojciech Samek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01795">Fractional Diffusion Bridge Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schrödinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_α$ atomic positions in protein structure prediction and lower Fréchet Inception Distance (FID) in unpaired image translation.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2510.24670.pdf' target='_blank'>https://arxiv.org/pdf/2510.24670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24670">Pearl: A Foundation Model for Placing Every Atom in the Right Location</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting the three-dimensional structures of protein-ligand complexes remains a fundamental challenge in computational drug discovery that limits the pace and success of therapeutic design. Deep learning methods have recently shown strong potential as structural prediction tools, achieving promising accuracy across diverse biomolecular systems. However, their performance and utility are constrained by scarce experimental data, inefficient architectures, physically invalid poses, and the limited ability to exploit auxiliary information available at inference. To address these issues, we introduce Pearl (Placing Every Atom in the Right Location), a foundation model for protein-ligand cofolding at scale. Pearl addresses these challenges with three key innovations: (1) training recipes that include large-scale synthetic data to overcome data scarcity; (2) architectures that incorporate an SO(3)-equivariant diffusion module to inherently respect 3D rotational symmetries, improving generalization and sample efficiency, and (3) controllable inference, including a generalized multi-chain templating system supporting both protein and non-polymeric components as well as dual unconditional/conditional modes. Pearl establishes a new state-of-the-art performance in protein-ligand cofolding. On the key metric of generating accurate (RMSD < 2 Å) and physically valid poses, Pearl surpasses AlphaFold 3 and other open source baselines on the public Runs N' Poses and PoseBusters benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the next best model. In the pocket-conditional cofolding regime, Pearl delivers $3.6\times$ improvement on a proprietary set of challenging, real-world drug targets at the more rigorous RMSD < 1 Å threshold. Finally, we demonstrate that model performance correlates directly with synthetic dataset size used in training.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2510.18870.pdf' target='_blank'>https://arxiv.org/pdf/2510.18870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeffrey Ouyang-Zhang, Pranav Murugan, Daniel J. Diaz, Gianluca Scarpellini, Richard Strong Bowen, Nate Gruver, Adam Klivans, Philipp Krähenbühl, Aleksandra Faust, Maruan Al-Shedivat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18870">Triangle Multiplication Is All You Need For Biomolecular Structure Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AlphaFold has transformed protein structure prediction, but emerging applications such as virtual ligand screening, proteome-wide folding, and de novo binder design demand predictions at a massive scale, where runtime and memory costs become prohibitive. A major bottleneck lies in the Pairformer backbone of AlphaFold3-style models, which relies on computationally expensive triangular primitives-especially triangle attention-for pairwise reasoning. We introduce Pairmixer, a streamlined alternative that eliminates triangle attention while preserving higher-order geometric reasoning capabilities that are critical for structure prediction. Pairmixer substantially improves computational efficiency, matching state-of-the-art structure predictors across folding and docking benchmarks, delivering up to 4x faster inference on long sequences while reducing training cost by 34%. Its efficiency alleviates the computational burden of downstream applications such as modeling large protein complexes, high-throughput ligand and binder screening, and hallucination-based design. Within BoltzDesign, for example, Pairmixer delivers over 2x faster sampling and scales to sequences ~30% longer than the memory limits of Pairformer.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2510.16897.pdf' target='_blank'>https://arxiv.org/pdf/2510.16897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jose Siguenza, Bharath Ramsundar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16897">DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks that incorporate geometric relationships respecting SE(3) group transformations (e.g. rotations and translations) are increasingly important in molecular applications, such as molecular property prediction, protein structure modeling, and materials design. These models, known as SE(3)-equivariant neural networks, ensure outputs transform predictably with input coordinate changes by explicitly encoding spatial atomic positions. Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful implementations, they often require substantial deep learning or mathematical prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13] with support for ready-to-use equivariant models, enabling scientists with minimal deep learning background to build, train, and evaluate models, such as SE(3)-Transformer and Tensor Field Networks. Our implementation includes equivariant models, complete training pipelines, and a toolkit of equivariant utilities, supported with comprehensive tests and documentation, to facilitate both application and further development of SE(3)-equivariant models.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2510.04233.pdf' target='_blank'>https://arxiv.org/pdf/2510.04233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yang, Yuqi Huang, Junheng Tao, Wanyu Wang, Qitian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04233">Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2510.02922.pdf' target='_blank'>https://arxiv.org/pdf/2510.02922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daphne Tsolissou, Theofanis Ganitidis, Konstantinos Mitsis, Stergios CHristodoulidis, Maria Vakalopoulou, Konstantina Nikita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02922">Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2509.25479.pdf' target='_blank'>https://arxiv.org/pdf/2509.25479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenfeng Deng, Ruijie Hou, Ningrui Xie, Mike Tyers, Michał Koziarski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25479">Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in structure-based protein design have accelerated de novo binder generation, yet interfaces on large domains or spanning multiple domains remain challenging due to high computational cost and declining success with increasing target size. We hypothesized that protein folding neural networks (PFNNs) operate in a ``local-first'' manner, prioritizing local interactions while displaying limited sensitivity to global foldability. Guided by this hypothesis, we propose an epitope-only strategy that retains only the discontinuous surface residues surrounding the binding site. Compared to intact-domain workflows, this approach improves in silico success rates by up to 80% and reduces the average time per successful design by up to forty-fold, enabling binder design against previously intractable targets such as ClpP and ALS3. Building on this foundation, we further developed a tailored pipeline that incorporates a Monte Carlo-based evolution step to overcome local minima and a position-specific biased inverse folding step to refine sequence patterns. Together, these advances not only establish a generalizable framework for efficient binder design against structurally large and otherwise inaccessible targets, but also support the broader ``local-first'' hypothesis as a guiding principle for PFNN-based design.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2509.19930.pdf' target='_blank'>https://arxiv.org/pdf/2509.19930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Tabish, Benedict Leimkuhler, Stefan Klus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19930">How deep is your network? Deep vs. shallow learning of transfer operators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a randomized neural network approach called RaNNDy for learning transfer operators and their spectral decompositions from data. The weights of the hidden layers of the neural network are randomly selected and only the output layer is trained. The main advantage is that without a noticeable reduction in accuracy, this approach significantly reduces the training time and resources while avoiding common problems associated with deep learning such as sensitivity to hyperparameters and slow convergence. Additionally, the proposed framework allows us to compute a closed-form solution for the output layer which directly represents the eigenfunctions of the operator. Moreover, it is possible to estimate uncertainties associated with the computed spectral properties via ensemble learning. We present results for different dynamical operators, including Koopman and Perron-Frobenius operators, which have important applications in analyzing the behavior of complex dynamical systems, and the SchrÃ¶dinger operator. The numerical examples, which highlight the strengths but also weaknesses of the proposed framework, include several stochastic dynamical systems, protein folding processes, and the quantum harmonic oscillator.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2509.11046.pdf' target='_blank'>https://arxiv.org/pdf/2509.11046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seon-Geun Jeong, Kyeong-Hwan Moon, Won-Joo Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11046">Hybrid Quantum Neural Networks for Efficient Protein-Ligand Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-ligand binding affinity is critical in drug discovery, but experimentally determining it is time-consuming and expensive. Artificial intelligence (AI) has been used to predict binding affinity, significantly accelerating this process. However, the high-performance requirements and vast datasets involved in affinity prediction demand increasingly large AI models, requiring substantial computational resources and training time. Quantum machine learning has emerged as a promising solution to these challenges. In particular, hybrid quantum-classical models can reduce the number of parameters while maintaining or improving performance compared to classical counterparts. Despite these advantages, challenges persist: why hybrid quantum models achieve these benefits, whether quantum neural networks (QNNs) can replace classical neural networks, and whether such models are feasible on noisy intermediate-scale quantum (NISQ) devices. This study addresses these challenges by proposing a hybrid quantum neural network (HQNN) that empirically demonstrates the capability to approximate non-linear functions in the latent feature space derived from classical embedding. The primary goal of this study is to achieve a parameter-efficient model in binding affinity prediction while ensuring feasibility on NISQ devices. Numerical results indicate that HQNN achieves comparable or superior performance and parameter efficiency compared to classical neural networks, underscoring its potential as a viable replacement. This study highlights the potential of hybrid QML in computational drug discovery, offering insights into its applicability and advantages in addressing the computational challenges of protein-ligand binding affinity prediction.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2509.07204.pdf' target='_blank'>https://arxiv.org/pdf/2509.07204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrien Couetoux, Thomas Devenyns, Lise Diagne, David Champagne, Pierre-Yves Mousset, Chris Anagnostopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07204">Predicting effect of novel treatments using molecular pathways and real-world data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in treating a particular disease prior to clinical testing or any real-world use has been challenging. In this paper, we propose a flexible and modular machine learning-based approach for predicting the efficacy of an untested pharmaceutical for treating a disease. We train a machine learning model using sets of pharmaceutical-pathway weight impact scores and patient data, which can include patient characteristics and observed clinical outcomes. The resulting model then analyses weighted impact scores of an untested pharmaceutical across human biological molecule-protein pathways to generate a predicted efficacy value. We demonstrate how the method works on a real-world dataset with patient treatments and outcomes, with two different weight impact score algorithms We include methods for evaluating the generalisation performance on unseen treatments, and to characterise conditions under which the approach can be expected to be most predictive. We discuss specific ways in which our approach can be iterated on, making it an initial framework to support future work on predicting the effect of untested drugs, leveraging RWD clinical data and drug embeddings.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2509.03084.pdf' target='_blank'>https://arxiv.org/pdf/2509.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Jones, Yue Yang, Felice C. Lightstone, Niema Moshiri, Jonathan E. Allen, Tajana S. Rosing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03084">SurGBSA: Learning Representations From Molecular Dynamics Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised pretraining from static structures of drug-like compounds and proteins enable powerful learned feature representations. Learned features demonstrate state of the art performance on a range of predictive tasks including molecular properties, structure generation, and protein-ligand interactions. The majority of approaches are limited by their use of static structures and it remains an open question, how best to use atomistic molecular dynamics (MD) simulations to develop more generalized models to improve prediction accuracy for novel molecular structures. We present SURrogate mmGBSA (SurGBSA) as a new modeling approach for MD-based representation learning, which learns a surrogate function of the Molecular Mechanics Generalized Born Surface Area (MMGBSA). We show for the first time the benefits of physics-informed pre-training to train a surrogate MMGBSA model on a collection of over 1.4 million 3D trajectories collected from MD simulations of the CASF-2016 benchmark. SurGBSA demonstrates a dramatic 27,927x speedup versus a traditional physics-based single-point MMGBSA calculation while nearly matching single-point MMGBSA accuracy on the challenging pose ranking problem for identification of the correct top pose (-0.4% difference). Our work advances the development of molecular foundation models by showing model improvements when training on MD simulations. Models, code and training data are made publicly available.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2507.13646.pdf' target='_blank'>https://arxiv.org/pdf/2507.13646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nimisha Ghosh, Daniele Santoni, Debaleena Nawn, Eleonora Ottaviani, Giovanni Felici
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13646">A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impact of Transformer-based language models has been unprecedented in Natural Language Processing (NLP). The success of such models has also led to their adoption in other fields including bioinformatics. Taking this into account, this paper discusses recent advances in Transformer-based models for protein sequence analysis and design. In this review, we have discussed and analysed a significant number of works pertaining to such applications. These applications encompass gene ontology, functional and structural protein identification, generation of de novo proteins and binding of proteins. We attempt to shed light on the strength and weaknesses of the discussed works to provide a comprehensive insight to readers. Finally, we highlight shortcomings in existing research and explore potential avenues for future developments. We believe that this review will help researchers working in this field to have an overall idea of the state of the art in this field, and to orient their future studies.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2507.11757.pdf' target='_blank'>https://arxiv.org/pdf/2507.11757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuehua Song, Yong Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11757">A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting drug-target interactions (DTIs) is pivotal for advancing drug discovery and target validation techniques. While machine learning approaches including those that are based on Graph Neural Networks (GNN) have achieved notable success in DTI prediction, many of them have difficulties in effectively integrating the diverse features of drugs, targets and their interactions. To address this limitation, we introduce a novel framework to take advantage of the power of both transductive learning and inductive learning so that features at molecular level and drug-target interaction network level can be exploited. Within this framework is a GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and target molecular structures as meta-nodes in a drug-target interaction graph, enabling a detailed exploration of their intricate relationships. To evaluate the proposed model, we have compiled a special benchmark comprising drug SMILES, protein sequences, and their interaction data, which is interesting in its own right. Our experimental results demonstrate that the GiG model significantly outperforms existing approaches across all evaluation metrics, highlighting the benefits of integrating different learning paradigms and interaction data.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2507.08966.pdf' target='_blank'>https://arxiv.org/pdf/2507.08966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Liu, Karl Leswing, Simon K. S. Chu, Farhad Ramezanghorbani, Griffin Young, Gabriel Marques, Prerna Das, Anjali Panikar, Esther Jamir, Mohammed Sulaiman Shamsudeen, K. Shawn Watts, Ananya Sen, Hari Priya Devannagari, Edward B. Miller, Muyun Lihan, Howook Hwang, Janet Paulsen, Xin Yu, Kyle Gion, Timur Rvachov, Emine Kucukbenli, Saee Gopal Paliwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08966">ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-ligand binding affinity prediction is essential for drug discovery and toxicity assessment. While machine learning (ML) promises fast and accurate predictions, its progress is constrained by the availability of reliable data. In contrast, physics-based methods such as absolute binding free energy perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive for high-throughput applications. To bridge this gap, we introduce ToxBench, the first large-scale AB-FEP dataset designed for ML development and focused on a single pharmaceutically critical target, Human Estrogen Receptor Alpha (ER$Î±$). ToxBench contains 8,770 ER$Î±$-ligand complex structures with binding free energies computed via AB-FEP with a subset validated against experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping ligand splits to assess model generalizability. Using ToxBench, we further benchmark state-of-the-art ML methods, and notably, our proposed DualBind model, which employs a dual-loss framework to effectively learn the binding energy function. The benchmark results demonstrate the superior performance of DualBind and the potential of ML to approximate AB-FEP at a fraction of the computational cost.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2506.06305.pdf' target='_blank'>https://arxiv.org/pdf/2506.06305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NoÃ©mie Bergues, Arthur CarrÃ©, Paul Join-Lambert, Brice Hoffmann, Arnaud Blondel, Hamza Tajmouati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06305">Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the 3D conformation of small molecules within protein binding sites is a key challenge in drug design. When a crystallized reference ligand (template) is available, it provides geometric priors that can guide 3D pose prediction. We present a two-stage method for ligand conformation generation guided by such templates. In the first stage, we introduce a molecular alignment approach based on flow-matching to generate 3D coordinates for the ligand, using the template structure as a reference. In the second stage, a differentiable pose optimization procedure refines this conformation based on shape and pharmacophore similarities, internal energy, and, optionally, the protein binding pocket. We evaluate our approach on a new benchmark of ligand pairs co-crystallized with the same target and show that it outperforms standard docking tools and open-access alignment methods, especially in cases involving low similarity to the template or high ligand flexibility.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2505.20098.pdf' target='_blank'>https://arxiv.org/pdf/2505.20098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowen Ling, Zhiqiang Li, Yanbin Wang, Zhuhong You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20098">Transformers in Protein: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As protein informatics advances rapidly, the demand for enhanced predictive accuracy, structural analysis, and functional understanding has intensified. Transformer models, as powerful deep learning architectures, have demonstrated unprecedented potential in addressing diverse challenges across protein research. However, a comprehensive review of Transformer applications in this field remains lacking. This paper bridges this gap by surveying over 100 studies, offering an in-depth analysis of practical implementations and research progress of Transformers in protein-related tasks. Our review systematically covers critical domains, including protein structure prediction, function prediction, protein-protein interaction analysis, functional annotation, and drug discovery/target identification. To contextualize these advancements across various protein domains, we adopt a domain-oriented classification system. We first introduce foundational concepts: the Transformer architecture and attention mechanisms, categorize Transformer variants tailored for protein science, and summarize essential protein knowledge. For each research domain, we outline its objectives and background, critically evaluate prior methods and their limitations, and highlight transformative contributions enabled by Transformer models. We also curate and summarize pivotal datasets and open-source code resources to facilitate reproducibility and benchmarking. Finally, we discuss persistent challenges in applying Transformers to protein informatics and propose future research directions. This review aims to provide a consolidated foundation for the synergistic integration of Transformer and protein informatics, fostering further innovation and expanded applications in the field.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2504.19017.pdf' target='_blank'>https://arxiv.org/pdf/2504.19017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Ghafarollahi, Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19017">Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence (AI) promise autonomous discovery, yet most systems still resurface knowledge latent in their training data. We present Sparks, a multi-modal multi-agent AI model that executes the entire discovery cycle that includes hypothesis generation, experiment design and iterative refinement to develop generalizable principles and a report without human intervention. Applied to protein science, Sparks uncovered two previously unknown phenomena: (i) a length-dependent mechanical crossover whereby beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond ~80 residues, establishing a new design principle for peptide mechanics; and (ii) a chain-length/secondary-structure stability map revealing unexpectedly robust beta-sheet-rich architectures and a "frustration zone" of high variance in mixed alpha/beta folds. These findings emerged from fully self-directed reasoning cycles that combined generative sequence design, high-accuracy structure prediction and physics-aware property models, with paired generation-and-reflection agents enforcing self-correction and reproducibility. The key result is that Sparks can independently conduct rigorous scientific inquiry and identify previously unknown scientific principles.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2504.16261.pdf' target='_blank'>https://arxiv.org/pdf/2504.16261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krinos Li, Xianglu Xiao, Zijun Zhong, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16261">Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2503.19186.pdf' target='_blank'>https://arxiv.org/pdf/2503.19186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parisa Mollaei, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19186">Protein Structure-Function Relationship: A Kernel-PCA Approach for Reaction Coordinate Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a Kernel-PCA model designed to capture structure-function relationships in a protein. This model also enables ranking of reaction coordinates according to their impact on protein properties. By leveraging machine learning techniques, including Kernel and principal component analysis (PCA), our model uncovers meaningful patterns in high-dimensional protein data obtained from molecular dynamics (MD) simulations. The effectiveness of our model in accurately identifying reaction coordinates has been demonstrated through its application to a G protein-coupled receptor. Furthermore, this model utilizes a network-based approach to uncover correlations in the dynamic behavior of residues associated with a specific protein property. These findings underscore the potential of our model as a powerful tool for protein structure-function analysis and visualization.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2502.19794.pdf' target='_blank'>https://arxiv.org/pdf/2502.19794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanliu Fan, Ziqiang Cao, Zicheng Ma, Nan Yu, Yimin Peng, Jun Zhang, Yiqin Gao, Guohong Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19794">ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-oriented de novo molecule design, namely generating molecules with specific property or substructure constraints, is a crucial yet challenging task in drug discovery. Existing methods, such as Bayesian optimization and reinforcement learning, often require training multiple property predictors and struggle to incorporate substructure constraints. Inspired by the success of Large Language Models (LLMs) in text generation, we propose ChatMol, a novel approach that leverages LLMs for molecule design across diverse constraint settings. Initially, we crafted a molecule representation compatible with LLMs and validated its efficacy across multiple online LLMs. Afterwards, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction. Finally, to address the limitations of LLMs in numerical recognition, we referred to the position encoding method and incorporated additional encoding for numerical values within the prompt. Experimental results across single-property, substructure-property, and multi-property constrained tasks demonstrate that ChatMol consistently outperforms state-of-the-art baselines, including VAE and RL-based methods. Notably, in multi-objective binding affinity maximization task, ChatMol achieves a significantly lower KD value of 0.25 for the protein target ESR1, while maintaining the highest overall performance, surpassing previous methods by 4.76%. Meanwhile, with numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules increased by up to 0.49. These findings highlight the potential of LLMs as a versatile framework for molecule generation, offering a promising alternative to traditional latent space and RL-based approaches.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2502.06846.pdf' target='_blank'>https://arxiv.org/pdf/2502.06846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicong Wang, Zicheng Ma, Ziqiang Cao, Changlong Zhou, Jun Zhang, Yiqin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06846">Prot2Chat: Protein LLM with Early-Fusion of Text, Sequence and Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: Proteins are of great significance in living organisms. However, understanding their functions encounters numerous challenges, such as insufficient integration of multimodal information, a large number of training parameters, limited flexibility of classification-based methods, and the lack of systematic evaluation metrics for protein Q&A systems. To tackle these issues, we propose the Prot2Chat framework. Results: We modified ProteinMPNN to encode protein sequence and structural information in a unified way. We used a large language model (LLM) to encode questions into vectors and developed a protein-text adapter to compress protein information into virtual tokens based on these vectors, achieving the early fusion of text and protein information. Finally, the same LLM reads the virtual tokens and the questions to generate answers. To optimize training efficiency, we froze the encoder and employed Low-Rank Adaptation (LoRA) techniques for the LLM. Experiments on two datasets show that both automated metrics and expert evaluations demonstrate the superior performance of our model, and zero-shot prediction results highlight its generalization ability. The models and codes are available at https://github.com/ wangzc1233/Prot2Chat. Contact: zqcao@suda.edu.cn or wangzc025@163.com Key words: Protein Q&A, Early-Fusion, LLM
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2412.15256.pdf' target='_blank'>https://arxiv.org/pdf/2412.15256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Kim, Manil Shrestha, Richard Foty, Tom DeLay, Vicki Seyfert-Margolis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15256">Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creation and curation of knowledge graphs can accelerate disease discovery and analysis in real-world data. While disease ontologies aid in biological data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture patient condition nuances or rare diseases. Multiple disease definitions across data sources complicate ontology mapping and disease clustering. We propose creating patient knowledge graphs using large language model extraction techniques, allowing data extraction via natural language rather than rigid ontological hierarchies. Our method maps to existing ontologies (MeSH, SNOMED-CT, RxNORM, HPO) to ground extracted entities.
  Using a large ambulatory care EHR database with 33.6M patients, we demonstrate our method through the patient search for Dravet syndrome, which received ICD10 recognition in October 2020. We describe our construction of patient-specific knowledge graphs and symptom-based patient searches. Using confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based entity extraction to characterize patients in grounded ontologies. We then apply this method to identify Beta-propeller protein-associated neurodegeneration (BPAN) patients, demonstrating real-world discovery where no ground truth exists.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2412.04069.pdf' target='_blank'>https://arxiv.org/pdf/2412.04069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Yu Guo, Yi-Fan Li, Yuan Liu, Xiaoyong Pan, Hong-Bin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04069">ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein design has become a critical method in advancing significant potential for various applications such as drug development and enzyme engineering. However, protein design methods utilizing large language models with solely pretraining and fine-tuning struggle to capture relationships in multi-modal protein data. To address this, we propose ProtDAT, a de novo fine-grained framework capable of designing proteins from any descriptive protein text input. ProtDAT builds upon the inherent characteristics of protein data to unify sequences and text as a cohesive whole rather than separate entities. It leverages an innovative multi-modal cross-attention, integrating protein sequences and textual information for a foundational level and seamless integration. Experimental results demonstrate that ProtDAT achieves the state-of-the-art performance in protein sequence generation, excelling in rationality, functionality, structural similarity, and validity. On 20,000 text-sequence pairs from Swiss-Prot, it improves pLDDT by 6%, TM-score by 0.26, and reduces RMSD by 1.2 Ã, highlighting its potential to advance protein design.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2411.14157.pdf' target='_blank'>https://arxiv.org/pdf/2411.14157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahsa Sheikholeslami, Navid Mazrouei, Yousof Gheisari, Afshin Fasihi, Matin Irajpour, Ali Motahharynia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14157">DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional drug design faces significant challenges due to inherent chemical and biological complexities, often resulting in high failure rates in clinical trials. Deep learning advancements, particularly generative models, offer potential solutions to these challenges. One promising algorithm is DrugGPT, a transformer-based model, that generates small molecules for input protein sequences. Although promising, it generates both chemically valid and invalid structures and does not incorporate the features of approved drugs, resulting in time-consuming and inefficient drug discovery. To address these issues, we introduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is fine-tuned on approved drug-target interactions and optimized with proximal policy optimization. By giving reward feedback from protein-ligand binding affinity prediction using pre-trained transformers (PLAPT) and a customized invalid structure assessor, DrugGen significantly improves performance. Evaluation across multiple targets demonstrated that DrugGen achieves 100% valid structure generation compared to 95.5% with DrugGPT and produced molecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared to DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking simulations further validate its ability to generate molecules targeting binding sites effectively. For example, in the case of fatty acid-binding protein 5 (FABP5), DrugGen generated molecules with superior docking scores (FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule (Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows potential for drug repositioning and creating novel pharmacophores for existing targets. By producing high-quality small molecules, DrugGen provides a high-performance medium for advancing pharmaceutical research and drug discovery.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2411.06090.pdf' target='_blank'>https://arxiv.org/pdf/2411.06090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aya Abdelsalam Ismail, Tuomas Oikarinen, Amy Wang, Julius Adebayo, Samuel Stanton, Taylor Joren, Joseph Kleinhenz, Allen Goodman, HÃ©ctor Corrada Bravo, Kyunghyun Cho, Nathan C. Frey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06090">Concept Bottleneck Language Models For protein design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Concept Bottleneck Protein Language Models (CB-pLM), a generative masked language model with a layer where each neuron corresponds to an interpretable concept. Our architecture offers three key benefits: i) Control: We can intervene on concept values to precisely control the properties of generated proteins, achieving a 3 times larger change in desired concept values compared to baselines. ii) Interpretability: A linear mapping between concept values and predicted tokens allows transparent analysis of the model's decision-making process. iii) Debugging: This transparency facilitates easy debugging of trained models. Our models achieve pre-training perplexity and downstream task performance comparable to traditional masked protein language models, demonstrating that interpretability does not compromise performance. While adaptable to any language model, we focus on masked protein language models due to their importance in drug discovery and the ability to validate our model's capabilities through real-world experiments and expert knowledge. We scale our CB-pLM from 24 million to 3 billion parameters, making them the largest Concept Bottleneck Models trained and the first capable of generative language modeling.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2411.04775.pdf' target='_blank'>https://arxiv.org/pdf/2411.04775.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Tabish, Neil K. Chada, Stefan Klus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04775">Learning dynamical systems from data: Gradient-based dictionary optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Koopman operator plays a crucial role in analyzing the global behavior of dynamical systems. Existing data-driven methods for approximating the Koopman operator or discovering the governing equations of the underlying system typically require a fixed set of basis functions, also called dictionary. The optimal choice of basis functions is highly problem-dependent and often requires domain knowledge. We present a novel gradient descent-based optimization framework for learning suitable and interpretable basis functions from data and show how it can be used in combination with EDMD, SINDy, and PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's circuit, a nonlinear heat equation, as well as protein-folding data.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2411.03972.pdf' target='_blank'>https://arxiv.org/pdf/2411.03972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenning Liu, Xiantao Li, Chunhao Wang, Jin-Peng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03972">Toward end-to-end quantum simulation for protein dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and simulating the protein folding process overall remains a grand challenge in computational biology. We systematically investigate end-to-end quantum algorithms for simulating various protein dynamics with effects, such as mechanical forces or stochastic noises. A major focus is the read-in of system settings for simulation, for which we discuss (i) efficient quantum algorithms to prepare initial states--whether for ensemble or single-state simulations, in particular, the first efficient procedure for preparing Gaussian pseudo-random amplitude states, and (ii) the first efficient loading of the connectivity matrices of the protein structure. For the read-out stage, our algorithms estimate a range of classical observables, including energy, low-frequency vibrational modes, density of states, displacement correlations, and optimal control parameters. Between these stages, we simulate the dynamic evolution of the protein system, by using normal mode models--such as Gaussian network models (GNM) and all-atom normal mode models. In addition, we conduct classical numerical experiments focused on accurately estimating the density of states and applying optimal control to facilitate conformational changes. These experiments serve to validate our claims regarding potential quantum speedups. Overall, our study demonstrates that quantum simulation of protein dynamics represents a robust, end-to-end application for both early-stage and fully fault-tolerant quantum computing.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2409.18582.pdf' target='_blank'>https://arxiv.org/pdf/2409.18582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Melis Ilayda Bal, Pier Giuseppe Sessa, Mojmir Mutny, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18582">Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian optimization (BO) is a powerful framework to optimize black-box expensive-to-evaluate functions via sequential interactions. In several important problems (e.g. drug discovery, circuit design, neural architecture search, etc.), though, such functions are defined over large $\textit{combinatorial and unstructured}$ spaces. This makes existing BO algorithms not feasible due to the intractable maximization of the acquisition function over these domains. To address this issue, we propose $\textbf{GameOpt}$, a novel game-theoretical approach to combinatorial BO. $\textbf{GameOpt}$ establishes a cooperative game between the different optimization variables, and selects points that are game $\textit{equilibria}$ of an upper confidence bound acquisition function. These are stable configurations from which no variable has an incentive to deviate$-$ analog to local optima in continuous domains. Crucially, this allows us to efficiently break down the complexity of the combinatorial domain into individual decision sets, making $\textbf{GameOpt}$ scalable to large combinatorial spaces. We demonstrate the application of $\textbf{GameOpt}$ to the challenging $\textit{protein design}$ problem and validate its performance on four real-world protein datasets. Each protein can take up to $20^{X}$ possible configurations, where $X$ is the length of a protein, making standard BO methods infeasible. Instead, our approach iteratively selects informative protein configurations and very quickly discovers highly active protein variants compared to other baselines.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2409.17726.pdf' target='_blank'>https://arxiv.org/pdf/2409.17726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luiz Felipe Vecchietti, Minji Lee, Begench Hangeldiyev, Hyunkyu Jung, Hahnbeom Park, Tae-Kyun Kim, Meeyoung Cha, Ho Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17726">Recent advances in interpretable machine learning using structure-based protein representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in machine learning (ML) are transforming the field of structural biology. For example, AlphaFold, a groundbreaking neural network for protein structure prediction, has been widely adopted by researchers. The availability of easy-to-use interfaces and interpretable outcomes from the neural network architecture, such as the confidence scores used to color the predicted structures, have made AlphaFold accessible even to non-ML experts. In this paper, we present various methods for representing protein 3D structures from low- to high-resolution, and show how interpretable ML methods can support tasks such as predicting protein structures, protein function, and protein-protein interactions. This survey also emphasizes the significance of interpreting and visualizing ML-based inference for structure-based protein representations that enhance interpretability and knowledge discovery. Developing such interpretable approaches promises to further accelerate fields including drug development and protein design.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2409.10579.pdf' target='_blank'>https://arxiv.org/pdf/2409.10579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghao Yan, Yunbi Nam, Lingyao Li, Rebecca A. Deek, Hongzhe Li, Siyuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10579">Recent advances in deep learning and language models for studying the microbiome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in deep learning, particularly large language models (LLMs), made a significant impact on how researchers study microbiome and metagenomics data. Microbial protein and genomic sequences, like natural languages, form a language of life, enabling the adoption of LLMs to extract useful insights from complex microbial ecologies. In this paper, we review applications of deep learning and language models in analyzing microbiome and metagenomics data. We focus on problem formulations, necessary datasets, and the integration of language modeling techniques. We provide an extensive overview of protein/genomic language modeling and their contributions to microbiome studies. We also discuss applications such as novel viromics language modeling, biosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2408.16068.pdf' target='_blank'>https://arxiv.org/pdf/2408.16068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huili Zheng, Qimin Zhang, Yiru Gong, Zheyan Liu, Shaohan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16068">Identification of Prognostic Biomarkers for Stage III Non-Small Cell Lung Carcinoma in Female Nonsmokers Using Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung cancer remains a leading cause of cancer-related deaths globally, with non-small cell lung cancer (NSCLC) being the most common subtype. This study aimed to identify key biomarkers associated with stage III NSCLC in non-smoking females using gene expression profiling from the GDS3837 dataset. Utilizing XGBoost, a machine learning algorithm, the analysis achieved a strong predictive performance with an AUC score of 0.835. The top biomarkers identified - CCAAT enhancer binding protein alpha (C/EBP-alpha), lactate dehydrogenase A4 (LDHA), UNC-45 myosin chaperone B (UNC-45B), checkpoint kinase 1 (CHK1), and hypoxia-inducible factor 1 subunit alpha (HIF-1-alpha) - have been validated in the literature as being significantly linked to lung cancer. These findings highlight the potential of these biomarkers for early diagnosis and personalized therapy, emphasizing the value of integrating machine learning with molecular profiling in cancer research.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2408.06391.pdf' target='_blank'>https://arxiv.org/pdf/2408.06391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingyi Rong, Wenzhuo Zheng, Bozitao Zhong, Zhouhan Lin, Liang Hong, Ning Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06391">Autoregressive Enzyme Function Prediction with Multi-scale Multi-modality Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of enzyme function is crucial for elucidating biological mechanisms and driving innovation across various sectors. Existing deep learning methods tend to rely solely on either sequence data or structural data and predict the EC number as a whole, neglecting the intrinsic hierarchical structure of EC numbers. To address these limitations, we introduce MAPred, a novel multi-modality and multi-scale model designed to autoregressively predict the EC number of proteins. MAPred integrates both the primary amino acid sequence and the 3D tokens of proteins, employing a dual-pathway approach to capture comprehensive protein characteristics and essential local functional sites. Additionally, MAPred utilizes an autoregressive prediction network to sequentially predict the digits of the EC number, leveraging the hierarchical organization of EC classifications. Evaluations on benchmark datasets, including New-392, Price, and New-815, demonstrate that our method outperforms existing models, marking a significant advance in the reliability and granularity of protein function prediction within bioinformatics.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2405.18986.pdf' target='_blank'>https://arxiv.org/pdf/2405.18986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minji Lee, Luiz Felipe Vecchietti, Hyunkyu Jung, Hyun Joo Ro, Meeyoung Cha, Ho Min Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18986">Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are complex molecules responsible for different functions in nature. Enhancing the functionality of proteins and cellular fitness can significantly impact various industries. However, protein optimization using computational methods remains challenging, especially when starting from low-fitness sequences. We propose LatProtRL, an optimization method to efficiently traverse a latent space learned by an encoder-decoder leveraging a large protein language model. To escape local optima, our optimization is modeled as a Markov decision process using reinforcement learning acting directly in latent space. We evaluate our approach on two important fitness optimization tasks, demonstrating its ability to achieve comparable or superior fitness over baseline methods. Our findings and in vitro evaluation show that the generated sequences can reach high-fitness regions, suggesting a substantial potential of LatProtRL in lab-in-the-loop scenarios.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2405.18075.pdf' target='_blank'>https://arxiv.org/pdf/2405.18075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NataÅ¡a Tagasovska, Vladimir GligorijeviÄ, Kyunghyun Cho, Andreas Loukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18075">Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, PropEn, inspired by ``matching'', which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2405.12961.pdf' target='_blank'>https://arxiv.org/pdf/2405.12961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12961">Aligning Transformers with Continuous Feedback via Energy Rank Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the "alignment" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers and protein language models to generate molecules and protein sequences, respectively, with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2405.06658.pdf' target='_blank'>https://arxiv.org/pdf/2405.06658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqing Shen, Outongyi Lv, Houying Zhu, Yu Guang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06658">ProteinEngine: Empower LLM with Domain Knowledge for Protein Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have garnered considerable attention for their proficiency in tackling intricate tasks, particularly leveraging their capacities for zero-shot and in-context learning. However, their utility has been predominantly restricted to general tasks due to an absence of domain-specific knowledge. This constraint becomes particularly pertinent in the realm of protein engineering, where specialized expertise is required for tasks such as protein function prediction, protein evolution analysis, and protein design, with a level of specialization that existing LLMs cannot furnish. In response to this challenge, we introduce \textsc{ProteinEngine}, a human-centered platform aimed at amplifying the capabilities of LLMs in protein engineering by seamlessly integrating a comprehensive range of relevant tools, packages, and software via API calls. Uniquely, \textsc{ProteinEngine} assigns three distinct roles to LLMs, facilitating efficient task delegation, specialized task resolution, and effective communication of results. This design fosters high extensibility and promotes the smooth incorporation of new algorithms, models, and features for future development. Extensive user studies, involving participants from both the AI and protein engineering communities across academia and industry, consistently validate the superiority of \textsc{ProteinEngine} in augmenting the reliability and precision of deep learning in protein engineering tasks. Consequently, our findings highlight the potential of \textsc{ProteinEngine} to bride the disconnected tools for future research in the protein engineering domain.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2405.05167.pdf' target='_blank'>https://arxiv.org/pdf/2405.05167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vanni Doffini, O. Anatole von Lilienfeld, Michael A. Nash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05167">Data-Error Scaling in Machine Learning on Natural Discrete Combinatorial Mutation-prone Sets: Case Studies on Peptides and Small Molecules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules. We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data. Our synthetic datasets comprise i) two naÃ¯ve functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs. In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data. We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set. We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots. Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling. This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2402.19095.pdf' target='_blank'>https://arxiv.org/pdf/2402.19095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He, Haotian Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19095">A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neural Networks (CCN) and a supervised Transformer protein language model for single-sequence protein structure prediction. The training features of the two are combined to predict the protein Transformer binding site matrix, and then the three-dimensional structure is reconstructed using energy minimization.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2402.01744.pdf' target='_blank'>https://arxiv.org/pdf/2402.01744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salvatore Contino, Paolo Sortino, Maria Rita Gulotta, Ugo Perricone, Roberto Pirrone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01744">Unveiling Molecular Moieties through Hierarchical Grad-CAM Graph Explainability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: Virtual Screening (VS) has become an essential tool in drug discovery, enabling the rapid and cost-effective identification of potential bioactive molecules. Among recent advancements, Graph Neural Networks (GNNs) have gained prominence for their ability to model complex molecular structures using graph-based representations. However, the integration of explainable methods to elucidate the specific contributions of molecular substructures to biological activity remains a significant challenge. This limitation hampers both the interpretability of predictive models and the rational design of novel therapeutics. Results: We trained 20 GNN models on a dataset of small molecules with the goal of predicting their activity on 20 distinct protein targets from the Kinase family. These classifiers achieved state-of-the-art performance in virtual screening tasks, demonstrating high accuracy and robustness on different targets. Building upon these models, we implemented the Hierarchical Grad-CAM graph Explainer (HGE) framework, enabling an in-depth analysis of the molecular moieties driving protein-ligand binding stabilization. HGE exploits Grad-CAM explanations at the atom, ring, and whole-molecule levels, leveraging the message-passing mechanism to highlight the most relevant chemical moieties. Validation against experimental data from the literature confirmed the ability of the explainer to recognize a molecular pattern of drugs and correctly annotate them to the known target. Conclusion: Our approach may represent a valid support to shorten both the screening and the hit discovery process. Detailed knowledge of the molecular substructures that play a role in the binding process can help the computational chemist to gain insights into the structure optimization, as well as in drug repurposing tasks.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2512.24810.pdf' target='_blank'>https://arxiv.org/pdf/2512.24810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bence Bolgár, András Millinghoffer, Péter Antal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24810">DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise probabilistic information about drug-target interaction (DTI) predictions is vital for understanding limitations and boosting predictive performance. Gaussian processes (GP) offer a scalable framework to integrate state-of-the-art DTI representations and Bayesian inference, enabling novel operations, such as Bayesian classification with rejection, top-$K$ selection, and ranking. We propose a deep kernel learning-based GP architecture (DTI-GP), which incorporates a combined neural embedding module for chemical compounds and protein targets, and a GP module. The workflow continues with sampling from the predictive distribution to estimate a Bayesian precedence matrix, which is used in fast and accurate selection and ranking operations. DTI-GP outperforms state-of-the-art solutions, and it allows (1) the construction of a Bayesian accuracy-confidence enrichment score, (2) rejection schemes for improved enrichment, and (3) estimation and search for top-$K$ selections and ranking with high expected utility.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2512.18454.pdf' target='_blank'>https://arxiv.org/pdf/2512.18454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Graber, Victor Armegioiu, Rebecca Buller, Siddhartha Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18454">Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2512.17534.pdf' target='_blank'>https://arxiv.org/pdf/2512.17534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17534">HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2512.00696.pdf' target='_blank'>https://arxiv.org/pdf/2512.00696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasi Hays, Yue Yu, William Richardson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00696">Hierarchical Molecular Language Models (HMLMs)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cellular signaling networks represent complex information processing systems that have been modeled via traditional mathematical or statistical approaches. However, these methods often struggle to capture context-dependent signaling, pathway cross-talk, and temporal dynamics across multiple biological scales. Here, we introduce hierarchical molecular language models (HMLMs), a novel architecture that proposes a molecular network-specific large language model (LLM) to use in intracellular communication as a specialized molecular language, which includes molecules as tokens, protein interactions, post-translational modifications, and regulatory events modeled as semantic relationships within an adapted transformer architecture. HMLMs employ graph-structured attention mechanisms to accommodate signaling network topology while integrating information across the molecular, pathway, and cellular scales through hierarchical attention patterns. We demonstrate HMLM superiority using a cardiac fibroblast signaling network comprising over 100 molecular species across functional modules connected by regulatory edges. HMLM achieved a mean squared error (MSE) of 0.058 for temporal signaling predictions, representing 30% improvement over graph neural networks (GNNs: 0.083) and 52% improvement over ordinary differential equation models (ODEs: 0.121), with particular advantages under sparse temporal sampling conditions where HMLM maintained MSE = 0.041 with only 4 time-points. The HMLMs offer a foundation for AI-driven biology and medicine with predictable scaling characteristics suitable for interactive applications. By bridging molecular mechanisms with cellular phenotypes through AI-driven molecular language representation, HMLMs provide a powerful paradigm for systems biology that advances precision medicine applications and therapeutic discovery in the era of AI.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2511.21900.pdf' target='_blank'>https://arxiv.org/pdf/2511.21900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patricia Suriana, Joshua A. Rackers, Ewa M. Nowara, Pedro O. Pinheiro, John M. Nicoloudis, Vishnu Sresht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21900">Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2511.17249.pdf' target='_blank'>https://arxiv.org/pdf/2511.17249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Tedoldi, Ola Engkvist, Patrick Bryant, Hossein Azizpour, Jon Paul Janet, Alessandro Tibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17249">FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2511.10165.pdf' target='_blank'>https://arxiv.org/pdf/2511.10165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuancheng Sun, Yuxuan Ren, Zhaoming Chen, Xu Han, Kang Liu, Qiwei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10165">EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2510.22520.pdf' target='_blank'>https://arxiv.org/pdf/2510.22520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ito, Danai Koutra, Jenna Wiens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22520">Random Search Neural Networks for Efficient and Expressive Graph Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Random walk neural networks (RWNNs) have emerged as a promising approach for graph representation learning, leveraging recent advances in sequence models to process random walks. However, under realistic sampling constraints, RWNNs often fail to capture global structure even in small graphs due to incomplete node and edge coverage, limiting their expressivity. To address this, we propose \textit{random search neural networks} (RSNNs), which operate on random searches, each of which guarantees full node coverage. Theoretically, we demonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed to achieve full edge coverage, substantially reducing sampling complexity compared to the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graph size). Furthermore, when paired with universal sequence models, RSNNs are universal approximators. We lastly show RSNNs are probabilistically invariant to graph isomorphisms, ensuring their expectation is an isomorphism-invariant graph function. Empirically, RSNNs consistently outperform RWNNs on molecular and protein benchmarks, achieving comparable or superior performance with up to 16$\times$ fewer sampled sequences. Our work bridges theoretical and practical advances in random walk based approaches, offering an efficient and expressive framework for learning on sparse graphs.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2510.16612.pdf' target='_blank'>https://arxiv.org/pdf/2510.16612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eli N. Weinstein, Andrei Slabodkin, Mattia G. Gollub, Elizabeth B. Wood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16612">Accelerated Learning on Large Scale Screens using Generative Library Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biological machine learning is often bottlenecked by a lack of scaled data. One promising route to relieving data bottlenecks is through high throughput screens, which can experimentally test the activity of $10^6-10^{12}$ protein sequences in parallel. In this article, we introduce algorithms to optimize high throughput screens for data creation and model training. We focus on the large scale regime, where dataset sizes are limited by the cost of measurement and sequencing. We show that when active sequences are rare, we maximize information gain if we only collect positive examples of active sequences, i.e. $x$ with $y>0$. We can correct for the missing negative examples using a generative model of the library, producing a consistent and efficient estimate of the true $p(y | x)$. We demonstrate this approach in simulation and on a large scale screen of antibodies. Overall, co-design of experiments and inference lets us accelerate learning dramatically.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2510.15601.pdf' target='_blank'>https://arxiv.org/pdf/2510.15601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Glaser, Steffanie Paul, Alissa M. Hummer, Charlotte M. Deane, Debora S. Marks, Alan N. Amin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15601">Kernel-Based Evaluation of Conditional Biological Sequence Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy between the true conditional distribution and the model's estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model's temperature hyperparameter to achieve a better fit.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2510.15233.pdf' target='_blank'>https://arxiv.org/pdf/2510.15233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitesh Badkul, Lei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15233">Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable, informative, and individual uncertainty quantification (UQ) remains missing in current ML community. This hinders the effective application of AI/ML to risk-sensitive domains. Most methods either fail to provide coverage on new data, inflate intervals so broadly that they are not actionable, or assign uncertainties that do not track actual error, especially under a distribution shift. In high-stakes drug discovery, protein-ligand affinity (PLI) prediction is especially challenging as assay noise is heterogeneous, chemical space is imbalanced and large, and practical evaluations routinely involve distribution shift. In this work, we introduce a novel uncertainty quantification method, Trustworthy Expert Split-conformal with Scaled Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides per-sample uncertainty with reliable coverage guarantee, informative and adaptive prediction interval widths that track the absolute error. We evaluate on protein-ligand binding affinity prediction under both independent and identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD) splits, comparing against strong UQ baselines. TESSERA attains near-nominal coverage and the best coverage-width trade-off as measured by the Coverage-Width Criterion (CWC), while maintaining competitive adaptivity (lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage (SSC) further confirms that intervals are right-sized, indicating width increases when data are scarce or noisy, and remain tight when predictions are reliable. By unifying Mixture of Expert (MoE) diversity with conformal calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties that are well-suited to selective prediction and downstream decision-making in the drug-discovery pipeline and other applications.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2510.07337.pdf' target='_blank'>https://arxiv.org/pdf/2510.07337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shawnak Shivakumar, Jefferson Hernandez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07337">Decoding the dark proteome: Deep learning-enabled discovery of druggable enzymes in Wuchereria bancrofti</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wuchereria bancrofti, the parasitic roundworm responsible for lymphatic filariasis, permanently disables over 36 million people and places 657 million at risk across 39 countries. A major bottleneck for drug discovery is the lack of functional annotation for more than 90 percent of the W. bancrofti dark proteome, leaving many potential targets unidentified. In this work, we present a novel computational pipeline that converts W. bancrofti's unannotated amino acid sequence data into precise four-level Enzyme Commission (EC) numbers and drug candidates. We utilized a DEtection TRansformer to estimate the probability of enzymatic function, fine-tuned a hierarchical nearest neighbor EC predictor on 4,476 labeled parasite proteins, and applied rejection sampling to retain only four-level EC classifications at 100 percent confidence. This pipeline assigned precise EC numbers to 14,772 previously uncharacterized proteins and discovered 543 EC classes not previously known in W. bancrofti. A qualitative triage emphasizing parasite-specific targets, chemical tractability, biochemical importance, and biological plausibility prioritized six enzymes across five separate strategies: anti-Wolbachia cell-wall inhibition, proteolysis blockade, transmission disruption, purinergic immune interference, and cGMP-signaling destabilization. We curated a 43-compound library from ChEMBL and BindingDB and co-folded across multiple protein conformers with Boltz-2. All six targets exhibited at least moderately strong predicted binding affinities below 1 micromolar, with moenomycin analogs against peptidoglycan glycosyltransferase and NTPase inhibitors showing promising nanomolar hits and well-defined binding pockets. While experimental validation remains essential, our results provide the first large-scale functional map of the W. bancrofti dark proteome and accelerate early-stage drug development for the species.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2510.06290.pdf' target='_blank'>https://arxiv.org/pdf/2510.06290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Chen, Lijun Guo, Houli Fan, Wentao He, Rong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06290">Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying cancer driver genes (CDGs) is essential for understanding cancer mechanisms and developing targeted therapies. Graph neural networks (GNNs) have recently been employed to identify CDGs by capturing patterns in biological interaction networks. However, most GNN-based approaches rely on a single protein-protein interaction (PPI) network, ignoring complementary information from other biological networks. Some studies integrate multiple networks by aligning features with consistency constraints to learn unified gene representations for CDG identification. However, such representation-level fusion often assumes congruent gene relationships across networks, which may overlook network heterogeneity and introduce conflicting information. To address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a novel framework for CDG identification across multiple networks at the decision level. Instead of enforcing feature-level consistency, SEFGNN treats each biological network as an independent evidence source and performs uncertainty-aware fusion at the decision level using Dempster-Shafer Theory (DST). To alleviate the risk of overconfidence from DST, we further introduce a Soft Evidence Smoothing (SES) module that improves ranking stability while preserving discriminative performance. Experiments on three cancer datasets show that SEFGNN consistently outperforms state-of-the-art baselines and exhibits strong potential in discovering novel CDGs.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2510.04776.pdf' target='_blank'>https://arxiv.org/pdf/2510.04776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ebenezer Awotoro, Chisom Ezekannagha, Florian Schwarz, Johannes Tauscher, Dominik Heider, Katharina Ladewig, Christel Le Bon, Karine Moncoq, Bruno Miroux, Georges Hattab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04776">MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural biology has made significant progress in determining membrane proteins, leading to a remarkable increase in the number of available structures in dedicated databases. The inherent complexity of membrane protein structures, coupled with challenges such as missing data, inconsistencies, and computational barriers from disparate sources, underscores the need for improved database integration. To address this gap, we present MetaMP, a framework that unifies membrane-protein databases within a web application and uses machine learning for classification. MetaMP improves data quality by enriching metadata, offering a user-friendly interface, and providing eight interactive views for streamlined exploration. MetaMP was effective across tasks of varying difficulty, demonstrating advantages across different levels without compromising speed or accuracy, according to user evaluations. Moreover, MetaMP supports essential functions such as structure classification and outlier detection. We present three practical applications of Artificial Intelligence (AI) in membrane protein research: predicting transmembrane segments, reconciling legacy databases, and classifying structures with explainable AI support. In a validation focused on statistics, MetaMP resolved 77% of data discrepancies and accurately predicted the class of newly identified membrane proteins 98% of the time and overtook expert curation. Altogether, MetaMP is a much-needed resource that harmonizes current knowledge and empowers AI-driven exploration of membrane-protein architecture.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2510.02734.pdf' target='_blank'>https://arxiv.org/pdf/2510.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taehan Kim, Sangdae Nam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02734">SAE-RNA: A Sparse Autoencoder Model for Interpreting RNA Language Model Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning, particularly with the advancement of Large Language Models, has transformed biomolecular modeling, with protein advances (e.g., ESM) inspiring emerging RNA language models such as RiNALMo. Yet how and what these RNA Language Models internally encode about messenger RNA (mRNA) or non-coding RNA (ncRNA) families remains unclear. We present SAE- RNA, interpretability model that analyzes RiNALMo representations and maps them to known human-level biological features. Our work frames RNA interpretability as concept discovery in pretrained embeddings, without end-to-end retraining, and provides practical tools to probe what RNA LMs may encode about ncRNA families. The model can be extended to close comparisons between RNA groups, and supporting hypothesis generation about previously unrecognized relationships.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2510.01632.pdf' target='_blank'>https://arxiv.org/pdf/2510.01632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wang, Carlos Oliver
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01632">BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein function is driven by coherent substructures which vary in size and topology, yet current protein representation learning models (PRL) distort these signals by relying on rigid substructures such as k-hop and fixed radius neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable module that represents proteins by dynamically partitioning structures into flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs are quantized into a shared and interpretable codebook, yielding a discrete vocabulary of function-relevant protein substructures used to compute protein embeddings. We show that BioBlobs representations improve the performance of widely used protein encoders such as GVP-GNN across various PRL tasks. Our approach highlights the value of architectures that directly capture function-relevant protein substructures, enabling both improved predictive performance and mechanistic insight into protein function.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2509.24895.pdf' target='_blank'>https://arxiv.org/pdf/2509.24895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kosio Beshkov, Anders Malthe-SÃ¸renssen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24895">Towards Understanding the Shape of Representations in Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While protein language models (PLMs) are one of the most promising avenues of research for future de novo protein design, the way in which they transform sequences to hidden representations, as well as the information encoded in such representations is yet to be fully understood. Several works have attempted to propose interpretability tools for PLMs, but they have focused on understanding how individual sequences are transformed by such models. Therefore, the way in which PLMs transform the whole space of sequences along with their relations is still unknown. In this work we attempt to understand this transformed space of sequences by identifying protein structure and representation with square-root velocity (SRV) representations and graph filtrations. Both approaches naturally lead to a metric space in which pairs of proteins or protein representations can be compared with each other. We analyze different types of proteins from the SCOP dataset and show that the Karcher mean and effective dimension of the SRV shape space follow a non-linear pattern as a function of the layers in ESM2 models of different sizes. Furthermore, we use graph filtrations as a tool to study the context lengths at which models encode the structural features of proteins. We find that PLMs preferentially encode immediate as well as local relations between residues, but start to degrade for larger context lengths. The most structurally faithful encoding tends to occur close to, but before the last layer of the models, indicating that training a folding model ontop of these layers might lead to improved folding performance.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2509.22337.pdf' target='_blank'>https://arxiv.org/pdf/2509.22337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Feng, Xin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22337">GPU-Accelerated Loopy Belief Propagation for Program Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Loopy Belief Propagation (LBP) is a widely used approximate inference algorithm in probabilistic graphical models, with applications in computer vision, error correction codes, protein folding, program analysis, etc. However, LBP faces significant computational challenges when applied to large-scale program analysis. While GPU (Graphics Processing Unit) parallel computing provides a promising solution, existing approaches lack support for flexible update strategies and have yet to integrate logical constraints with GPU acceleration, leading to suboptimal practical performance. This paper presents a GPU-accelerated LBP algorithm for program analysis. To support the diverse update strategies required by users, we propose a unified representation for specifying arbitrary user-defined update strategies, along with a dependency analysis algorithm. Furthermore, building on previous work that leverages the local structure of Horn clauses to simplify message passing, we group messages to minimize warp divergence and better utilize GPU resources. Experimental results on datarace analysis over eight real-world Java programs show that our approach achieves an average speedup of $2.14\times$ over the state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art GPU-based approach, while maintaining high accuracy.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2509.20542.pdf' target='_blank'>https://arxiv.org/pdf/2509.20542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rujie Yin, Yang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20542">A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2509.17937.pdf' target='_blank'>https://arxiv.org/pdf/2509.17937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jayashrita Debnath, Gerhard Hummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17937">Random functions as data compressors for machine learning of molecular processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) is rapidly transforming the way molecular dynamics simulations are performed and analyzed, from materials modeling to studies of protein folding and function. ML algorithms are often employed to learn low-dimensional representations of conformational landscapes and to cluster trajectories into relevant metastable states. Most of these algorithms require selecting a small number of features that describe the problem of interest. Although deep neural networks can tackle large numbers of input features, the training costs increase with input size, which makes the selection of a subset of features mandatory for most problems of practical interest. Here, we show that random nonlinear projections can be used to compress large feature spaces and make computations faster without substantial loss of information. We describe an efficient way to produce random projections and then exemplify the general procedure for protein folding. For our test cases NTL9 and the double-norleucin variant of the villin headpiece, we find that random compression retains the core static and dynamic information of the original high dimensional feature space and makes trajectory analysis more robust.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2509.05541.pdf' target='_blank'>https://arxiv.org/pdf/2509.05541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Sanchez Espinosa, Erik H Thiede, Yunan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05541">Cryo-EM as a Stochastic Inverse Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of biomolecules, but structural heterogeneity remains a major challenge in 3D reconstruction. Traditional methods assume a discrete set of conformations, limiting their ability to recover continuous structural variability. In this work, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP) over probability measures, where the observed images are modeled as the push-forward of an unknown distribution over molecular structures via a random forward operator. We pose the reconstruction problem as the minimization of a variational discrepancy between observed and simulated image distributions, using statistical distances such as the KL divergence and the Maximum Mean Discrepancy. The resulting optimization is performed over the space of probability measures via a Wasserstein gradient flow, which we numerically solve using particles to represent and evolve conformational ensembles. We validate our approach using synthetic examples, including a realistic protein model, which demonstrates its ability to recover continuous distributions over structural states. We analyze the connection between our formulation and Maximum A Posteriori (MAP) approaches, which can be interpreted as instances of the discretize-then-optimize (DTO) framework. We further provide a consistency analysis, establishing conditions under which DTO methods, such as MAP estimation, converge to the solution of the underlying infinite-dimensional continuous problem. Beyond cryo-EM, the framework provides a general methodology for solving SIPs involving random forward operators.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2509.04998.pdf' target='_blank'>https://arxiv.org/pdf/2509.04998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MatouÅ¡ SoldÃ¡t, JiÅÃ­ KlÃ©ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04998">Directed Evolution of Proteins via Bayesian Optimization in Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directed evolution is an iterative laboratory process of designing proteins with improved function by iteratively synthesizing new protein variants and evaluating their desired property with expensive and time-consuming biochemical screening. Machine learning methods can help select informative or promising variants for screening to increase their quality and reduce the amount of necessary screening. In this paper, we present a novel method for machine-learning-assisted directed evolution of proteins which combines Bayesian optimization with informative representation of protein variants extracted from a pre-trained protein language model. We demonstrate that the new representation based on the sequence embeddings significantly improves the performance of Bayesian optimization yielding better results with the same number of conducted screening in total. At the same time, our method outperforms the state-of-the-art machine-learning-assisted directed evolution methods with regression objective.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2509.03351.pdf' target='_blank'>https://arxiv.org/pdf/2509.03351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Natalia Flechas Manrique, Alberto MartÃ­nez, Elena LÃ³pez-MartÃ­nez, Luc Andrea, RomÃ¡n Orus, Aitor Manteca, Aitziber L. Cortajarena, LlorenÃ§ Espinosa-PortalÃ©s
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03351">epiGPTope: A machine learning-based epitope generator and classifier</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Epitopes are short antigenic peptide sequences which are recognized by antibodies or immune cell receptors. These are central to the development of immunotherapies, vaccines, and diagnostics. However, the rational design of synthetic epitope libraries is challenging due to the large combinatorial sequence space, $20^n$ combinations for linear epitopes of n amino acids, making screening and testing unfeasible, even with high throughput experimental techniques. In this study, we present a large language model, epiGPTope, pre-trained on protein data and specifically fine-tuned on linear epitopes, which for the first time can directly generate novel epitope-like sequences, which are found to possess statistical properties analogous to the ones of known epitopes. This generative approach can be used to prepare libraries of epitope candidate sequences. We further train statistical classifiers to predict whether an epitope sequence is of bacterial or viral origin, thus narrowing the candidate library and increasing the likelihood of identifying specific epitopes. We propose that such combination of generative and predictive models can be of assistance in epitope discovery. The approach uses only primary amino acid sequences of linear epitopes, bypassing the need for a geometric framework or hand-crafted features of the sequences. By developing a method to create biologically feasible sequences, we anticipate faster and more cost-effective generation and screening of synthetic epitopes, with relevant applications in the development of new biotechnologies.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2509.02610.pdf' target='_blank'>https://arxiv.org/pdf/2509.02610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Feldman, Tal Feldman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02610">Resilient Biosecurity in the Era of AI-Enabled Bioweapons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative biology have enabled the design of novel proteins, creating significant opportunities for drug discovery while also introducing new risks, including the potential development of synthetic bioweapons. Existing biosafety measures primarily rely on inference-time filters such as sequence alignment and protein-protein interaction (PPI) prediction to detect dangerous outputs. In this study, we evaluate the performance of three leading PPI prediction tools: AlphaFold 3, AF3Complex, and SpatialPPIv2. These models were tested on well-characterized viral-host interactions, such as those involving Hepatitis B and SARS-CoV-2. Despite being trained on many of the same viruses, the models fail to detect a substantial number of known interactions. Strikingly, none of the tools successfully identify any of the four experimentally validated SARS-CoV-2 mutants with confirmed binding. These findings suggest that current predictive filters are inadequate for reliably flagging even known biological threats and are even more unlikely to detect novel ones. We argue for a shift toward response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and regulatory frameworks capable of operating at the speed of AI-driven developments.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2508.18949.pdf' target='_blank'>https://arxiv.org/pdf/2508.18949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyin Zhou, Christopher Iliffe Sprague, Vsevolod Viliuga, Matteo Tadiello, Arne Elofsson, Hossein Azizpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18949">Energy-Based Flow Matching for Generating 3D Molecular Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design. Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution. In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2508.09499.pdf' target='_blank'>https://arxiv.org/pdf/2508.09499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyan Jia, Chuan-Xian Ren, Hong Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09499">CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting the binding conformation of small-molecule ligands to protein targets is a critical step in rational drug design. Although recent deep learning-based docking surpasses traditional methods in speed and accuracy, many approaches rely on graph representations and language model-inspired encoders while neglecting critical geometric information, resulting in inaccurate pocket localization and unrealistic binding conformations. In this study, we introduce CWFBind, a weighted, fast, and accurate docking method based on local curvature features. Specifically, we integrate local curvature descriptors during the feature extraction phase to enrich the geometric representation of both proteins and ligands, complementing existing chemical, sequence, and structural features. Furthermore, we embed degree-aware weighting mechanisms into the message passing process, enhancing the model's ability to capture spatial structural distinctions and interaction strengths. To address the class imbalance challenge in pocket prediction, CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced loss function, facilitating more precise identification of binding regions and key residues. Comprehensive experimental evaluations demonstrate that CWFBind achieves competitive performance across multiple docking benchmarks, offering a balanced trade-off between accuracy and efficiency.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2508.04743.pdf' target='_blank'>https://arxiv.org/pdf/2508.04743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debanjan Konar, Neerav Sreekumar, Richard Jiang, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04743">Alz-QNet: A Quantum Regression Network for Studying Alzheimer's Gene Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the molecular-level mechanisms underpinning Alzheimer's disease (AD) by studying crucial genes associated with the disease remains a challenge. Alzheimer's, being a multifactorial disease, requires understanding the gene-gene interactions underlying it for theranostics and progress. In this article, a novel attempt has been made using a quantum regression to decode how some crucial genes in the AD Amyloid Beta Precursor Protein ($APP$), Sterol regulatory element binding transcription factor 14 ($FGF14$), Yin Yang 1 ($YY1$), and Phospholipase D Family Member 3 ($PLD3$) etc. become influenced by other prominent switching genes during disease progression, which may help in gene expression-based therapy for AD. Our proposed Quantum Regression Network (Alz-QNet) introduces a pioneering approach with insights from the state-of-the-art Quantum Gene Regulatory Networks (QGRN) to unravel the gene interactions involved in AD pathology, particularly within the Entorhinal Cortex (EC), where early pathological changes occur. Using the proposed Alz-QNet framework, we explore the interactions between key genes ($APP$, $FGF14$, $YY1$, $EGR1$, $GAS7$, $AKT3$, $SREBF2$, and $PLD3$) within the CE microenvironment of AD patients, studying genetic samples from the database $GSE138852$, all of which are believed to play a crucial role in the progression of AD. Our investigation uncovers intricate gene-gene interactions, shedding light on the potential regulatory mechanisms that underlie the pathogenesis of AD, which help us to find potential gene inhibitors or regulators for theranostics.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2507.21938.pdf' target='_blank'>https://arxiv.org/pdf/2507.21938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Abrudan, Sebastian Pujalte Ojeda, Chaitanya K. Joshi, Matthew Greenig, Felipe Engelberger, Alena Khmelinskaia, Jens Meiler, Michele Vendruscolo, Tuomas P. J. Knowles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21938">Multi-state Protein Design with DynamicMPNN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural biology has long been dominated by the one sequence, one structure, one function paradigm, yet many critical biological processes - from enzyme catalysis to membrane transport - depend on proteins that adopt multiple conformational states. Existing multi-state design approaches rely on post-hoc aggregation of single-state predictions, achieving poor experimental success rates compared to single-state design. We introduce DynamicMPNN, an inverse folding model explicitly trained to generate sequences compatible with multiple conformations through joint learning across conformational ensembles. Trained on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13% on structure-normalized RMSD across our challenging multi-state protein benchmark.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2507.18603.pdf' target='_blank'>https://arxiv.org/pdf/2507.18603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Ling, Yi Shi, Da Yan, Yang Zhou, Bo Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18603">Demystify Protein Generation with Hierarchical Conditional Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating novel and functional protein sequences is critical to a wide range of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering the biological function of a protein is determined by multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based and structure-based information for efficient end-to-end protein design guided by specified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and discriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with the benchmark datasets, and the results on conditional protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2507.18558.pdf' target='_blank'>https://arxiv.org/pdf/2507.18558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Feng, Chaitanya Pallerla, Xiaomin Lin, Pouya Sohrabipour, Philip Crandall, Wan Shou, Yu She, Dongyi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18558">Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The poultry industry has been driven by broiler chicken production and has grown into the world's largest animal protein sector. Automated detection of chicken carcasses on processing lines is vital for quality control, food safety, and operational efficiency in slaughterhouses and poultry processing plants. However, developing robust deep learning models for tasks like instance segmentation in these fast-paced industrial environments is often hampered by the need for laborious acquisition and annotation of large-scale real-world image datasets. We present the first pipeline generating photo-realistic, automatically labeled synthetic images of chicken carcasses. We also introduce a new benchmark dataset containing 300 annotated real-world images, curated specifically for poultry segmentation research. Using these datasets, this study investigates the efficacy of synthetic data and automatic data annotation to enhance the instance segmentation of chicken carcasses, particularly when real annotated data from the processing line is scarce. A small real dataset with varying proportions of synthetic images was evaluated in prominent instance segmentation models. Results show that synthetic data significantly boosts segmentation performance for chicken carcasses across all models. This research underscores the value of synthetic data augmentation as a viable and effective strategy to mitigate data scarcity, reduce manual annotation efforts, and advance the development of robust AI-driven automated detection systems for chicken carcasses in the poultry processing industry.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2507.12470.pdf' target='_blank'>https://arxiv.org/pdf/2507.12470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Xu, XiaoLong Shi, Xin Chen, Fang Wang, Sirui Li, Pali Ye, Boliang Zhang, Di Deng, Zheng Kou, Xiaoli Qiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12470">DNA Probe Computing System for Solving NP-Complete Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently solving NP-complete problems-such as protein structure prediction, cryptographic decryption, and vulnerability detection-remains a central challenge in computer science. Traditional electronic computers, constrained by the Turing machine's one-dimensional data processing and sequential operations, struggle to address these issues effectively. To overcome this bottleneck, computational models must adopt multidimensional data structures and parallel information processing mechanisms. Building on our team's proposed probe machine model (a non-Turing computational framework), this study develops a blocking probe technique that leverages DNA computing's inherent parallelism to identify all valid solutions for NP-complete problems in a single probe operation. Using the 27-vertex 3-coloring problem as a case study, we successfully retrieved all solutions through DNA molecular probe experiments. This breakthrough demonstrates the first implementation of a fully parallel computing system at the molecular level, offering a novel paradigm for tackling computational complexity. Our results indicate that the probe machine, with its parallel architecture and molecular implementation, transcends the limitations of classical models and holds promise for solving intricate real-world problems.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2507.11818.pdf' target='_blank'>https://arxiv.org/pdf/2507.11818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Rekesh, Miruna Cretu, Dmytro Shevchuk, Vignesh Ram Somnath, Pietro LiÃ², Robert A. Batey, Mike Tyers, MichaÅ Koziarski, Cheng-Hao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11818">SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring synthesizability in generative small molecule design remains a major challenge. While recent developments in synthesizable molecule generation have demonstrated promising results, these efforts have been largely confined to 2D molecular graph representations, limiting the ability to perform geometry-based conditional generation. In this work, we present SynCoGen (Synthesizable Co-Generation), a single framework that combines simultaneous masked graph diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen samples from the joint distribution of molecular building blocks, chemical reactions, and atomic coordinates. To train the model, we curated SynSpace, a dataset containing over 600K synthesis-aware building block graphs and 3.3M conformers. SynCoGen achieves state-of-the-art performance in unconditional small molecule graph and conformer generation, and the model delivers competitive performance in zero-shot molecular linker design for protein ligand generation in drug discovery. Overall, this multimodal formulation represents a foundation for future applications enabled by non-autoregressive molecular generation, including analog expansion, lead optimization, and direct structure conditioning.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2506.17963.pdf' target='_blank'>https://arxiv.org/pdf/2506.17963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Nie, Hongyu Zhang, Hao Jiang, Yutian Liu, Xiansong Huang, Fan Xu, Jie Fu, Zhixiang Ren, Yonghong Tian, Wen-Bin Zhang, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17963">OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and modeling enzyme-substrate interactions is crucial for catalytic mechanism research, enzyme engineering, and metabolic engineering. Although a large number of predictive methods have emerged, they do not incorporate prior knowledge of enzyme catalysis to rationally modulate general protein-molecule features that are misaligned with catalytic patterns. To address this issue, we introduce a two-stage progressive framework, OmniESI, for enzyme-substrate interaction prediction through conditional deep learning. By decomposing the modeling of enzyme-substrate interactions into a two-stage progressive process, OmniESI incorporates two conditional networks that respectively emphasize enzymatic reaction specificity and crucial catalysis-related interactions, facilitating a gradual feature modulation in the latent space from general protein-molecule domain to catalysis-aware domain. On top of this unified architecture, OmniESI can adapt to a variety of downstream tasks, including enzyme kinetic parameter prediction, enzyme-substrate pairing prediction, enzyme mutational effect prediction, and enzymatic active site annotation. Under the multi-perspective performance evaluation of in-distribution and out-of-distribution settings, OmniESI consistently delivered superior performance than state-of-the-art specialized methods across seven benchmarks. More importantly, the proposed conditional networks were shown to internalize the fundamental patterns of catalytic efficiency while significantly improving prediction performance, with only negligible parameter increases (0.16%), as demonstrated by ablation studies on key components. Overall, OmniESI represents a unified predictive approach for enzyme-substrate interactions, providing an effective tool for catalytic mechanism cracking and enzyme engineering with strong generalization and broad applicability.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2506.07035.pdf' target='_blank'>https://arxiv.org/pdf/2506.07035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Jiang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07035">AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deciphering protein function remains a fundamental challenge in protein representation learning. The task presents significant difficulties for protein language models (PLMs) due to the sheer volume of functional annotation categories and the highly imbalanced distribution of annotated instances across biological ontologies. Inspired by the remarkable success of reinforcement learning from human feedback (RLHF) in large language model (LLM) alignment, we propose AnnoDPO, a novel multi-modal framework for protein function prediction that leverages Direct Preference Optimization (DPO) to enhance annotation learning. Our methodology addresses the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives, establishing a new paradigm for biological knowledge integration in protein representation learning.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2506.00297.pdf' target='_blank'>https://arxiv.org/pdf/2506.00297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanglei Xue, Andrew Kubaney, Zhichun Guo, Joseph K. Min, Ge Liu, Yi Yang, David Baker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00297">Improving Protein Sequence Design through Designability Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2505.23354.pdf' target='_blank'>https://arxiv.org/pdf/2505.23354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meital Bojan, Sanketh Vedula, Advaith Maddipatla, Nadav Bojan Sellam, Federico Napoli, Paul Schanda, Alex M. Bronstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23354">Representing local protein environments with atomistic foundation models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The local structure of a protein strongly impacts its function and interactions with other molecules. Therefore, a concise, informative representation of a local protein environment is essential for modeling and designing proteins and biomolecular interactions. However, these environments' extensive structural and chemical variability makes them challenging to model, and such representations remain under-explored. In this work, we propose a novel representation for a local protein environment derived from the intermediate features of atomistic foundation models (AFMs). We demonstrate that this embedding effectively captures both local structure (e.g., secondary motifs), and chemical features (e.g., amino-acid identity and protonation state). We further show that the AFM-derived representation space exhibits meaningful structure, enabling the construction of data-driven priors over the distribution of biomolecular environments. Finally, in the context of biomolecular NMR spectroscopy, we demonstrate that the proposed representations enable a first-of-its-kind physics-informed chemical shift predictor that achieves state-of-the-art accuracy. Our results demonstrate the surprising effectiveness of atomistic foundation models and their emergent representations for protein modeling beyond traditional molecular simulations. We believe this will open new lines of work in constructing effective functional representations for protein environments.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2505.22494.pdf' target='_blank'>https://arxiv.org/pdf/2505.22494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22494">ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein sequences of both high fitness and novelty is a challenging task in data-efficient protein engineering. Exploration beyond wild-type neighborhoods often leads to biologically implausible sequences or relies on surrogate models that lose fidelity in novel regions. Here, we propose ProSpero, an active learning framework in which a frozen pre-trained generative model is guided by a surrogate updated from oracle feedback. By integrating fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling, our approach enables exploration beyond wild-type neighborhoods while preserving biological plausibility. We show that our framework remains effective even when the surrogate is misspecified. ProSpero consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2505.11185.pdf' target='_blank'>https://arxiv.org/pdf/2505.11185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Madeddu, Lucia Testa, Gianluca De Carlo, Michele Pieroni, Andrea Mastropietro, Aris Anagnostopoulos, Paolo Tieri, Sergio Barbarossa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11185">VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intrinsic complexity of human biology presents ongoing challenges to scientific understanding. Researchers collaborate across disciplines to expand our knowledge of the biological interactions that define human life. AI methodologies have emerged as powerful tools across scientific domains, particularly in computational biology, where graph data structures effectively model biological entities such as protein-protein interaction (PPI) networks and gene functional networks. Those networks are used as datasets for paramount network medicine tasks, such as gene-disease association prediction, drug repurposing, and polypharmacy side effect studies. Reliable predictions from machine learning models require high-quality foundational data. In this work, we present a comprehensive multi-purpose biological knowledge graph constructed by integrating and refining multiple publicly available datasets. Building upon the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing information from the main available public data sources, and c) enriching the graph nodes with expressive feature vectors such as molecular fingerprints and gene ontologies. Biologically and chemically relevant features improve the capacity of machine learning models to generate accurate and well-structured embedding spaces. The resulting resource represents a coherent and reliable biological knowledge graph that serves as a state-of-the-art platform to advance research in computational biology and precision medicine. Moreover, it offers the opportunity to benchmark graph-based machine learning and network medicine models on relevant tasks. We demonstrate the effectiveness of the proposed dataset by benchmarking it against the task of drug repurposing, PPI prediction, and side-effect prediction, modeled as link prediction problems.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2505.01433.pdf' target='_blank'>https://arxiv.org/pdf/2505.01433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Qi, Hanzhang Fang, Siqi jiang, Tianxing Hu, Wei Zhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01433">Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2504.17624.pdf' target='_blank'>https://arxiv.org/pdf/2504.17624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Chunhao Zhu, Xiaobing Lan, Haiming Zhuang, Mingyu Li, Jian Zhang, Shaoyong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17624">Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled receptor superfamily, plays an important role in modulating dopaminergic neuronal activity and eliciting opioid-independent analgesia. Recent studies suggest that promoting \{beta}-arrestin-biased signaling in NTSR1 may diminish drugs of abuse, such as psychostimulants, thereby offering a potential avenue for treating human addiction-related disorders. In this study, we utilized a novel computational and experimental approach that combined nudged elastic band-based molecular dynamics simulations, Markov state models, temporal communication network analysis, site-directed mutagenesis, and conformational biosensors, to explore the intricate mechanisms underlying NTSR1 activation and biased signaling. Our study reveals a dynamic stepwise transition mechanism and activated transmission network associated with NTSR1 activation. It also yields valuable insights into the complex interplay between the unique polar network, non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we identified a cryptic allosteric site located in the intracellular region of the receptor that exists in an intermediate state within the activation pathway. Collectively, these findings contribute to a more profound understanding of NTSR1 activation and biased signaling at the atomic level, thereby providing a potential strategy for the development of NTSR1 allosteric modulators in the realm of G protein-coupled receptor biology, biophysics, and medicine.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2504.10564.pdf' target='_blank'>https://arxiv.org/pdf/2504.10564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Cremer, Ross Irwin, Alessandro Tibo, Jon Paul Janet, Simon Olsson, Djork-ArnÃ© Clevert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10564">FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FLOWR, a novel structure-based framework for the generation and optimization of three-dimensional ligands. FLOWR integrates continuous and categorical flow matching with equivariant optimal transport, enhanced by an efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a thoroughly curated dataset comprising ligand-pocket co-crystal complexes specifically designed to address existing data quality issues. Empirical evaluations demonstrate that FLOWR surpasses current state-of-the-art diffusion- and flow-based methods in terms of PoseBusters-validity, pose accuracy, and interaction recovery, while offering a significant inference speedup, achieving up to 70-fold faster performance. In addition, we introduce FLOWR:multi, a highly accurate multi-purpose model allowing for the targeted sampling of novel ligands that adhere to predefined interaction profiles and chemical substructures for fragment-based design without the need of re-training or any re-sampling strategies
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2504.04453.pdf' target='_blank'>https://arxiv.org/pdf/2504.04453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Amaan Sayeed, Engin Tekin, Maryam Nadeem, Nancy A. ElNaker, Aahan Singh, Natalia Vassilieva, Boulbaba Ben Amor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04453">Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlocking the next generation of biotechnology and therapeutic innovation demands overcoming the inherent complexity and resource-intensity of conventional protein engineering methods. Recent GenAI-powered computational techniques often rely on the availability of the target protein's 3D structures and specific binding sites to generate high-affinity binders, constraints exhibited by models such as AlphaProteo and RFdiffusion. In this work, we explore the use of Protein Language Models (pLMs) for high-affinity binder generation. We introduce Prot42, a novel family of Protein Language Models (pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing deep evolutionary, structural, and functional insights through an advanced auto-regressive, decoder-only architecture inspired by breakthroughs in natural language processing, Prot42 dramatically expands the capabilities of computational protein design based on language only. Remarkably, our models handle sequences up to 8,192 amino acids, significantly surpassing standard limitations and enabling precise modeling of large proteins and complex multi-domain sequences. Demonstrating powerful practical applications, Prot42 excels in generating high-affinity protein binders and sequence-specific DNA-binding proteins. Our innovative models are publicly available, offering the scientific community an efficient and precise computational toolkit for rapid protein engineering.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2503.22939.pdf' target='_blank'>https://arxiv.org/pdf/2503.22939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Alharbi, Nishant Budhiraja, Aleksandar Vakanski, Boyu Zhang, Murtada K. Elbashir, Harshith Guduru, Mohanad Mohammed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22939">Interpretable Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and Biomarker Identification using Multi-Omics Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of heterogeneous multi-omics datasets at a systems level remains a central challenge for developing analytical and computational models in precision cancer diagnostics. This paper introduces Multi-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes messenger-RNA, micro-RNA sequences, and DNA methylation samples together with Protein-Protein Interaction (PPI) networks for cancer classification across 31 different cancer types. The proposed approach combines differential gene expression with DESeq2, Linear Models for Microarray (LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression to reduce multi-omics data dimensionality while preserving relevant biological features. The model architecture is based on the Kolmogorov-Arnold theorem principle and uses trainable univariate functions to enhance interpretability and feature analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits low experimental variability in comparison to related deep learning-based models. The biomarkers identified by MOGKAN were validated as cancer-related markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. By integrating multi-omics data with graph-based deep learning, our proposed approach demonstrates robust predictive performance and interpretability with potential to enhance the translation of complex multi-omics data into clinically actionable cancer diagnostics.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2503.16563.pdf' target='_blank'>https://arxiv.org/pdf/2503.16563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aahan Singh, Engin Tekin, Maryam Nadeem, Nancy A. ElNaker, Mohammad Amaan Sayeed, Natalia Vassilieva, Boulbaba Ben Amor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16563">Chem42: a Family of chemical Language Models for Target-aware Ligand Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Revolutionizing drug discovery demands more than just understanding molecular interactions - it requires generative models that can design novel ligands tailored to specific biological targets. While chemical Language Models (cLMs) have made strides in learning molecular properties, most fail to incorporate target-specific insights, restricting their ability to drive de-novo ligand generation. Chem42, a cutting-edge family of generative chemical Language Models, is designed to bridge this gap. By integrating atomic-level interactions with multimodal inputs from Prot42, a complementary protein Language Model, Chem42 achieves a sophisticated cross-modal representation of molecular structures, interactions, and binding patterns. This innovative framework enables the creation of structurally valid, synthetically accessible ligands with enhanced target specificity. Evaluations across diverse protein targets confirm that Chem42 surpasses existing approaches in chemical validity, target-aware design, and predicted binding affinity. By reducing the search space of viable drug candidates, Chem42 could accelerate the drug discovery pipeline, offering a powerful generative AI tool for precision medicine. Our Chem42 models set a new benchmark in molecule property prediction, conditional molecule generation, and target-aware ligand design. The models are publicly available at huggingface.co/inceptionai.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2503.05738.pdf' target='_blank'>https://arxiv.org/pdf/2503.05738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Wolf, Leif Seute, Vsevolod Viliuga, Simon Wagner, Jan StÃ¼hmer, Frauke GrÃ¤ter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05738">Learning conformational ensembles of proteins based on backbone geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have recently been proposed for sampling protein conformations from the Boltzmann distribution, as an alternative to often prohibitively expensive Molecular Dynamics simulations. However, current state-of-the-art approaches rely on fine-tuning pre-trained folding models and evolutionary sequence information, limiting their applicability and efficiency, and introducing potential biases. In this work, we propose a flow matching model for sampling protein conformations based solely on backbone geometry. We introduce a geometric encoding of the backbone equilibrium structure as input and propose to condition not only the flow but also the prior distribution on the respective equilibrium structure, eliminating the need for evolutionary information. The resulting model is orders of magnitudes faster than current state-of-the-art approaches at comparable accuracy and can be trained from scratch in a few GPU days. In our experiments, we demonstrate that the proposed model achieves competitive performance with reduced inference time, across not only an established benchmark of naturally occurring proteins but also de novo proteins, for which evolutionary information is scarce.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2503.04734.pdf' target='_blank'>https://arxiv.org/pdf/2503.04734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna T. Thomas, Adam Yee, Andrew Mayne, Maya B. Mathur, Dan Jurafsky, Kristina GligoriÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04734">What can large language models do for sustainable food?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with combinatorial optimization to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2502.18875.pdf' target='_blank'>https://arxiv.org/pdf/2502.18875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanglei Xue, Meihan Zhang, Shuqi Li, Xinyu Gao, James A. Wohlschlegel, Wenbing Huang, Yi Yang, Weixian Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18875">SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Targeted protein degradation (TPD) induced by small molecules has emerged as a rapidly evolving modality in drug discovery, targeting proteins traditionally considered "undruggable". Proteolysis-targeting chimeras (PROTACs) and molecular glue degraders (MGDs) are the primary small molecules that induce TPD. Both types of molecules form a ternary complex linking an E3 ligase with a target protein, a crucial step for drug discovery. While significant advances have been made in binary structure prediction for proteins and small molecules, ternary structure prediction remains challenging due to obscure interaction mechanisms and insufficient training data. Traditional methods relying on manually assigned rules perform poorly and are computationally demanding due to extensive random sampling. In this work, we introduce DeepTernary, a novel deep learning-based approach that directly predicts ternary structures in an end-to-end manner using an encoder-decoder architecture. DeepTernary leverages an SE(3)-equivariant graph neural network (GNN) with both intra-graph and ternary inter-graph attention mechanisms to capture intricate ternary interactions from our collected high-quality training dataset, TernaryDB. The proposed query-based Pocket Points Decoder extracts the 3D structure of the final binding ternary complex from learned ternary embeddings, demonstrating state-of-the-art accuracy and speed in existing PROTAC benchmarks without prior knowledge from known PROTACs. It also achieves notable accuracy on the more challenging MGD benchmark under the blind docking protocol. Remarkably, our experiments reveal that the buried surface area calculated from predicted structures correlates with experimentally obtained degradation potency-related metrics. Consequently, DeepTernary shows potential in effectively assisting and accelerating the development of TPDs for previously undruggable targets.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2502.16446.pdf' target='_blank'>https://arxiv.org/pdf/2502.16446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haocheng Tang, Jing Long, Beihong Ji, Junmei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16446">Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity. Our method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN across three representative cases. First, on nucleic acid- and protein-targeting molecules, ADSeqGAN shows superior capability in generating nucleic acid binders compared to baseline models. Second, through oversampling, it markedly improves CNS drug generation, achieving higher yields than traditional de novo models. Third, in cannabinoid receptor type 1 (CB1) ligand design, ADSeqGAN generates novel druglike molecules, with 32.8\% predicted actives surpassing hit rates of CB1-focused and general-purpose libraries when assessed by a target-specific LRIP-SF scoring function. Overall, ADSeqGAN offers a versatile framework for molecular design in data-scarce scenarios, with demonstrated applications in nucleic acid binders, CNS drugs, and CB1 ligands.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2502.01809.pdf' target='_blank'>https://arxiv.org/pdf/2502.01809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianming Huang, Hiroyuki Kasai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01809">Self-supervised Subgraph Neural Network With Deep Reinforcement Walk Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph data, with its structurally variable nature, represents complex real-world phenomena like chemical compounds, protein structures, and social networks. Traditional Graph Neural Networks (GNNs) primarily utilize the message-passing mechanism, but their expressive power is limited and their prediction lacks explainability. To address these limitations, researchers have focused on graph substructures. Subgraph neural networks (SGNNs) and GNN explainers have emerged as potential solutions, but each has its limitations. SGNNs computes graph representations based on the bags of subgraphs to enhance the expressive power. However, they often rely on predefined algorithm-based sampling strategies, which is inefficient. GNN explainers adopt data-driven approaches to generate important subgraphs to provide explanation. Nevertheless, their explanation is difficult to be translated into practical improvements on GNNs. To overcome these issues, we propose a novel self-supervised framework that integrates SGNNs with the generation approach of GNN explainers, named the Reinforcement Walk Exploration SGNN (RWE-SGNN). Our approach features a sampling model trained in an explainer fashion, optimizing subgraphs to enhance model performance. To achieve a data-driven sampling approach, unlike traditional subgraph generation approaches, we propose a novel walk exploration process, which efficiently extracts important substructures, simplifying the embedding process and avoiding isomorphism problems. Moreover, we prove that our proposed walk exploration process has equivalent generation capability to the traditional subgraph generation process. Experimental results on various graph datasets validate the effectiveness of our proposed method, demonstrating significant improvements in performance and precision.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2412.11082.pdf' target='_blank'>https://arxiv.org/pdf/2412.11082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Tian, Yuxin Xu, Yixuan Yang, Zhen Wang, Ziqi Liu, Pengju Yan, Xiaolin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11082">EquiFlow: Equivariant Conditional Flow Matching with Optimal Transport for 3D Molecular Conformation Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular 3D conformations play a key role in determining how molecules interact with other molecules or protein surfaces. Recent deep learning advancements have improved conformation prediction, but slow training speeds and difficulties in utilizing high-degree features limit performance. We propose EquiFlow, an equivariant conditional flow matching model with optimal transport. EquiFlow uniquely applies conditional flow matching in molecular 3D conformation prediction, leveraging simulation-free training to address slow training speeds. It uses a modified Equiformer model to encode Cartesian molecular conformations along with their atomic and bond properties into higher-degree embeddings. Additionally, EquiFlow employs an ODE solver, providing faster inference speeds compared to diffusion models with SDEs. Experiments on the QM9 dataset show that EquiFlow predicts small molecule conformations more accurately than current state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2412.03496.pdf' target='_blank'>https://arxiv.org/pdf/2412.03496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ricci, Guy Pelc, Zoe Piran, Noa Moriel, Mor Nitzan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03496">TRENDy: Temporal Regression of Effective Nonlinear Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatiotemporal dynamics pervade the natural sciences, from the morphogen dynamics underlying patterning in animal pigmentation to the protein waves controlling cell division. A central challenge lies in understanding how controllable parameters induce qualitative changes in system behavior called bifurcations. This endeavor is particularly difficult in realistic settings where governing partial differential equations (PDEs) are unknown and data is limited and noisy. To address this challenge, we propose TRENDy (Temporal Regression of Effective Nonlinear Dynamics), an equation-free approach to learning low-dimensional, predictive models of spatiotemporal dynamics. TRENDy first maps input data to a low-dimensional space of effective dynamics through a cascade of multiscale filtering operations. Our key insight is the recognition that these effective dynamics can be fit by a neural ordinary differential equation (NODE) having the same parameter space as the input PDE. The preceding filtering operations strongly regularize the phase space of the NODE, making TRENDy significantly more robust to noise compared to existing methods. We train TRENDy to predict the effective dynamics of synthetic and real data representing dynamics from across the physical and life sciences. We then demonstrate how we can automatically locate both Turing and Hopf bifurcations in unseen regions of parameter space. We finally apply our method to the analysis of spatial patterning of the ocellated lizard through development. We found that TRENDy's predicted effective state not only accurately predicts spatial changes over time but also identifies distinct pattern features unique to different anatomical regions, such as the tail, neck, and body--an insight that highlights the potential influence of surface geometry on reaction-diffusion mechanisms and their role in driving spatially varying pattern dynamics.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2411.08286.pdf' target='_blank'>https://arxiv.org/pdf/2411.08286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Han, Wu-Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08286">Hashing for Protein Structure Similarity Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure similarity search (PSSS), which tries to search proteins with similar structures, plays a crucial role across diverse domains from drug design to protein function prediction and molecular evolution. Traditional alignment-based PSSS methods, which directly calculate alignment on the protein structures, are highly time-consuming with high memory cost. Recently, alignment-free methods, which represent protein structures as fixed-length real-valued vectors, are proposed for PSSS. Although these methods have lower time and memory cost than alignment-based methods, their time and memory cost is still too high for large-scale PSSS, and their accuracy is unsatisfactory. In this paper, we propose a novel method, called $\underline{\text{p}}$r$\underline{\text{o}}$tein $\underline{\text{s}}$tructure $\underline{\text{h}}$ashing (POSH), for PSSS. POSH learns a binary vector representation for each protein structure, which can dramatically reduce the time and memory cost for PSSS compared with real-valued vector representation based methods. Furthermore, in POSH we also propose expressive hand-crafted features and a structure encoder to well model both node and edge interactions in proteins. Experimental results on real datasets show that POSH can outperform other methods to achieve state-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more than six times and speed improvement of more than four times, compared with other methods.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2411.05238.pdf' target='_blank'>https://arxiv.org/pdf/2411.05238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Wagner, Leif Seute, Vsevolod Viliuga, Nicolas Wolf, Frauke GrÃ¤ter, Jan StÃ¼hmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05238">Generating Highly Designable Proteins with Geometric Algebra Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a generative model for protein backbone design utilizing geometric products and higher order message passing. In particular, we propose Clifford Frame Attention (CFA), an extension of the invariant point attention (IPA) architecture from AlphaFold2, in which the backbone residue frames and geometric features are represented in the projective geometric algebra. This enables to construct geometrically expressive messages between residues, including higher order terms, using the bilinear operations of the algebra. We evaluate our architecture by incorporating it into the framework of FrameFlow, a state-of-the-art flow matching model for protein backbone generation. The proposed model achieves high designability, diversity and novelty, while also sampling protein backbones that follow the statistical distribution of secondary structure elements found in naturally occurring proteins, a property so far only insufficiently achieved by many state-of-the-art generative models.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2411.04863.pdf' target='_blank'>https://arxiv.org/pdf/2411.04863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klemens FlÃ¶ge, Srisruthi Udayakumar, Johanna Sommer, Marie Piraud, Stefan Kesselheim, Vincent Fortuin, Stephan GÃ¼nneman, Karel J van der Weg, Holger Gohlke, Erinc Merdivan, Alina Bazarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04863">OneProt: Towards Multi-Modal Protein Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Artificial Intelligence have enabled multi-modal systems to model and translate diverse information spaces. Extending beyond text and vision, we introduce OneProt, a multi-modal AI for proteins that integrates structural, sequence, text, and binding site data. Using the ImageBind framework, OneProt aligns the latent spaces of protein modality encoders in a lightweight fine-tuning scheme that focuses on pairwise alignment with sequence data rather than requiring full matches. This novel approach comprises a mix of Graph Neural Networks and transformer architectures. It demonstrates strong performance in retrieval tasks and showcases the efficacy of multi-modal systems in Protein Machine Learning through a broad spectrum of downstream baselines, including enzyme function prediction and binding site analysis. Furthermore, OneProt enables the transfer of representational information from specialized encoders to the sequence encoder, enhancing capabilities for distinguishing evolutionarily related and unrelated sequences and exhibiting representational properties where evolutionarily related proteins align in similar directions within the latent space. In addition, we extensively investigate modality ablations to identify the encoders that contribute most to predictive performance, highlighting the significance of the binding site encoder, which has not been used in similar models previously. This work expands the horizons of multi-modal protein models, paving the way for transformative applications in drug discovery, biocatalytic reaction planning, and protein engineering.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2411.04130.pdf' target='_blank'>https://arxiv.org/pdf/2411.04130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keir Adams, Kento Abeywardane, Jenna Fromer, Connor W. Coley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04130">ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic, and pharmacophore similarity scoring functions. We instead hypothesize that a generative model which learns the joint distribution over 3D molecular structures and their interaction profiles may facilitate 3D interaction-aware chemical design. We specifically design ShEPhERD, an SE(3)-equivariant diffusion model which jointly diffuses/denoises 3D molecular graphs and representations of their shapes, electrostatic potential surfaces, and (directional) pharmacophores to/from Gaussian noise. Inspired by traditional ligand discovery, we compose 3D similarity scoring functions to assess ShEPhERD's ability to conditionally generate novel molecules with desired interaction profiles. We demonstrate ShEPhERD's potential for impact via exemplary drug design tasks including natural product ligand hopping, protein-blind bioactive hit diversification, and bioisosteric fragment merging.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2410.21518.pdf' target='_blank'>https://arxiv.org/pdf/2410.21518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxian Shi, Menghua Wu, Regina Barzilay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21518">Predicting sub-population specific viral evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forecasting the change in the distribution of viral variants is crucial for therapeutic design and disease surveillance. This task poses significant modeling challenges due to the sharp differences in virus distributions across sub-populations (e.g., countries) and their dynamic interactions. Existing machine learning approaches that model the variant distribution as a whole are incapable of making location-specific predictions and ignore transmissions that shape the viral landscape. In this paper, we propose a sub-population specific protein evolution model, which predicts the time-resolved distributions of viral proteins in different locations. The algorithm explicitly models the transmission rates between sub-populations and learns their interdependence from data. The change in protein distributions across all sub-populations is defined through a linear ordinary differential equation (ODE) parametrized by transmission rates. Solving this ODE yields the likelihood of a given protein occurring in particular sub-populations. Multi-year evaluation on both SARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms baselines in accurately predicting distributions of viral proteins across continents and countries. We also find that the transmission rates learned from data are consistent with the transmission pathways discovered by retrospective phylogenetic analysis.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2410.13264.pdf' target='_blank'>https://arxiv.org/pdf/2410.13264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Han, Yuancheng Sun, Kai Chen, Kang Liu, Qiwei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13264">The Latent Road to Atoms: Backmapping Coarse-grained Protein Structures with Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coarse-grained(CG) molecular dynamics simulations offer computational efficiency for exploring protein conformational ensembles and thermodynamic properties. Though coarse representations enable large-scale simulations across extended temporal and spatial ranges, the sacrifice of atomic-level details limits their utility in tasks such as ligand docking and protein-protein interaction prediction. Backmapping, the process of reconstructing all-atom structures from coarse-grained representations, is crucial for recovering these fine details. While recent machine learning methods have made strides in protein structure generation, challenges persist in reconstructing diverse atomistic conformations that maintain geometric accuracy and chemical validity. In this paper, we present Latent Diffusion Backmapping (LDB), a novel approach leveraging denoising diffusion within latent space to address these challenges. By combining discrete latent encoding with diffusion, LDB bypasses the need for equivariant and internal coordinate manipulation, significantly simplifying the training and sampling processes as well as facilitating better and wider exploration in configuration space. We evaluate LDB's state-of-the-art performance on three distinct protein datasets, demonstrating its ability to efficiently reconstruct structures with high structural accuracy and chemical validity. Moreover, LDB shows exceptional versatility in capturing diverse protein ensembles, highlighting its capability to explore intricate conformational spaces. Our results position LDB as a powerful and scalable approach for backmapping, effectively bridging the gap between CG simulations and atomic-level analyses in computational biology.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2410.07364.pdf' target='_blank'>https://arxiv.org/pdf/2410.07364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ismail Erbas, Aporva Amarnath, Vikas Pandey, Karthik Swaminathan, Naigang Wang, Xavier Intes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07364">Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fluorescence lifetime imaging (FLI) is a widely used technique in the biomedical field for measuring the decay times of fluorescent molecules, providing insights into metabolic states, protein interactions, and ligand-receptor bindings. However, its broader application in fast biological processes, such as dynamic activity monitoring, and clinical use, such as in guided surgery, is limited by long data acquisition times and computationally demanding data processing. While deep learning has reduced post-processing times, time-resolved data acquisition remains a bottleneck for real-time applications. To address this, we propose a method to achieve real-time FLI using an FPGA-based hardware accelerator. Specifically, we implemented a GRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with time-resolved cameras. The GRU model balances accurate processing with the resource constraints of FPGAs, which have limited DSP units and BRAM. The limited memory and computational resources on the FPGA require efficient scheduling of operations and memory allocation to deploy deep learning models for low-latency applications. We address these challenges by using STOMP, a queue-based discrete-event simulator that automates and optimizes task scheduling and memory management on hardware. By integrating a GRU-based Seq2Seq model and its compressed version, called Seq2SeqLite, generated through knowledge distillation, we were able to process multiple pixels in parallel, reducing latency compared to sequential processing. We explore various levels of parallelism to achieve an optimal balance between performance and resource utilization. Our results indicate that the proposed techniques achieved a 17.7x and 52.0x speedup over manual scheduling for the Seq2Seq model and the Seq2SeqLite model, respectively.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2410.05325.pdf' target='_blank'>https://arxiv.org/pdf/2410.05325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Alharbi, Aleksandar Vakanski, Boyu Zhang, Murtada K. Elbashir, Mohanad Mohammed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05325">Comparative Analysis of Multi-Omics Integration Using Advanced Graph Neural Networks for Cancer Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-omics data is increasingly being utilized to advance computational methods for cancer classification. However, multi-omics data integration poses significant challenges due to the high dimensionality, data complexity, and distinct characteristics of various omics types. This study addresses these challenges and evaluates three graph neural network architectures for multi-omics (MO) integration based on graph-convolutional networks (GCN), graph-attention networks (GAT), and graph-transformer networks (GTN) for classifying 31 cancer types and normal tissues. To address the high-dimensionality of multi-omics data, we employed LASSO (Least Absolute Shrinkage and Selection Operator) regression for feature selection, leading to the creation of LASSO-MOGCN, LASSO-MOGAT, and LASSO-MOTGN models. Graph structures for the networks were constructed using gene correlation matrices and protein-protein interaction networks for multi-omics integration of messenger-RNA, micro-RNA, and DNA methylation data. Such data integration enables the networks to dynamically focus on important relationships between biological entities, improving both model performance and interpretability. Among the models, LASSO-MOGAT with a correlation-based graph structure achieved state-of-the-art accuracy (95.9%) and outperformed the LASSO-MOGCN and LASSO-MOTGN models in terms of precision, recall, and F1-score. Our findings demonstrate that integrating multi-omics data in graph-based architectures enhances cancer classification performance by uncovering distinct molecular patterns that contribute to a better understanding of cancer biology and potential biomarkers for disease progression.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2409.10964.pdf' target='_blank'>https://arxiv.org/pdf/2409.10964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairi Furui, Masahito Ohue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10964">Active learning for energy-based antibody optimization and enhanced screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction and optimization of protein-protein binding affinity is crucial for therapeutic antibody development. Although machine learning-based prediction methods $ÎÎG$ are suitable for large-scale mutant screening, they struggle to predict the effects of multiple mutations for targets without existing binders. Energy function-based methods, though more accurate, are time consuming and not ideal for large-scale screening. To address this, we propose an active learning workflow that efficiently trains a deep learning model to learn energy functions for specific targets, combining the advantages of both approaches. Our method integrates the RDE-Network deep learning model with Rosetta's energy function-based Flex ddG to efficiently explore mutants. In a case study targeting HER2-binding Trastuzumab mutants, our approach significantly improved the screening performance over random selection and demonstrated the ability to identify mutants with better binding properties without experimental $ÎÎG$ data. This workflow advances computational antibody design by combining machine learning, physics-based computations, and active learning to achieve more efficient antibody development.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2408.17384.pdf' target='_blank'>https://arxiv.org/pdf/2408.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fadi Alharbi, Aleksandar Vakanski, Murtada K. Elbashir, Mohanad Mohammed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17384">LASSO-MOGAT: A Multi-Omics Graph Attention Framework for Cancer Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of machine learning methods to analyze changes in gene expression patterns has recently emerged as a powerful approach in cancer research, enhancing our understanding of the molecular mechanisms underpinning cancer development and progression. Combining gene expression data with other types of omics data has been reported by numerous works to improve cancer classification outcomes. Despite these advances, effectively integrating high-dimensional multi-omics data and capturing the complex relationships across different biological layers remains challenging. This paper introduces LASSO-MOGAT (LASSO-Multi-Omics Gated ATtention), a novel graph-based deep learning framework that integrates messenger RNA, microRNA, and DNA methylation data to classify 31 cancer types. Utilizing differential expression analysis with LIMMA and LASSO regression for feature selection, and leveraging Graph Attention Networks (GATs) to incorporate protein-protein interaction (PPI) networks, LASSO-MOGAT effectively captures intricate relationships within multi-omics data. Experimental validation using five-fold cross-validation demonstrates the method's precision, reliability, and capacity for providing comprehensive insights into cancer molecular mechanisms. The computation of attention coefficients for the edges in the graph by the proposed graph-attention architecture based on protein-protein interactions proved beneficial for identifying synergies in multi-omics data for cancer classification.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2408.12682.pdf' target='_blank'>https://arxiv.org/pdf/2408.12682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shentong Mo, Paul Pu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12682">MultiMed: Massively Multimodal and Multitask Medical Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biomedical data is inherently multimodal, consisting of electronic health records, medical imaging, digital pathology, genome sequencing, wearable sensors, and more. The application of artificial intelligence tools to these multifaceted sensing technologies has the potential to revolutionize the prognosis, diagnosis, and management of human health and disease. However, current approaches to biomedical AI typically only train and evaluate with one or a small set of medical modalities and tasks. This limitation hampers the development of comprehensive tools that can leverage the rich interconnected information across many heterogeneous biomedical sensors. To address this challenge, we present MultiMed, a benchmark designed to evaluate and enable large-scale learning across a wide spectrum of medical modalities and tasks. MultiMed consists of 2.56 million samples across ten medical modalities such as medical reports, pathology, genomics, and protein data, and is structured into eleven challenging tasks, including disease prognosis, protein structure prediction, and medical question answering. Using MultiMed, we conduct comprehensive experiments benchmarking state-of-the-art unimodal, multimodal, and multitask models. Our analysis highlights the advantages of training large-scale medical models across many related modalities and tasks. Moreover, MultiMed enables studies of generalization across related medical concepts, robustness to real-world noisy data and distribution shifts, and novel modality combinations to improve prediction performance. MultiMed will be publicly available and regularly updated and welcomes inputs from the community.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2407.19790.pdf' target='_blank'>https://arxiv.org/pdf/2407.19790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Han, Yun Hong, Wu-Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19790">Hashing based Contrastive Learning for Virtual Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual screening (VS) is a critical step in computer-aided drug discovery, aiming to identify molecules that bind to a specific target receptor like protein. Traditional VS methods, such as docking, are often too time-consuming for screening large-scale molecular databases. Recent advances in deep learning have demonstrated that learning vector representations for both proteins and molecules using contrastive learning can outperform traditional docking methods. However, given that target databases often contain billions of molecules, real-valued vector representations adopted by existing methods can still incur significant memory and time costs in VS. To address this problem, in this paper we propose a hashing-based contrastive learning method, called DrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient binary hash codes for retrieval. In particular, DrugHash designs a simple yet effective hashing strategy to enable end-to-end learning of binary hash codes for both protein and molecule modalities, which can dramatically reduce the memory and time costs with higher accuracy compared with existing methods. Experimental results show that DrugHash can outperform existing methods to achieve state-of-the-art accuracy, with a memory saving of 32$\times$ and a speed improvement of 3.5$\times$.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2407.15220.pdf' target='_blank'>https://arxiv.org/pdf/2407.15220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliya Burankova, Miriam Abele, Mohammad Bakhtiari, Christine von TÃ¶rne, Teresa Barth, Lisa Schweizer, Pieter Giesbertz, Johannes R. Schmidt, Stefan Kalkhof, Janina MÃ¼ller-Deile, Peter A van Veelen, Yassene Mohammed, Elke Hammer, Lis Arend, Klaudia Adamowicz, Tanja Laske, Anne Hartebrodt, Tobias Frisch, Chen Meng, Julian Matschinske, Julian SpÃ¤th, Richard RÃ¶ttger, Veit SchwÃ¤mmle, Stefanie M. Hauck, Stefan Lichtenthaler, Axel Imhof, Matthias Mann, Christina Ludwig, Bernhard Kuster, Jan Baumbach, Olga Zolotareva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15220">Privacy-Preserving Multi-Center Differential Protein Abundance Analysis with FedProt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative mass spectrometry has revolutionized proteomics by enabling simultaneous quantification of thousands of proteins. Pooling patient-derived data from multiple institutions enhances statistical power but raises significant privacy concerns. Here we introduce FedProt, the first privacy-preserving tool for collaborative differential protein abundance analysis of distributed data, which utilizes federated learning and additive secret sharing. In the absence of a multicenter patient-derived dataset for evaluation, we created two, one at five centers from LFQ E.coli experiments and one at three centers from TMT human serum. Evaluations using these datasets confirm that FedProt achieves accuracy equivalent to DEqMS applied to pooled data, with completely negligible absolute differences no greater than $\text{$4 \times 10^{-12}$}$. In contrast, -log10(p-values) computed by the most accurate meta-analysis methods diverged from the centralized analysis results by up to 25-27. FedProt is available as a web tool with detailed documentation as a FeatureCloud App.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2407.04465.pdf' target='_blank'>https://arxiv.org/pdf/2407.04465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanujit Chakraborty, Swarup Chattopadhyay, Suchismita Das, Shraddha M. Naik, Chittaranjan Hens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04465">A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2407.03655.pdf' target='_blank'>https://arxiv.org/pdf/2407.03655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuqiang Chen, Ranran Zhang, Boyun Zheng, Yiwen Sun, Jiahui He, Wenjian Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03655">Pathological Semantics-Preserving Learning for H&E-to-IHC Virtual Staining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional hematoxylin-eosin (H&E) staining is limited to revealing cell morphology and distribution, whereas immunohistochemical (IHC) staining provides precise and specific visualization of protein activation at the molecular level. Virtual staining technology has emerged as a solution for highly efficient IHC examination, which directly transforms H&E-stained images to IHC-stained images. However, virtual staining is challenged by the insufficient mining of pathological semantics and the spatial misalignment of pathological semantics. To address these issues, we propose the Pathological Semantics-Preserving Learning method for Virtual Staining (PSPStain), which directly incorporates the molecular-level semantic information and enhances semantics interaction despite any spatial inconsistency. Specifically, PSPStain comprises two novel learning strategies: 1) Protein-Aware Learning Strategy (PALS) with Focal Optical Density (FOD) map maintains the coherence of protein expression level, which represents molecular-level semantic information; 2) Prototype-Consistent Learning Strategy (PCLS), which enhances cross-image semantic interaction by prototypical consistency learning. We evaluate PSPStain on two public datasets using five metrics: three clinically relevant metrics and two for image quality. Extensive experiments indicate that PSPStain outperforms current state-of-the-art H&E-to-IHC virtual staining methods and demonstrates a high pathological correlation between the staging of real and virtual stains.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2407.00002.pdf' target='_blank'>https://arxiv.org/pdf/2407.00002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter MÃ¸rch Groth, Mads Herbert Kerrn, Lars Olsen, Jesper Salomon, Wouter Boomsma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00002">Kermut: Composite kernel regression for protein variant effects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable prediction of protein variant effects is crucial for both protein optimization and for advancing biological understanding. For practical use in protein engineering, it is important that we can also provide reliable uncertainty estimates for our predictions, and while prediction accuracy has seen much progress in recent years, uncertainty metrics are rarely reported. We here provide a Gaussian process regression model, Kermut, with a novel composite kernel for modeling mutation similarity, which obtains state-of-the-art performance for supervised protein variant effect prediction while also offering estimates of uncertainty through its posterior. An analysis of the quality of the uncertainty estimates demonstrates that our model provides meaningful levels of overall calibration, but that instance-specific uncertainty calibration remains more challenging.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2406.14442.pdf' target='_blank'>https://arxiv.org/pdf/2406.14442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elisa GÃ³mez de Lope, Saurabh Deshpande, RamÃ³n ViÃ±as TornÃ©, Pietro LiÃ², Enrico Glaab, StÃ©phane P. A. Bordas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14442">Graph Representation Learning Strategies for Omics Data: A Case Study on Parkinson's Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omics data analysis is crucial for studying complex diseases, but its high dimensionality and heterogeneity challenge classical statistical and machine learning methods. Graph neural networks have emerged as promising alternatives, yet the optimal strategies for their design and optimization in real-world biomedical challenges remain unclear. This study evaluates various graph representation learning models for case-control classification using high-throughput biological data from Parkinson's disease and control samples. We compare topologies derived from sample similarity networks and molecular interaction networks, including protein-protein and metabolite-metabolite interactions (PPI, MMI). Graph Convolutional Network (GCNs), Chebyshev spectral graph convolution (ChebyNet), and Graph Attention Network (GAT), are evaluated alongside advanced architectures like graph transformers, the graph U-net, and simpler models like multilayer perceptron (MLP).
  These models are systematically applied to transcriptomics and metabolomics data independently. Our comparative analysis highlights the benefits and limitations of various architectures in extracting patterns from omics data, paving the way for more accurate and interpretable models in biomedical research.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2406.14150.pdf' target='_blank'>https://arxiv.org/pdf/2406.14150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Jose Garau-Luis, Patrick Bordes, Liam Gonzalez, Masa Roller, Bernardo P. de Almeida, Lorenz Hexemer, Christopher Blum, Stefan Laurent, Jan Grzegorzewski, Maren Lang, Thomas Pierrot, Guillaume Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14150">Multi-modal Transfer Learning between Biological Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biological sequences encode fundamental instructions for the building blocks of life, in the form of DNA, RNA, and proteins. Modeling these sequences is key to understand disease mechanisms and is an active research area in computational biology. Recently, Large Language Models have shown great promise in solving certain biological tasks but current approaches are limited to a single sequence modality (DNA, RNA, or protein). Key problems in genomics intrinsically involve multiple modalities, but it remains unclear how to adapt general-purpose sequence models to those cases. In this work we propose a multi-modal model that connects DNA, RNA, and proteins by leveraging information from different pre-trained modality-specific encoders. We demonstrate its capabilities by applying it to the largely unsolved problem of predicting how multiple RNA transcript isoforms originate from the same gene (i.e. same DNA sequence) and map to different transcription expression levels across various human tissues. We show that our model, dubbed IsoFormer, is able to accurately predict differential transcript expression, outperforming existing methods and leveraging the use of multiple modalities. Our framework also achieves efficient transfer knowledge from the encoders pre-training as well as in between modalities. We open-source our model, paving the way for new multi-modal gene expression approaches.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2406.03686.pdf' target='_blank'>https://arxiv.org/pdf/2406.03686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Zholus, Maksim Kuznetsov, Roman Schutski, Rim Shayakhmetov, Daniil Polykovskiy, Sarath Chandar, Alex Zhavoronkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03686">BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2405.16861.pdf' target='_blank'>https://arxiv.org/pdf/2405.16861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joongwon Lee, Wonho Zhung, Jisu Seo, Woo Youn Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16861">BInD: Bond and Interaction-generating Diffusion Model for Multi-objective Structure-based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A remarkable advance in geometric deep generative models with accumulated structural data enables structure-based drug design (SBDD) with target protein information only. However, most existing models struggle to address multi-objectives simultaneously while performing well only in their specialized tasks. Here, we present BInD, a diffusion model with knowledge-based guidance for multi-objective SBDD. BInD is designed to co-generate molecules and their interactions with a target protein to consider all key objectives equally well, including target-specific interactions, molecular properties, and local geometry. Comprehensive evaluations show that BInD achieves robust performance for all objectives while outperforming or matching state-of-the-art methods for each. Finally, we propose a train-free optimization method empowered by retrieving target-specific interactions, highlighting the role of non-covalent interactions in achieving higher selectivity and binding affinities to a target protein.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2405.07622.pdf' target='_blank'>https://arxiv.org/pdf/2405.07622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Cutting, FrÃ©dÃ©ric A. Dreyer, David Errington, Constantin Schneider, Charlotte M. Deane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07622">De novo antibody design with SE(3) diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce IgDiff, an antibody variable domain diffusion model based on a general protein backbone diffusion framework which was extended to handle multiple chains. Assessing the designability and novelty of the structures generated with our model, we find that IgDiff produces highly designable antibodies that can contain novel binding regions. The backbone dihedral angles of sampled structures show good agreement with a reference antibody distribution. We verify these designed antibodies experimentally and find that all express with high yield. Finally, we compare our model with a state-of-the-art generative backbone diffusion model on a range of antibody design tasks, such as the design of the complementarity determining regions or the pairing of a light chain to an existing heavy chain, and show improved properties and designability.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2404.19206.pdf' target='_blank'>https://arxiv.org/pdf/2404.19206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cenk Demir, Mamadou Diagne, Miroslav Krstic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19206">Periodic Event-Triggered Boundary Control of Neuron Growth with Actuation at Soma</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring novel strategies for the regulation of axon growth, we introduce a periodic event-triggered control (PETC) to enhance the practical implementation of the associated PDE backstepping control law. Neurological injuries may impair neuronal function, but therapies like Chondroitinase ABC (ChABC) have shown promise in improving axon elongation by influencing the extracellular matrix. This matrix, composed of extracellular macromolecules and minerals, regulates tubulin protein concentration, potentially aiding in neuronal recovery. The concentration and spatial distribution of tubulin influence axon elongation dynamics. Recent research explores feedback control strategies for this model, leading to the development of an event-triggering control (CETC) approach. In this approach, the control law updates when the monitored triggering condition is met, reducing actuation resource consumption. Through the meticulous redesign of the triggering mechanism, we introduce a periodic event-triggering control (PETC), updating control inputs at specific intervals, but evaluating the event-trigger only periodically, an ideal tool for standard time-sliced actuators like ChABC. PETC is a step forward to the design of practically feasible feedback laws for the neuron growth process. The PETC strategy establishes an upper bound on event triggers between periodic examinations, ensuring convergence and preventing Zeno behavior. Through Lyapunov analysis, we demonstrate the local exponential convergence of the system with the periodic event-triggering mechanism in the $L^2$-norm sense. Numerical examples are presented to confirm the theoretical findings.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2404.10450.pdf' target='_blank'>https://arxiv.org/pdf/2404.10450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingda Xu, Peisheng Qian, Ziyuan Zhao, Zeng Zeng, Jianguo Chen, Weide Liu, Xulei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10450">Graph Neural Networks for Protein-Protein Interactions -- A Short Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) play key roles in a broad range of biological processes. Numerous strategies have been proposed for predicting PPIs, and among them, graph-based methods have demonstrated promising outcomes owing to the inherent graph structure of PPI networks. This paper reviews various graph-based methodologies, and discusses their applications in PPI prediction. We classify these approaches into two primary groups based on their model structures. The first category employs Graph Neural Networks (GNN) or Graph Convolutional Networks (GCN), while the second category utilizes Graph Attention Networks (GAT), Graph Auto-Encoders and Graph-BERT. We highlight the distinctive methodologies of each approach in managing the graph-structured data inherent in PPI networks and anticipate future research directions in this domain.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2404.00050.pdf' target='_blank'>https://arxiv.org/pdf/2404.00050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leif Seute, Eric Hartmann, Jan StÃ¼hmer, Frauke GrÃ¤ter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00050">Grappa -- A Machine Learned Molecular Mechanics Force Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating large molecular systems over long timescales requires force fields that are both accurate and efficient. In recent years, E(3) equivariant neural networks have lifted the tension between computational efficiency and accuracy of force fields, but they are still several orders of magnitude more expensive than established molecular mechanics (MM) force fields. Here, we propose Grappa, a machine learning framework to predict MM parameters from the molecular graph, employing a graph attentional neural network and a transformer with symmetry-preserving positional encoding. The resulting Grappa force field outperformstabulated and machine-learned MM force fields in terms of accuracy at the same computational efficiency and can be used in existing Molecular Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces of small molecules, peptides, RNA and - showcasing its extensibility to uncharted regions of chemical space - radicals at state-of-the-art MM accuracy. We demonstrate Grappa's transferability to macromolecules in MD simulations from a small fast folding protein up to a whole virus particle. Our force field sets the stage for biomolecular simulations closer to chemical accuracy, but with the same computational cost as established protein force fields.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2403.17889.pdf' target='_blank'>https://arxiv.org/pdf/2403.17889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Kenlay, FrÃ©dÃ©ric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa, Douglas Pires, Charlotte M. Deane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17889">Large scale paired antibody language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering. This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2512.11105.pdf' target='_blank'>https://arxiv.org/pdf/2512.11105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngseung Jeon, Christopher Hwang, Ziwen Li, Taylor Le Lievre, Jesus J. Campagna, Cohn Whitaker, Varghese John, Eunice Jun, Xiang Anthony Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11105">Supporting Medicinal Chemists in Iterative Hypothesis Generation for Drug Target Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While drug discovery is vital for human health, the process remains inefficient. Medicinal chemists must navigate a vast protein space to identify target proteins that meet three criteria: physical and functional interactions, therapeutic impact, and docking potential. Prior approaches have provided fragmented support for each criterion, limiting the generation of promising hypotheses for wet-lab experiments. We present HAPPIER, an AI-powered tool that supports hypothesis generation with integrated multi-criteria support for target identification. HAPPIER enables medicinal chemists to 1) efficiently explore and verify proteins in a single integrated graph component showing multi-criteria satisfaction and 2) validate AI suggestions with domain knowledge. These capabilities facilitate iterative cycles of divergent and convergent thinking, essential for hypothesis generation. We evaluated HAPPIER with ten medicinal chemists, finding that it increased the number of high-confidence hypotheses and support for the iterative cycle, and further demonstrated the relationship between engaging in such cycles and confidence in outputs.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2512.05794.pdf' target='_blank'>https://arxiv.org/pdf/2512.05794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebonto Haque, Oliver M. Turnbull, Anisha Parsan, Nithin Parsan, John J. Yang, Charlotte M. Deane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05794">Mechanistic Interpretability of Antibody Language Models Using SAEs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2512.03592.pdf' target='_blank'>https://arxiv.org/pdf/2512.03592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guang Yang, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03592">Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding. In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2511.14559.pdf' target='_blank'>https://arxiv.org/pdf/2511.14559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Zheng, Shiyu Jiang, Gustavo Seabra, Chenglong Li, Yanjun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14559">Apo2Mol: 3D Molecule Generation via Dynamic Pocket-Aware Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models are rapidly advancing structure-based drug design, offering substantial promise for generating small molecule ligands that bind to specific protein targets. However, most current approaches assume a rigid protein binding pocket, neglecting the intrinsic flexibility of proteins and the conformational rearrangements induced by ligand binding, limiting their applicability in practical drug discovery. Here, we propose Apo2Mol, a diffusion-based generative framework for 3D molecule design that explicitly accounts for conformational flexibility in protein binding pockets. To support this, we curate a dataset of over 24,000 experimentally resolved apo-holo structure pairs from the Protein Data Bank, enabling the characterization of protein structure changes associated with ligand binding. Apo2Mol employs a full-atom hierarchical graph-based diffusion model that simultaneously generates 3D ligand molecules and their corresponding holo pocket conformations from input apo states. Empirical studies demonstrate that Apo2Mol can achieve state-of-the-art performance in generating high-affinity ligands and accurately capture realistic protein pocket conformational changes.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2511.13685.pdf' target='_blank'>https://arxiv.org/pdf/2511.13685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Disha Varshney, Samarth Garg, Sarthak Tyagi, Deeksha Varshney, Nayan Deep, Asif Ekbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13685">Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2511.13244.pdf' target='_blank'>https://arxiv.org/pdf/2511.13244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadav Bojan Sellam, Meital Bojan, Paul Schanda, Alex Bronstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13244">Seek and You Shall Fold</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2511.09465.pdf' target='_blank'>https://arxiv.org/pdf/2511.09465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hedwig Nora Nordlinder, Lukas Billera, Jack Collier Ryder, Anton Oresten, Aron Stålmarck, Theodor Mosetti Björk, Ben Murrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09465">Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori. Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2511.04854.pdf' target='_blank'>https://arxiv.org/pdf/2511.04854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvaro Prat, Leo Zhang, Charlotte M. Deane, Yee Whye Teh, Garrett M. Morris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04854">SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the binding pose of a ligand to a protein, known as molecular docking, is a fundamental task in drug discovery. Generative approaches promise faster, improved, and more diverse pose sampling than physics-based methods, but are often hindered by chemically implausible outputs, poor generalisability, and high computational cost. To address these challenges, we introduce a novel fragmentation scheme, leveraging inductive biases from structural chemistry, to decompose ligands into rigid-body fragments. Building on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion model that generates poses by learning to reassemble these rigid bodies within the binding pocket. By operating at the level of fragments in SE(3), SigmaDock exploits well-established geometric priors while avoiding overly complex diffusion processes and unstable training dynamics. Experimentally, we show SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates (RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8% reported by recent deep learning approaches, whilst demonstrating consistent generalisation to unseen proteins. SigmaDock is the first deep learning approach to surpass classical physics-based docking under the PB train-test split, marking a significant leap forward in the reliability and feasibility of deep learning for molecular modelling.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2510.16253.pdf' target='_blank'>https://arxiv.org/pdf/2510.16253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arielle Sanford, Shuo Sun, Christian B. Mendl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16253">Protein Folding with Neural Ordinary Differential Equations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2507.13580.pdf' target='_blank'>https://arxiv.org/pdf/2507.13580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tuo, Yan Li, Xuanning Hu, Haishi Zhao, Xueyan Liu, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13580">A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combinatorial optimization algorithm is essential in computer-aided drug design by progressively exploring chemical space to design lead compounds with high affinity to target protein. However current methods face inherent challenges in integrating domain knowledge, limiting their performance in identifying lead compounds with novel and valid binding mode. Here, we propose AutoLeadDesign, a lead compounds design framework that inspires extensive domain knowledge encoded in large language models with chemical fragments to progressively implement efficient exploration of vast chemical space. The comprehensive experiments indicate that AutoLeadDesign outperforms baseline methods. Significantly, empirical lead design campaigns targeting two clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate AutoLeadDesign's competence in de novo generation of lead compounds achieving expert-competitive design efficacy. Structural analysis further confirms their mechanism-validated inhibitory patterns. By tracing the process of design, we find that AutoLeadDesign shares analogous mechanisms with fragment-based drug design which traditionally rely on the expert decision-making, further revealing why it works. Overall, AutoLeadDesign offers an efficient approach for lead compounds design, suggesting its potential utility in drug design.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2506.20598.pdf' target='_blank'>https://arxiv.org/pdf/2506.20598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander D. Kalian, Jaewook Lee, Stefan P. Johannesson, Lennart Otte, Christer Hogstrand, Miao Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20598">Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The global demand for sustainable protein sources has accelerated the need for intelligent tools that can rapidly process and synthesise domain-specific scientific knowledge. In this study, we present a proof-of-concept multi-agent Artificial Intelligence (AI) framework designed to support sustainable protein production research, with an initial focus on microbial protein sources. Our Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based LLM agents: (1) a literature search agent that retrieves relevant scientific literature on microbial protein production for a specified microbial strain, and (2) an information extraction agent that processes the retrieved content to extract relevant biological and chemical information. Two parallel methodologies, fine-tuning and prompt engineering, were explored for agent optimisation. Both methods demonstrated effectiveness at improving the performance of the information extraction agent in terms of transformer-based cosine similarity scores between obtained and ideal outputs. Mean cosine similarity scores were increased by up to 25%, while universally reaching mean scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved the mean scores to a greater extent (consistently of $\geq 0.94$) compared to prompt engineering, although lower statistical uncertainties were observed with the latter approach. A user interface was developed and published for enabling the use of the multi-agent AI system, alongside preliminary exploration of additional chemical safety-based search capabilities
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2506.15782.pdf' target='_blank'>https://arxiv.org/pdf/2506.15782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas BoullÃ©, Matthew J. Colbrook, Gustav Conradie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15782">Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven spectral analysis of Koopman operators is a powerful tool for understanding numerous real-world dynamical systems, from neuronal activity to variations in sea surface temperature. The Koopman operator acts on a function space and is most commonly studied on the space of square-integrable functions. However, defining it on a suitable reproducing kernel Hilbert space (RKHS) offers numerous practical advantages, including pointwise predictions with error bounds, improved spectral properties that facilitate computations, and more efficient algorithms, particularly in high dimensions. We introduce the first general, provably convergent, data-driven algorithms for computing spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These methods efficiently compute spectra and pseudospectra with error control and spectral measures while exploiting the RKHS structure to avoid the large-data limits required in the $L^2$ settings. The function space is determined by a user-specified kernel, eliminating the need for quadrature-based sampling as in $L^2$ and enabling greater flexibility with finite, externally provided datasets. Using the Solvability Complexity Index hierarchy, we construct adversarial dynamical systems for these problems to show that no algorithm can succeed in fewer limits, thereby proving the optimality of our algorithms. Notably, this impossibility extends to randomized algorithms and datasets. We demonstrate the effectiveness of our algorithms on challenging, high-dimensional datasets arising from real-world measurements and high-fidelity numerical simulations, including turbulent channel flow, molecular dynamics of a binding protein, Antarctic sea ice concentration, and Northern Hemisphere sea surface height. The algorithms are publicly available in the software package $\texttt{SpecRKHS}$.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2506.09496.pdf' target='_blank'>https://arxiv.org/pdf/2506.09496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingyi Rong, Haotian Lu, Wenzhuo Zheng, Fan Zhang, Shuangjia Zheng, Ning Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09496">EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein sequences with optimal energetic stability is a key challenge in protein inverse folding, as current deep learning methods are primarily trained by maximizing sequence recovery rates, often neglecting the energy of the generated sequences. This work aims to overcome this limitation by developing a model that directly generates low-energy, stable protein sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused on generating low-energy, high-stability protein sequences. Our core innovation lies in: First, integrating Markov Bridges with Direct Preference Optimization (DPO), where energy-based preferences are used to fine-tune the Markov Bridge model. The Markov Bridge initiates optimization from an information-rich prior sequence, providing DPO with a pool of structurally plausible sequence candidates. Second, an explicit energy constraint loss is introduced, which enhances the energy-driven nature of DPO based on prior sequences, enabling the model to effectively learn energy representations from a wealth of prior knowledge and directly predict sequence energy values, thereby capturing quantitative features of the energy landscape. Our evaluations demonstrate that EnerBridge-DPO can design protein complex sequences with lower energy while maintaining sequence recovery rates comparable to state-of-the-art models, and accurately predicts $ÎÎG$ values between various sequences.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2506.08954.pdf' target='_blank'>https://arxiv.org/pdf/2506.08954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruben Weitzman, Peter MÃ¸rch Groth, Lood Van Niekerk, Aoi Otani, Yarin Gal, Debora Marks, Pascal Notin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08954">Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieving homologous protein sequences is essential for a broad range of protein modeling tasks such as fitness prediction, protein design, structure modeling, and protein-protein interactions. Traditional workflows have relied on a two-step process: first retrieving homologs via Multiple Sequence Alignments (MSA), then training models on one or more of these alignments. However, MSA-based retrieval is computationally expensive, struggles with highly divergent sequences or complex insertions & deletions patterns, and operates independently of the downstream modeling objective. We introduce Protriever, an end-to-end differentiable framework that learns to retrieve relevant homologs while simultaneously training for the target task. When applied to protein fitness prediction, Protriever achieves state-of-the-art performance compared to sequence-based models that rely on MSA-based homolog retrieval, while being two orders of magnitude faster through efficient vector search. Protriever is both architecture- and task-agnostic, and can flexibly adapt to different retrieval strategies and protein databases at inference time -- offering a scalable alternative to alignment-centric approaches.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2506.06450.pdf' target='_blank'>https://arxiv.org/pdf/2506.06450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio JesÃºs Banegas-Luna, Baldomero ImbernÃ³n Tudela, Carlos MartÃ­nez-CortÃ©s, JosÃ© MarÃ­a Cecilia, Horacio PÃ©rez-SÃ¡nchez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06450">Performance Impact of Containerized METADOCK 2 on Heterogeneous Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual screening (VS) is a computationally intensive process crucial for drug discovery, often requiring significant resources to analyze large chemical libraries and predict ligand-protein interactions. This study evaluates the performance impact of containerization on METADOCK 2, a high-throughput docking software when deployed on heterogeneous high-performance computing (HPC) platforms. By testing three containerization technologies - Docker, Singularity, and Apptainer - across varying CPU and GPU configurations, the experiments reveal that containerization introduces negligible performance overhead, with deviations below 1%. Moreover, METADOCK 2 demonstrated the capability to efficiently process large molecular complexes, surpassing the limitations of commercial tools such as AutoDock Vina. The results underscore the advantages of container-based deployment for ensuring portability, reproducibility, and scalability in scientific computing. This study concludes that containerized METADOCK 2 is a robust and efficient solution for VS tasks on heterogeneous HPC platforms.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2505.23823.pdf' target='_blank'>https://arxiv.org/pdf/2505.23823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngseung Jeon, Ziwen Li, Thomas Li, JiaSyuan Chang, Morteza Ziyadi, Xiang 'Anthony' Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23823">RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieving the biological impacts of protein-protein interactions (PPIs) is essential for target identification (Target ID) in drug development. Given the vast number of proteins involved, this process remains time-consuming and challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have supported Target ID; however, no benchmark currently exists for identifying the biological impacts of PPIs. To bridge this gap, we introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer benchmark of 4,420 question-answer pairs that focus on the potential biological impacts of PPIs. Through interviews with experts, we identified criteria for a benchmark dataset, such as a type of QA and source. We built a gold-standard dataset (500 QA pairs) through expert-driven data annotation. We developed an ensemble auto-evaluation LLM that reflected expert labeling characteristics, which facilitates the construction of a silver-standard dataset (3,720 QA pairs). We are committed to maintaining RAGPPI as a resource to support the research community in advancing RAG systems for drug discovery QA solutions.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2505.20052.pdf' target='_blank'>https://arxiv.org/pdf/2505.20052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hazem Alsamkary, Mohamed Elshaffei, Mohamed Elkerdawy, Ahmed Elnaggar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20052">Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2505.20036.pdf' target='_blank'>https://arxiv.org/pdf/2505.20036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hazem Alsamkary, Mohamed Elshaffei, Mohamed Soudy, Sara Ossman, Abdallah Amr, Nehal Adel Abdelsalam, Mohamed Elkerdawy, Ahmed Elnaggar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20036">Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2505.18890.pdf' target='_blank'>https://arxiv.org/pdf/2505.18890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Morteza Rakhshaninejad, Mira Jurgens, Nicolas Dewolf, Willem Waegeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18890">Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate drug-target interaction (DTI) prediction with machine learning models is essential for drug discovery. Such models should also provide a credible representation of their uncertainty, but applying classical marginal conformal prediction (CP) in DTI prediction often overlooks variability across drug and protein subgroups. In this work, we analyze three cluster-conditioned CP methods for DTI prediction, and compare them with marginal and group-conditioned CP. Clusterings are obtained via nonconformity scores, feature similarity, and nearest neighbors, respectively. Experiments on the KIBA dataset using four data-splitting strategies show that nonconformity-based clustering yields the tightest intervals and most reliable subgroup coverage, especially in random and fully unseen drug-protein splits. Group-conditioned CP works well when one entity is familiar, but residual-driven clustering provides robust uncertainty estimates even in sparse or novel scenarios. These results highlight the potential of cluster-based CP for improving DTI prediction under uncertainty.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2505.18150.pdf' target='_blank'>https://arxiv.org/pdf/2505.18150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nic Fishman, Gokul Gowri, Peng Yin, Jonathan Gootenberg, Omar Abudayyeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18150">Generative Distribution Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2505.16896.pdf' target='_blank'>https://arxiv.org/pdf/2505.16896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Chen, David Heurtel-Depeiges, Robert M. Vernon, Christopher James Langmead, Yoshua Bengio, Quentin Fournier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16896">Structure-Aligned Protein Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but lack the structural knowledge essential for many biological applications. To address this, we integrate structural insights from pre-trained protein graph neural networks (pGNNs) into pLMs through a latent-level contrastive learning task. This task aligns residue representations from pLMs with those from pGNNs across multiple proteins, enriching pLMs with inter-protein structural knowledge. Additionally, we incorporate a physical-level task that infuses intra-protein structural knowledge by optimizing pLMs to predict structural tokens. The proposed dual-task framework effectively incorporates both inter-protein and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module, which uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method to the state-of-the-art ESM2 and AMPLIFY results in notable performance gains across a wide range of tasks, including a 12.7% increase in ESM2 contact prediction. The data, code, and resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2505.05893.pdf' target='_blank'>https://arxiv.org/pdf/2505.05893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunghee Han, Soongyu Choi, Joo-Young Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05893">LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Protein Structure Prediction Models (PPMs), such as AlphaFold2 and ESMFold, have revolutionized computational biology by achieving unprecedented accuracy in predicting three-dimensional protein folding structures. However, these models face significant scalability challenges, particularly when processing proteins with long amino acid sequences (e.g., sequence length > 1,000). The primary bottleneck that arises from the exponential growth in activation sizes is driven by the unique data structure in PPM, which introduces an additional dimension that leads to substantial memory and computational demands. These limitations have hindered the effective scaling of PPM for real-world applications, such as analyzing large proteins or complex multimers with critical biological and pharmaceutical relevance.
  In this paper, we present LightNobel, the first hardware-software co-designed accelerator developed to overcome scalability limitations on the sequence length in PPM. At the software level, we propose Token-wise Adaptive Activation Quantization (AAQ), which leverages unique token-wise characteristics, such as distogram patterns in PPM activations, to enable fine-grained quantization techniques without compromising accuracy. At the hardware level, LightNobel integrates the multi-precision reconfigurable matrix processing unit (RMPU) and versatile vector processing unit (VVPU) to enable the efficient execution of AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100 GPUs, respectively, while maintaining negligible accuracy loss. It also reduces the peak memory requirement up to 120.05x in PPM, enabling scalable processing for proteins with long sequences.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2504.11091.pdf' target='_blank'>https://arxiv.org/pdf/2504.11091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian G. Schuh, Joshua Hesse, Stephan A. Sieber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11091">AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2504.09365.pdf' target='_blank'>https://arxiv.org/pdf/2504.09365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aspen Erlandsson Brisebois, Jason Broderick, Zahed Khatooni, Heather L. Wilson, Steven Rayan, Gordon Broderick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09365">Identifying Protein Co-regulatory Network Logic by Solving B-SAT Problems through Gate-based Quantum Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is growing awareness that the success of pharmacologic interventions on living organisms is significantly impacted by context and timing of exposure. In turn, this complexity has led to an increased focus on regulatory network dynamics in biology and our ability to represent them in a high-fidelity way, in silico. Logic network models show great promise here and their parameter estimation can be formulated as a constraint satisfaction problem (CSP) that is well-suited to the often sparse, incomplete data in biology. Unfortunately, even in the case of Boolean logic, the combinatorial complexity of these problems grows rapidly, challenging the creation of models at physiologically-relevant scales. That said, quantum computing, while still nascent, facilitates novel information-processing paradigms with the potential for transformative impact in problems such as this one. In this work, we take a first step at actualizing this potential by identifying the structure and Boolean decisional logic of a well-studied network linking 5 proteins involved in the neural development of the mammalian cortical area of the brain. We identify the protein-protein connectivity and binary decisional logic governing this network by formulating it as a Boolean Satisfiability (B-SAT) problem. We employ Grover's algorithm to solve the NP-hard problem faster than the exponential time complexity required by deterministic classical algorithms. Using approaches deployed on both quantum simulators and actual noisy intermediate scale quantum (NISQ) hardware, we accurately recover several high-likelihood models from very sparse protein expression data. The results highlight the differential roles of data types in supporting accurate models; the impact of quantum algorithm design as it pertains to the mutability of quantum hardware; and the opportunities for accelerated discovery enabled by this approach.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2504.08437.pdf' target='_blank'>https://arxiv.org/pdf/2504.08437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neeru Dubey, Elin Karlsson, Miguel Angel Redondo, Johan ReimegÃ¥rd, Anna Rising, Hedvig KjellstrÃ¶m
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08437">Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characteristics and repeat sequences is challenging due to the intricate sequence-structure-function relationships of MaSps and the limited availability of annotated datasets. In this study, we present a novel computational framework for designing MaSp repeat sequences with customizable mechanical properties. To achieve this, we developed a lightweight GPT-based generative model by distilling the pre-trained ProtGPT2 protein language model. The distilled model was subjected to multilevel fine-tuning using curated subsets of the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat generation using 6,000 MaSp repeat sequences and further refine it with 572 repeats associated with experimentally determined fiber-level mechanical properties. Our model generates biologically plausible MaSp repeat regions tailored to specific mechanical properties while also predicting those properties for given sequences. Validation includes sequence-level analysis, assessing physicochemical attributes and expected distribution of key motifs as well as secondary structure compositions. A correlation study using BLAST on the Spider Silkome dataset and a test set of MaSp repeats with known mechanical properties further confirmed the predictive accuracy of the model. This framework advances the rational design of spider silk-inspired biomaterials, offering a versatile tool for engineering protein sequences with tailored mechanical attributes.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2504.05271.pdf' target='_blank'>https://arxiv.org/pdf/2504.05271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusef Ahsini, Marc Escoto, J. Alberto Conejero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05271">AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomalous diffusion occurs in a wide range of systems, including protein transport within cells, animal movement in complex habitats, pollutant dispersion in groundwater, and nanoparticle motion in synthetic materials. Accurately estimating the anomalous diffusion exponent and the diffusion coefficient from the particle trajectories is essential to distinguish between sub-diffusive, super-diffusive, or normal diffusion regimes. These estimates provide a deeper insight into the underlying dynamics of the system, facilitating the identification of particle behaviors and the detection of changes in diffusion states. However, analyzing short and noisy video data, which often yield incomplete and heterogeneous trajectories, poses a significant challenge for traditional statistical approaches. We introduce a data-driven method that integrates particle tracking, an attention
  U-Net architecture, and a change-point detection algorithm to address these issues. This approach not only infers the anomalous diffusion parameters with high accuracy but also identifies temporal transitions between different states, even in the presence of noise and limited temporal resolution. Our methodology demonstrated strong performance in the 2nd Anomalous Diffusion (AnDi) Challenge benchmark within the top submissions for video tasks.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2503.08160.pdf' target='_blank'>https://arxiv.org/pdf/2503.08160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taojie Kuang, Qianli Ma, Athanasios V. Vasilakos, Yu Wang, Qiang, Cheng, Zhixiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08160">Concept-Driven Deep Learning for Enhanced Protein-Specific Molecular Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, deep learning techniques have made significant strides in molecular generation for specific targets, driving advancements in drug discovery. However, existing molecular generation methods present significant limitations: those operating at the atomic level often lack synthetic feasibility, drug-likeness, and interpretability, while fragment-based approaches frequently overlook comprehensive factors that influence protein-molecule interactions. To address these challenges, we propose a novel fragment-based molecular generation framework tailored for specific proteins. Our method begins by constructing a protein subpocket and molecular arm concept-based neural network, which systematically integrates interaction force information and geometric complementarity to sample molecular arms for specific protein subpockets. Subsequently, we introduce a diffusion model to generate molecular backbones that connect these arms, ensuring structural integrity and chemical diversity. Our approach significantly improves synthetic feasibility and binding affinity, with a 4% increase in drug-likeness and a 6% improvement in synthetic feasibility. Furthermore, by integrating explicit interaction data through a concept-based model, our framework enhances interpretability, offering valuable insights into the molecular design process.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2503.05560.pdf' target='_blank'>https://arxiv.org/pdf/2503.05560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mirja Granfors, JesÃºs Pineda, Blanca Zufiria GerbolÃ©s, Joana B. Pereira, Carlo Manzo, Giovanni Volpe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05560">Global graph features unveiled by unsupervised geometric deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphs provide a powerful framework for modeling complex systems, but their structural variability poses significant challenges for analysis and classification. To address these challenges, we introduce GAUDI (Graph Autoencoder Uncovering Descriptive Information), a novel unsupervised geometric deep learning framework designed to capture both local details and global structure. GAUDI employs an innovative hourglass architecture with hierarchical pooling and upsampling layers linked through skip connections, which preserve essential connectivity information throughout the encoding-decoding process. Even though identical or highly similar underlying parameters describing a system's state can lead to significant variability in graph realizations, GAUDI consistently maps them into nearby regions of a structured and continuous latent space, effectively disentangling invariant process-level features from stochastic noise. We demonstrate GAUDI's versatility across multiple applications, including small-world networks modeling, characterization of protein assemblies from super-resolution microscopy, analysis of collective motion in the Vicsek model, and identification of age-related changes in brain connectivity. Comparison with related approaches highlights GAUDI's superior performance in analyzing complex graphs, providing new insights into emergent phenomena across diverse scientific domains.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2502.20050.pdf' target='_blank'>https://arxiv.org/pdf/2502.20050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Fang, Yihan He, Xiao Gong, Gengchiau Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20050">A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D Protein Folding Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the post-Moore era, the need for efficient solutions to non-deterministic polynomial-time (NP) problems is becoming more pressing. In this context, the Ising model implemented by the probabilistic computing systems with probabilistic bits (p-bits) has attracted attention due to the widespread availability of p-bits and support for large-scale simulations. This study marks the first work to apply probabilistic computing to tackle protein folding, a significant NP-complete problem challenge in biology. We represent proteins as sequences of hydrophobic (H) and polar (P) beads within a three-dimensional (3-D) grid and introduce a novel many-body interaction-based encoding method to map the problem onto an Ising model. Our simulations show that this approach significantly simplifies the energy landscape for short peptide sequences of six amino acids, halving the number of energy levels. Furthermore, the proposed mapping method achieves approximately 100 times acceleration for sequences consisting of ten amino acids in identifying the correct folding configuration. We predicted the optimal folding configuration for a peptide sequence of 36 amino acids by identifying the ground state. These findings highlight the unique potential of the proposed encoding method for solving protein folding and, importantly, provide new tools for solving similar NP-complete problems in biology by probabilistic computing approach.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2502.17189.pdf' target='_blank'>https://arxiv.org/pdf/2502.17189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Havrilla, David Alvarez-Melis, Nicolo Fusi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17189">IGDA: Interactive Graph Discovery through Large Language Model Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models ($\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2502.12565.pdf' target='_blank'>https://arxiv.org/pdf/2502.12565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hikaru Asano, Tadashi Kozuno, Yukino Baba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12565">Self Iterative Label Refinement via Robust Unlabeled Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1).
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2502.12479.pdf' target='_blank'>https://arxiv.org/pdf/2502.12479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoqi Zheng, Bo Zhang, Kieran Didi, Kevin K. Yang, Jason Yim, Joseph L. Watson, Hai-Feng Chen, Brian L. Trippe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12479">MotifBench: A standardized protein design benchmark for motif-scaffolding problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The motif-scaffolding problem is a central task in computational protein design: Given the coordinates of atoms in a geometry chosen to confer a desired biochemical function (a motif), the task is to identify diverse protein structures (scaffolds) that include the motif and maintain its geometry. Significant recent progress on motif-scaffolding has been made due to computational evaluation with reliable protein structure prediction and fixed-backbone sequence design methods. However, significant variability in evaluation strategies across publications has hindered comparability of results, challenged reproducibility, and impeded robust progress. In response we introduce MotifBench, comprising (1) a precisely specified pipeline and evaluation metrics, (2) a collection of 30 benchmark problems, and (3) an implementation of this benchmark and leaderboard at github.com/blt2114/MotifBench. The MotifBench test cases are more difficult compared to earlier benchmarks, and include protein design problems for which solutions are known but on which, to the best of our knowledge, state-of-the-art methods fail to identify any solution.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2502.10365.pdf' target='_blank'>https://arxiv.org/pdf/2502.10365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Chen, Karla-Luise Herpoldt, Chenchao Zhao, Zichen Wang, Marcus Collins, Shang Shang, Ron Benson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10365">AffinityFlow: Guided Flows for Antibody Affinity Maturation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity.This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an alternating optimization framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection. A key challenge is the lack of labeled data for training both predictors. To address this, we develop a co-teaching module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art performance in affinity maturation experiments. We plan to open-source our code after acceptance.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2502.10173.pdf' target='_blank'>https://arxiv.org/pdf/2502.10173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Ni, Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10173">Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2502.06914.pdf' target='_blank'>https://arxiv.org/pdf/2502.06914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenao Li, Shuo Yan, Enyan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06914">UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzyme-catalyzed protein cleavage is essential for many biological functions. Accurate prediction of cleavage sites can facilitate various applications such as drug development, enzyme design, and a deeper understanding of biological mechanisms. However, most existing models are restricted to an individual enzyme, which neglects shared knowledge of enzymes and fails generalize to novel enzymes. Thus, we introduce a unified protein cleavage site predictor named UniZyme, which can generalize across diverse enzymes. To enhance the enzyme encoding for the protein cleavage site prediction, UniZyme employs a novel biochemically-informed model architecture along with active-site knowledge of proteolytic enzymes. Extensive experiments demonstrate that UniZyme achieves high accuracy in predicting cleavage sites across a range of proteolytic enzymes, including unseen enzymes. The code is available in https://anonymous.4open.science/r/UniZyme-4A67.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2502.01533.pdf' target='_blank'>https://arxiv.org/pdf/2502.01533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Ellmen, Constantin Schneider, Matthew I. J. Raybould, Charlotte M. Deane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01533">Transformers trained on proteins can learn to attend to Euclidean distance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2501.18223.pdf' target='_blank'>https://arxiv.org/pdf/2501.18223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel F. Mollon, Joaquin Gonzalez-Rodriguez, Alicia Lozano-Diez, Daniel Ramos, Doroteo T. Toledano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18223">Exploring Large Protein Language Models in Constrained Evaluation Scenarios within the FLIP Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we expand upon the FLIP benchmark-designed for evaluating protein fitness prediction models in small, specialized prediction tasks-by assessing the performance of state-of-the-art large protein language models, including ESM-2 and SaProt on the FLIP dataset. Unlike larger, more diverse benchmarks such as ProteinGym, which cover a broad spectrum of tasks, FLIP focuses on constrained settings where data availability is limited. This makes it an ideal framework to evaluate model performance in scenarios with scarce task-specific data. We investigate whether recent advances in protein language models lead to significant improvements in such settings. Our findings provide valuable insights into the performance of large-scale models in specialized protein prediction tasks.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2501.16382.pdf' target='_blank'>https://arxiv.org/pdf/2501.16382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwen Li, Xiang 'Anthony' Chen, Youngseung Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16382">GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug discovery (DD) has tremendously contributed to maintaining and improving public health. Hypothesizing that inhibiting protein misfolding can slow disease progression, researchers focus on target identification (Target ID) to find protein structures for drug binding. While Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug discovery, integrating models into cohesive workflows remains challenging. We conducted a user study with drug discovery researchers to identify the applicability of LLMs and RAGs in Target ID. We identified two main findings: 1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on an initial protein and protein candidates that have a therapeutic impact; 2) the model must provide the PPI and relevant explanations for better understanding. Based on these observations, we identified three limitations in previous approaches for Target ID: 1) semantic ambiguity, 2) lack of explainability, and 3) short retrieval units. To address these issues, we propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve agent pipeline RAG framework to support large-scale PPI signaling pathway exploration in understanding therapeutic impacts by decomposing the analysis of entire PPI pathways into sub-tasks focused on the analysis of PPI edges.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2501.09938.pdf' target='_blank'>https://arxiv.org/pdf/2501.09938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sajjad Saleem, Adil Hussain, Nabila Majeed, Zahid Akhtar, Kamran Siddique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09938">A Multi-Scale Feature Extraction and Fusion Deep Learning Method for Classification of Wheat Diseases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheat is an important source of dietary fiber and protein that is negatively impacted by a number of risks to its growth. The difficulty of identifying and classifying wheat diseases is discussed with an emphasis on wheat loose smut, leaf rust, and crown and root rot. Addressing conditions like crown and root rot, this study introduces an innovative approach that integrates multi-scale feature extraction with advanced image segmentation techniques to enhance classification accuracy. The proposed method uses neural network models Xception, Inception V3, and ResNet 50 to train on a large wheat disease classification dataset 2020 in conjunction with an ensemble of machine vision classifiers, including voting and stacking. The study shows that the suggested methodology has a superior accuracy of 99.75% in the classification of wheat diseases when compared to current state-of-the-art approaches. A deep learning ensemble model Xception showed the highest accuracy.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2412.20744.pdf' target='_blank'>https://arxiv.org/pdf/2412.20744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Roy, Bhavesh Gyanchandani, Aditya Oza, Abhishek Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20744">Advancing Parkinson's Disease Progression Prediction: Comparing Long Short-Term Memory Networks and Kolmogorov-Arnold Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parkinson's Disease (PD) is a degenerative neurological disorder that impairs motor and non-motor functions, significantly reducing quality of life and increasing mortality risk. Early and accurate detection of PD progression is vital for effective management and improved patient outcomes. Current diagnostic methods, however, are often costly, time-consuming, and require specialized equipment and expertise. This work proposes an innovative approach to predicting PD progression using regression methods, Long Short-Term Memory (LSTM) networks, and Kolmogorov Arnold Networks (KAN). KAN, utilizing spline-parametrized univariate functions, allows for dynamic learning of activation patterns, unlike traditional linear models.
  The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS) is a comprehensive tool for evaluating PD symptoms and is commonly used to measure disease progression. Additionally, protein or peptide abnormalities are linked to PD onset and progression. Identifying these associations can aid in predicting disease progression and understanding molecular changes.
  Comparing multiple models, including LSTM and KAN, this study aims to identify the method that delivers the highest metrics. The analysis reveals that KAN, with its dynamic learning capabilities, outperforms other approaches in predicting PD progression. This research highlights the potential of AI and machine learning in healthcare, paving the way for advanced computational models to enhance clinical predictions and improve patient care and treatment strategies in PD management.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2412.13223.pdf' target='_blank'>https://arxiv.org/pdf/2412.13223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Advaith Maddipatla, Nadav Bojan Sellam, Sanketh Vedula, Ailie Marx, Alex Bronstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13223">Generative modeling of protein ensembles guided by crystallographic electron densities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are dynamic, adopting ensembles of conformations. The nature of this conformational heterogenity is imprinted in the raw electron density measurements obtained from X-ray crystallography experiments. Fitting an ensemble of protein structures to these measurements is a challenging, ill-posed inverse problem. We propose a non-i.i.d. ensemble guidance approach to solve this problem using existing protein structure generative models and demonstrate that it accurately recovers complicated multi-modal alternate protein backbone conformations observed in certain single crystal measurements.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2412.10743.pdf' target='_blank'>https://arxiv.org/pdf/2412.10743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Qiao, Feizhi Ding, Thomas Dresselhaus, Mia A. Rosenfeld, Xiaotian Han, Owen Howell, Aniketh Iyengar, Stephen Opalenski, Anders S. Christensen, Sai Krishna Sirumalla, Frederick R. Manby, Thomas F. Miller, Matthew Welborn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10743">NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through newly developed benchmarking strategies, NeuralPLexer3 excels in vital areas that are crucial to structure-based drug design, such as physical validity and ligand-induced conformational changes.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2411.15418.pdf' target='_blank'>https://arxiv.org/pdf/2411.15418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew T. McNutt, Abhinav K. Adduri, Caleb N. Ellington, Monica T. Dayao, Eric P. Xing, Hosein Mohimani, David R. Koes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15418">Scaling Structure Aware Virtual Screening to Billions of Molecules with SPRINT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Virtual screening of small molecules against protein targets can accelerate drug discovery and development by predicting drug-target interactions (DTIs). However, structure-based methods like molecular docking are too slow to allow for broad proteome-scale screens, limiting their application in screening for off-target effects or new molecular mechanisms. Recently, vector-based methods using protein language models (PLMs) have emerged as a complementary approach that bypasses explicit 3D structure modeling. Here, we develop SPRINT, a vector-based approach for screening entire chemical libraries against whole proteomes for DTIs and novel mechanisms of action. SPRINT improves on prior work by using a self-attention based architecture and structure-aware PLMs to learn drug-target co-embeddings for binder prediction, search, and retrieval. SPRINT achieves SOTA enrichment factors in virtual screening on LIT-PCBA, DTI classification benchmarks, and binding affinity prediction benchmarks, while providing interpretability in the form of residue-level attention maps. In addition to being both accurate and interpretable, SPRINT is ultra-fast: querying the whole human proteome against the ENAMINE Real Database (6.7B drugs) for the 100 most likely binders per protein takes 16 minutes. SPRINT promises to enable virtual screening at an unprecedented scale, opening up new opportunities for in silico drug repurposing and development. SPRINT is available on the web as ColabScreen: https://bit.ly/colab-screen
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2411.12098.pdf' target='_blank'>https://arxiv.org/pdf/2411.12098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Gagan Agrawal, Rajiv Ramnath, Ruoming Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12098">Federated Contrastive Learning of Graph-Level Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph-level representations (and clustering/classification based on these representations) are required in a variety of applications. Examples include identifying malicious network traffic, prediction of protein properties, and many others. Often, data has to stay in isolated local systems (i.e., cannot be centrally shared for analysis) due to a variety of considerations like privacy concerns, lack of trust between the parties, regulations, or simply because the data is too large to be shared sufficiently quickly. This points to the need for federated learning for graph-level representations, a topic that has not been explored much, especially in an unsupervised setting.
  Addressing this problem, this paper presents a new framework we refer to as Federated Contrastive Learning of Graph-level Representations (FCLG). As the name suggests, our approach builds on contrastive learning. However, what is unique is that we apply contrastive learning at two levels. The first application is for local unsupervised learning of graph representations. The second level is to address the challenge associated with data distribution variation (i.e. the ``Non-IID issue") when combining local models. Through extensive experiments on the downstream task of graph-level clustering, we demonstrate FCLG outperforms baselines (which apply existing federated methods on existing graph-level clustering methods) with significant margins.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2411.08992.pdf' target='_blank'>https://arxiv.org/pdf/2411.08992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdurahman Ali Mohammed, Catherine Fonder, Donald S. Sakaguchi, Wallapak Tavanapong, Surya K. Mallapragada, Azeez Idris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08992">IDCIA: Immunocytochemistry Dataset for Cellular Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new annotated microscopic cellular image dataset to improve the effectiveness of machine learning methods for cellular image analysis. Cell counting is an important step in cell analysis. Typically, domain experts manually count cells in a microscopic image. Automated cell counting can potentially eliminate this tedious, time-consuming process. However, a good, labeled dataset is required for training an accurate machine learning model. Our dataset includes microscopic images of cells, and for each image, the cell count and the location of individual cells. The data were collected as part of an ongoing study investigating the potential of electrical stimulation to modulate stem cell differentiation and possible applications for neural repair. Compared to existing publicly available datasets, our dataset has more images of cells stained with more variety of antibodies (protein components of immune responses against invaders) typically used for cell analysis. The experimental results on this dataset indicate that none of the five existing models under this study are able to achieve sufficiently accurate count to replace the manual methods. The dataset is available at https://figshare.com/articles/dataset/Dataset/21970604.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2411.05055.pdf' target='_blank'>https://arxiv.org/pdf/2411.05055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youssef Boulaimen, Gabriele Fossi, Leila Outemzabet, Nathalie Jeanray, Oleksandr Levenets, Stephane Gerart, Sebastien Vachenc, Salvatore Raieli, Joanna Giemza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05055">Integrating Large Language Models for Genetic Variant Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The classification of genetic variants, particularly Variants of Uncertain Significance (VUS), poses a significant challenge in clinical genetics and precision medicine. Large Language Models (LLMs) have emerged as transformative tools in this realm. These models can uncover intricate patterns and predictive insights that traditional methods might miss, thus enhancing the predictive accuracy of genetic variant pathogenicity.
  This study investigates the integration of state-of-the-art LLMs, including GPN-MSA, ESM1b, and AlphaMissense, which leverage DNA and protein sequence data alongside structural insights to form a comprehensive analytical framework for variant classification. Our approach evaluates these integrated models using the well-annotated ProteinGym and ClinVar datasets, setting new benchmarks in classification performance. The models were rigorously tested on a set of challenging variants, demonstrating substantial improvements over existing state-of-the-art tools, especially in handling ambiguous and clinically uncertain variants.
  The results of this research underline the efficacy of combining multiple modeling approaches to significantly refine the accuracy and reliability of genetic variant classification systems. These findings support the deployment of these advanced computational models in clinical environments, where they can significantly enhance the diagnostic processes for genetic disorders, ultimately pushing the boundaries of personalized medicine by offering more detailed and actionable genetic insights.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2411.00593.pdf' target='_blank'>https://arxiv.org/pdf/2411.00593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhili Feng, Tanya Marwah, Nicolo Fusi, David Alvarez-Melis, Lester Mackey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00593">Adapting Language Models via Token Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern large language models use a fixed tokenizer to effectively compress text drawn from a source domain. However, applying the same tokenizer to a new target domain often leads to inferior compression, more costly inference, and reduced semantic alignment. To address this deficiency, we introduce Sparse Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the target domain and learns to translate between target and source tokens, enabling more effective reuse of the pre-trained next-source-token predictor. In our experiments with finetuned English language models, S2T2 improves both the perplexity and the compression of out-of-domain protein sequences, outperforming direct finetuning with either the source or target tokenizer. In addition, we find that token translations learned for smaller, less expensive models can be directly transferred to larger, more powerful models to reap the benefits of S2T2 at lower cost.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2410.19704.pdf' target='_blank'>https://arxiv.org/pdf/2410.19704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parthasarathy Suryanarayanan, Yunguang Qiu, Shreyans Sethi, Diwakar Mahajan, Hongyang Li, Yuxin Yang, Elif Eyigoz, Aldo Guzman Saenz, Daniel E. Platt, Timothy H. Rumbell, Kenney Ng, Sanjoy Dey, Myson Burch, Bum Chul Kwon, Pablo Meyer, Feixiong Cheng, Jianying Hu, Joseph A. Morrone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19704">Multi-view biomedical foundation models for molecule-target and property prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quality molecular representations are key to foundation model development in bio-medical research. Previous efforts have typically focused on a single representation or molecular view, which may have strengths or weaknesses on a given task. We develop Multi-view Molecular Embedding with Late Fusion (MMELON), an approach that integrates graph, image and text views in a foundation model setting and may be readily extended to additional representations. Single-view foundation models are each pre-trained on a dataset of up to 200M molecules. The multi-view model performs robustly, matching the performance of the highest-ranked single-view. It is validated on over 120 tasks, including molecular solubility, ADME properties, and activity against G Protein-Coupled receptors (GPCRs). We identify 33 GPCRs that are related to Alzheimer's disease and employ the multi-view model to select strong binders from a compound screen. Predictions are validated through structure-based modeling and identification of key binding motifs.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2410.14433.pdf' target='_blank'>https://arxiv.org/pdf/2410.14433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rabea Khatun, Wahia Tasnim, Maksuda Akter, Md Manowarul Islam, Md. Ashraf Uddin, Md. Zulfiker Mahmud, Saurav Chandra Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14433">A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gallbladder cancer (GBC) is the most frequent cause of disease among biliary tract neoplasms. Identifying the molecular mechanisms and biomarkers linked to GBC progression has been a significant challenge in scientific research. Few recent studies have explored the roles of biomarkers in GBC. Our study aimed to identify biomarkers in GBC using machine learning (ML) and bioinformatics techniques. We compared GBC tumor samples with normal samples to identify differentially expressed genes (DEGs) from two microarray datasets (GSE100363, GSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found, with 39 up-regulated and 107 down-regulated genes. Functional enrichment analysis of these DEGs was performed using Gene Ontology (GO) terms and REACTOME pathways through DAVID. The protein-protein interaction network was constructed using the STRING database. To identify hub genes, we applied three ranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of hub genes from these algorithms yielded 11 hub genes. Simultaneously, two feature selection methods (Pearson correlation and recursive feature elimination) were used to identify significant gene subsets. We then developed ML models using SVM and RF on the GSE100363 dataset, with validation on GSE139682, to determine the gene subset that best distinguishes GBC samples. The hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1, SCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were identified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly linked to GBC development and prediction.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2410.08631.pdf' target='_blank'>https://arxiv.org/pdf/2410.08631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhou, Yilai Li, Jing Yuan, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08631">CryoFM: A Flow-based Foundation Model for Cryo-EM Densities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryo-electron microscopy (cryo-EM) is a powerful technique in structural biology and drug discovery, enabling the study of biomolecules at high resolution. Significant advancements by structural biologists using cryo-EM have led to the production of over 38,626 protein density maps at various resolutions1. However, cryo-EM data processing algorithms have yet to fully benefit from our knowledge of biomolecular density maps, with only a few recent models being data-driven but limited to specific tasks. In this study, we present CryoFM, a foundation model designed as a generative model, learning the distribution of high-quality density maps and generalizing effectively to downstream tasks. Built on flow matching, CryoFM is trained to accurately capture the prior distribution of biomolecular density maps. Furthermore, we introduce a flow posterior sampling method that leverages CRYOFM as a flexible prior for several downstream tasks in cryo-EM and cryo-electron tomography (cryo-ET) without the need for fine-tuning, achieving state-of-the-art performance on most tasks and demonstrating its potential as a foundational model for broader applications in these fields.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2410.08203.pdf' target='_blank'>https://arxiv.org/pdf/2410.08203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Anosova, Alexey Gorelov, William Jeffcott, Ziqiu Jiang, Vitaliy Kurlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08203">Complete and bi-continuous invariant of protein backbones under rigid motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are large biomolecules that regulate all living organisms and consist of one or several chains. The primary structure of a protein chain is a sequence of amino acid residues whose three main atoms (alpha-carbon, nitrogen, and carbonyl carbon) form a protein backbone. The tertiary structure is the rigid shape of a protein chain represented by atomic positions in 3-dimensional space. Because different geometric structures often have distinct functional properties, it is important to continuously quantify differences in rigid shapes of protein backbones. Unfortunately, many widely used similarities of proteins fail axioms of a distance metric and discontinuously change under tiny perturbations of atoms.
  This paper develops a complete invariant that identifies any protein backbone in 3-dimensional space, uniquely under rigid motion. This invariant is Lipschitz bi-continuous in the sense that it changes up to a constant multiple of a maximum perturbation of atoms, and vice versa. The new invariant has been used to detect thousands of (near-)duplicates in the Protein Data Bank, whose presence inevitably skews machine learning predictions. The resulting invariant space allows low-dimensional maps with analytically defined coordinates that reveal substantial variability in the protein universe.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2410.04543.pdf' target='_blank'>https://arxiv.org/pdf/2410.04543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Friso de Kruiff, Erik Bekkers, Ozan Ãktem, Carola-Bibiane SchÃ¶nlieb, Willem Diepeveen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04543">Pullback Flow Matching on Data Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds. Unlike existing methods that assume or learn restrictive closed-form manifold mappings for training Riemannian Flow Matching (RFM) models, PFM leverages pullback geometry and isometric learning to preserve the underlying manifold's geometry while enabling efficient generation and precise interpolation in latent space. This approach not only facilitates closed-form mappings on the data manifold but also allows for designable latent spaces, using assumed metrics on both data and latent manifolds. By enhancing isometric learning through Neural ODEs and proposing a scalable training objective, we achieve a latent space more suitable for interpolation, leading to improved manifold learning and generative performance. We demonstrate PFM's effectiveness through applications in synthetic data, protein dynamics and protein sequence data, generating novel proteins with specific properties. This method shows strong potential for drug discovery and materials science, where generating novel samples with specific properties is of great interest.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2409.17852.pdf' target='_blank'>https://arxiv.org/pdf/2409.17852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Mirarchi, Raul P. Pelaez, Guillem Simeon, Gianni De Fabritiis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17852">AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>All-atom molecular simulations offer detailed insights into macromolecular phenomena, but their substantial computational cost hinders the exploration of complex biological processes. We introduce Advanced Machine-learning Atomic Representation Omni-force-field (AMARO), a new neural network potential (NNP) that combines an O(3)-equivariant message-passing neural network architecture, TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMARO demonstrates the feasibility of training coarser NNP, without prior energy terms, to run stable protein dynamics with scalability and generalization capabilities.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2409.07189.pdf' target='_blank'>https://arxiv.org/pdf/2409.07189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Dhouioui, Jonathan Barnoud, Rhoslyn Roebuck Williams, Harry J. Stroud, Phil Bates, David R. Glowacki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07189">A Perspective on AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently been developed as a 'human-in-the-loop' strategy, which leverages high-performance computing to accelerate the researcher's ability to solve the hyperdimensional sampling problem. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular motion, iMD-VR enables researchers and students to efficiently and intuitively explore and navigate these complex, high-dimensional systems. iMD-VR platforms offer a unique opportunity to quickly generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the possibility of employing user-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL is an important technique in robotics that enables agents to mimic complex behaviors from expert demonstrations, thus circumventing the need for explicit programming or intricate reward design. We review the utilization of IL for manipulation tasks in robotics and discuss how iMD-VR recordings could be used to train IL models for solving specific molecular 'tasks'. We then investigate how such approaches could be applied to the data captured from iMD-VR recordings. Finally, we outline the future research directions and potential challenges of using AI agents to augment human expertise to efficiently navigate conformational spaces, highlighting how this approach could provide valuable insight across domains such as materials science, protein engineering, and computer-aided drug design.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2409.06722.pdf' target='_blank'>https://arxiv.org/pdf/2409.06722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Jiao, Hananeh Derakhshan, Barbara St. Pierre Schneider, Emma Regentova, Mei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06722">Automated Quantification of White Blood Cells in Light Microscopic Images of Injured Skeletal Muscle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>White blood cells (WBCs) are the most diverse cell types observed in the healing process of injured skeletal muscles. In the course of healing, WBCs exhibit dynamic cellular response and undergo multiple protein expression changes. The progress of healing can be analyzed by quantifying the number of WBCs or the amount of specific proteins in light microscopic images obtained at different time points after injury. In this paper, we propose an automated quantifying and analysis framework to analyze WBCs using light microscopic images of uninjured and injured muscles. The proposed framework is based on the Localized Iterative Otsu's threshold method with muscle edge detection and region of interest extraction. Compared with the threshold methods used in ImageJ, the LI Otsu's threshold method has high resistance to background area and achieves better accuracy. The CD68-positive cell results are presented for demonstrating the effectiveness of the proposed work.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2409.02732.pdf' target='_blank'>https://arxiv.org/pdf/2409.02732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokul Gowri, Xiao-Kang Lun, Allon M. Klein, Peng Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02732">Approximating mutual information of high-dimensional variables using learned representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual information (MI) is a general measure of statistical dependence with widespread application across the sciences. However, estimating MI between multi-dimensional variables is challenging because the number of samples necessary to converge to an accurate estimate scales unfavorably with dimensionality. In practice, existing techniques can reliably estimate MI in up to tens of dimensions, but fail in higher dimensions, where sufficient sample sizes are infeasible. Here, we explore the idea that underlying low-dimensional structure in high-dimensional data can be exploited to faithfully approximate MI in high-dimensional settings with realistic sample sizes. We develop a method that we call latent MI (LMI) approximation, which applies a nonparametric MI estimator to low-dimensional representations learned by a simple, theoretically-motivated model architecture. Using several benchmarks, we show that unlike existing techniques, LMI can approximate MI well for variables with $> 10^3$ dimensions if their dependence structure has low intrinsic dimensionality. Finally, we showcase LMI on two open problems in biology. First, we approximate MI between protein language model (pLM) representations of interacting proteins, and find that pLMs encode non-trivial information about protein-protein interactions. Second, we quantify cell fate information contained in single-cell RNA-seq (scRNA-seq) measurements of hematopoietic stem cells, and find a sharp transition during neutrophil differentiation when fate information captured by scRNA-seq increases dramatically.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2408.09048.pdf' target='_blank'>https://arxiv.org/pdf/2408.09048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honggen Zhang, Xiangrui Gao, June Zhang, Lipeng Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09048">mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Messenger RNA (mRNA)-based vaccines are accelerating the discovery of new drugs and revolutionizing the pharmaceutical industry. However, selecting particular mRNA sequences for vaccines and therapeutics from extensive mRNA libraries is costly. Effective mRNA therapeutics require carefully designed sequences with optimized expression levels and stability. This paper proposes a novel contextual language model (LM)-based embedding method: mRNA2vec. In contrast to existing mRNA embedding approaches, our method is based on the self-supervised teacher-student learning framework of data2vec. We jointly use the 5' untranslated region (UTR) and coding sequence (CDS) region as the input sequences. We adapt our LM-based approach specifically to mRNA by 1) considering the importance of location on the mRNA sequence with probabilistic masking, 2) using Minimum Free Energy (MFE) prediction and Secondary Structure (SS) classification as additional pretext tasks. mRNA2vec demonstrates significant improvements in translation efficiency (TE) and expression level (EL) prediction tasks in UTR compared to SOTA methods such as UTR-LM. It also gives a competitive performance in mRNA stability and protein production level tasks in CDS such as CodonBERT.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2408.00040.pdf' target='_blank'>https://arxiv.org/pdf/2408.00040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian G. Schuh, Davide Boldini, Annkathrin I. Bohne, Stephan A. Sieber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00040">Barlow Twins Deep Neural Network for Advanced 1D Drug-Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of drug-target interactions is critical for advancing drug discovery. By reducing time and cost, machine learning and deep learning can accelerate this laborious discovery process. In a novel approach, BarlowDTI, we utilise the powerful Barlow Twins architecture for feature-extraction while considering the structure of the target protein. Our method achieves state-of-the-art predictive performance against multiple established benchmarks using only one-dimensional input. The use of gradient boosting machine as the underlying predictor ensures fast and efficient predictions without the need for substantial computational resources. We also investigate how the model reaches its decision based on individual training samples. By comparing co-crystal structures, we find that BarlowDTI effectively exploits catalytically active and stabilising residues, highlighting the model's ability to generalise from one-dimensional input data. In addition, we further benchmark new baselines against existing methods. Together, these innovations improve the efficiency and effectiveness of drug-target interaction predictions, providing robust tools for accelerating drug development and deepening the understanding of molecular interactions. Therefore, we provide an easy-to-use web interface that can be freely accessed at https://www.bio.nat.tum.de/oc2/barlowdti .
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2407.20054.pdf' target='_blank'>https://arxiv.org/pdf/2407.20054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filip OpÃ¡lenÃ½, Pavol Ulbrich, Joan Planas-Iglesias, Jan ByÅ¡ka, Jan Å touraÄ, David BednÃ¡Å, KatarÃ­na FurmanovÃ¡, Barbora KozlÃ­kovÃ¡
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20054">Visual Support for the Loop Grafting Workflow on Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In understanding and redesigning the function of proteins in modern biochemistry, protein engineers are increasingly focusing on exploring regions in proteins called loops. Analyzing various characteristics of these regions helps the experts design the transfer of the desired function from one protein to another. This process is denoted as loop grafting. We designed a set of interactive visualizations that provide experts with visual support through all the loop grafting pipeline steps. The workflow is divided into several phases, reflecting the steps of the pipeline. Each phase is supported by a specific set of abstracted 2D visual representations of proteins and their loops that are interactively linked with the 3D View of proteins. By sequentially passing through the individual phases, the user shapes the list of loops that are potential candidates for loop grafting. Finally, the actual in-silico insertion of the loop candidates from one protein to the other is performed, and the results are visually presented to the user. In this way, the fully computational rational design of proteins and their loops results in newly designed protein structures that can be further assembled and tested through in-vitro experiments. We showcase the contribution of our visual support design on a real case scenario changing the enantiomer selectivity of the engineered enzyme. Moreover, we provide the readers with the experts' feedback.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2407.03154.pdf' target='_blank'>https://arxiv.org/pdf/2407.03154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jithendaraa Subramanian, Shivakanth Sujit, Niloy Irtisam, Umong Sain, Riashat Islam, Derek Nowrouzezahrai, Samira Ebrahimi Kahou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03154">Reinforcement Learning for Sequence Design Leveraging Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein sequence design, determined by amino acid sequences, are essential to protein engineering problems in drug discovery. Prior approaches have resorted to evolutionary strategies or Monte-Carlo methods for protein design, but often fail to exploit the structure of the combinatorial search space, to generalize to unseen sequences. In the context of discrete black box optimization over large search spaces, learning a mutation policy to generate novel sequences with reinforcement learning is appealing. Recent advances in protein language models (PLMs) trained on large corpora of protein sequences offer a potential solution to this problem by scoring proteins according to their biological plausibility (such as the TM-score). In this work, we propose to use PLMs as a reward function to generate new sequences. Yet the PLM can be computationally expensive to query due to its large size. To this end, we propose an alternative paradigm where optimization can be performed on scores from a smaller proxy model that is periodically finetuned, jointly while learning the mutation policy. We perform extensive experiments on various sequence lengths to benchmark RL-based approaches, and provide comprehensive evaluations along biological plausibility and diversity of the protein. Our experimental results include favorable evaluations of the proposed sequences, along with high diversity scores, demonstrating that RL is a strong candidate for biological sequence design. Finally, we provide a modular open source implementation can be easily integrated in most RL training loops, with support for replacing the reward model with other PLMs, to spur further research in this domain. The code for all experiments is provided in the supplementary material.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2406.16995.pdf' target='_blank'>https://arxiv.org/pdf/2406.16995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Fang, Chenpeng Yu, Shiye Tian, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16995">tcrLM: a lightweight protein language model for predicting T cell receptor and epitope binding specificity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The anti-cancer immune response relies on the bindings between T-cell receptors (TCRs) and antigens, which elicits adaptive immunity to eliminate tumor cells. This ability of the immune system to respond to novel various neoantigens arises from the immense diversity of TCR repository. However, TCR diversity poses a significant challenge on accurately predicting antigen-TCR bindings. In this study, we introduce a lightweight masked language model, termed tcrLM, to address this challenge. Our approach involves randomly masking segments of TCR sequences and training tcrLM to infer the masked segments, thereby enabling the extraction of expressive features from TCR sequences. To further enhance robustness, we incorporate virtual adversarial training into tcrLM. We construct the largest TCR CDR3 sequence set with more than 100 million distinct sequences, and pretrain tcrLM on these sequences. The pre-trained encoder is subsequently applied to predict TCR-antigen binding specificity. We evaluate model performance on three test datasets: independent, external, and COVID-19 test set. The results demonstrate that tcrLM not only surpasses existing TCR-antigen binding prediction methods, but also outperforms other mainstream protein language models. More interestingly, tcrLM effectively captures the biochemical properties and positional preference of amino acids within TCR sequences. Additionally, the predicted TCR-neoantigen binding scores indicates the immunotherapy responses and clinical outcomes in a melanoma cohort. These findings demonstrate the potential of tcrLM in predicting TCR-antigen binding specificity, with significant implications for advancing immunotherapy and personalized medicine.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2406.06397.pdf' target='_blank'>https://arxiv.org/pdf/2406.06397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuta Nagano, Andrew Pyo, Martina Milighetti, James Henderson, John Shawe-Taylor, Benny Chain, Andreas Tiffeau-Mayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06397">Contrastive learning of T cell receptor representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational prediction of the interaction of T cell receptors (TCRs) and their ligands is a grand challenge in immunology. Despite advances in high-throughput assays, specificity-labelled TCR data remains sparse. In other domains, the pre-training of language models on unlabelled data has been successfully used to address data bottlenecks. However, it is unclear how to best pre-train protein language models for TCR specificity prediction. Here we introduce a TCR language model called SCEPTR (Simple Contrastive Embedding of the Primary sequence of T cell Receptors), capable of data-efficient transfer learning. Through our model, we introduce a novel pre-training strategy combining autocontrastive learning and masked-language modelling, which enables SCEPTR to achieve its state-of-the-art performance. In contrast, existing protein language models and a variant of SCEPTR pre-trained without autocontrastive learning are outperformed by sequence alignment-based methods. We anticipate that contrastive learning will be a useful paradigm to decode the rules of TCR specificity.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2405.16796.pdf' target='_blank'>https://arxiv.org/pdf/2405.16796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostofa Rafid Uddin, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16796">DualContrast: Unsupervised Disentangling of Content and Transformations with Implicit Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised disentanglement of content and transformation is significantly important for analyzing shape-focused scientific image datasets, given their efficacy in solving downstream image-based shape-analyses tasks. The existing relevant works address the problem by explicitly parameterizing the transformation latent codes in a generative model, significantly reducing their expressiveness. Moreover, they are not applicable in cases where transformations can not be readily parametrized. An alternative to such explicit approaches is contrastive methods with data augmentation, which implicitly disentangles transformations and content. However, the existing contrastive strategies are insufficient to this end. Therefore, we developed a novel contrastive method with generative modeling, DualContrast, specifically for unsupervised disentanglement of content and transformations in shape-focused image datasets. DualContrast creates positive and negative pairs for content and transformation from data and latent spaces. Our extensive experiments showcase the efficacy of DualContrast over existing self-supervised and explicit parameterization approaches. With DualContrast, we disentangled protein composition and conformations in cellular 3D protein images, which was unattainable with existing disentanglement approaches
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2405.06657.pdf' target='_blank'>https://arxiv.org/pdf/2405.06657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emanuele Triuzzi, Riccardo Mengoni, Francesco Micucci, Domenico Bonanni, Daniele Ottaviani, Andrea Beccari, Gianluca Palermo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06657">Molecular Docking via Weighted Subgraph Isomorphism on Quantum Annealers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is an essential step in the drug discovery process involving the detection of three-dimensional poses of a ligand inside the active site of the protein. In this paper, we address the Molecular Docking search phase by formulating the problem in QUBO terms, suitable for an annealing approach. We propose a problem formulation as a weighted subgraph isomorphism between the ligand graph and the grid of the target protein pocket. In particular, we applied a graph representation to the ligand embedding all the geometrical properties of the molecule including its flexibility, and we created a weighted spatial grid to the 3D space region inside the pocket. Results and performance obtained with quantum annealers are compared with classical simulated annealing solvers.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2404.17041.pdf' target='_blank'>https://arxiv.org/pdf/2404.17041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adith Jeyasangar, Abdullah Alsalemi, Shan E Ahmed Raza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17041">Nuclei-Location Based Point Set Registration of Multi-Stained Whole Slide Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole Slide Images (WSIs) provide exceptional detail for studying tissue architecture at the cell level. To study tumour microenvironment (TME) with the context of various protein biomarkers and cell sub-types, analysis and registration of features using multi-stained WSIs is often required. Multi-stained WSI pairs normally suffer from rigid and non-rigid deformities in addition to slide artefacts and control tissue which present challenges at precise registration. Traditional registration methods mainly focus on global rigid/non-rigid registration but struggle with aligning slides with complex tissue deformations at the nuclei level. However, nuclei level non-rigid registration is essential for downstream tasks such as cell sub-type analysis in the context of protein biomarker signatures. This paper focuses on local level non-rigid registration using a nuclei-location based point set registration approach for aligning multi-stained WSIs. We exploit the spatial distribution of nuclei that is prominent and consistent (to a large level) across different stains to establish a spatial correspondence. We evaluate our approach using the HYRECO dataset consisting of 54 re-stained images of H\&E and PHH3 image pairs. The approach can be extended to other IHC and IF stained WSIs considering a good nuclei detection algorithm is accessible. The performance of the model is tested against established registration algorithms and is shown to outperform the model for nuclei level registration.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2404.12565.pdf' target='_blank'>https://arxiv.org/pdf/2404.12565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Henderson, Yuta Nagano, Martina Milighetti, Andreas Tiffeau-Mayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12565">Limits on Inferring T-cell Specificity from Partial Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in molecular biology is to decipher the mapping of protein sequence to function. To perform this mapping requires the identification of sequence features most informative about function. Here, we quantify the amount of information (in bits) that T-cell receptor (TCR) sequence features provide about antigen specificity. We identify informative features by their degree of conservation among antigen-specific receptors relative to null expectations. We find that TCR specificity synergistically depends on the hypervariable regions of both receptor chains, with a degree of synergy that strongly depends on the ligand. Using a coincidence-based approach to measuring information enables us to directly bound the accuracy with which TCR specificity can be predicted from partial matches to reference sequences. We anticipate that our statistical framework will be of use for developing machine learning models for TCR specificity prediction and for optimizing TCRs for cell therapies. The proposed coincidence-based information measures might find further applications in bounding the performance of pairwise classifiers in other fields.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2404.10573.pdf' target='_blank'>https://arxiv.org/pdf/2404.10573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijun Liu, Jiali Yang, Jianfei Song, Xinglin Yang, Lele Niu, Zeqi Cai, Hui Shi, Tingjun Hou, Chang-yu Hsieh, Weiran Shen, Yafeng Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10573">AAVDiff: Experimental Validation of Enhanced Viability and Diversity in Recombinant Adeno-Associated Virus (AAV) Capsids through Diffusion Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recombinant adeno-associated virus (rAAV) vectors have revolutionized gene therapy, but their broad tropism and suboptimal transduction efficiency limit their clinical applications. To overcome these limitations, researchers have focused on designing and screening capsid libraries to identify improved vectors. However, the large sequence space and limited resources present challenges in identifying viable capsid variants. In this study, we propose an end-to-end diffusion model to generate capsid sequences with enhanced viability. Using publicly available AAV2 data, we generated 38,000 diverse AAV2 viral protein (VP) sequences, and evaluated 8,000 for viral selection. The results attested the superiority of our model compared to traditional methods. Additionally, in the absence of AAV9 capsid data, apart from one wild-type sequence, we used the same model to directly generate a number of viable sequences with up to 9 mutations. we transferred the remaining 30,000 samples to the AAV9 domain. Furthermore, we conducted mutagenesis on AAV9 VP hypervariable regions VI and V, contributing to the continuous improvement of the AAV9 VP sequence. This research represents a significant advancement in the design and functional validation of rAAV vectors, offering innovative solutions to enhance specificity and transduction efficiency in gene therapy applications.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2402.15020.pdf' target='_blank'>https://arxiv.org/pdf/2402.15020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Creston Brooks, Robert Calef, Charlie Cowen-Breen, Anna Sappington
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15020">Towards Probabilistically-Sound Beam Search with Masked Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. However, estimating such distributions has important domain-specific applications such as ancient text restoration and protein engineering. Here we present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound inference time modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains. Notably, our method probes the inductive biases of MLMs and explores the surprising contextual sensitivity of mask tokens for text infilling.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2402.10291.pdf' target='_blank'>https://arxiv.org/pdf/2402.10291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vijayalakshmi Saravanan, Perry Siehien, Shinjae Yoo, Hubertus Van Dam, Thomas Flynn, Christopher Kelly, Khaled Z Ibrahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10291">An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends KCUSUM's pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data's underlying distribution. Furthermore, by harnessing MMD's inherent random-walk structure, we can theoretically analyze KCUSUM's performance across various use cases, including metrics like expected delay and mean runtime to false alarms. Finally, we discuss real-world use cases from scientific simulations such as NWChem CODAR and protein folding data, demonstrating KCUSUM's practical effectiveness in online change point detection.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2402.07631.pdf' target='_blank'>https://arxiv.org/pdf/2402.07631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue Gong, Desmond J. Higham, Konstantinos Zygalakis, Ginestra Bianconi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07631">Higher-order Connection Laplacians for Directed Simplicial Complexes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions. Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics. However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges. On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality. Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions. Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes. The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2401.09840.pdf' target='_blank'>https://arxiv.org/pdf/2401.09840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Telepov, Artem Tsypin, Kuzma Khrabrov, Sergey Yakukhnov, Pavel Strashnov, Petr Zhilyaev, Egor Rumiantsev, Daniel Ezhov, Manvel Avetisian, Olga Popova, Artur Kadurin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09840">FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2401.08519.pdf' target='_blank'>https://arxiv.org/pdf/2401.08519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbang Wang, Jon Kleinberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08519">From Graphs to Hypergraphs: Hypergraph Projection and its Remediation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the implications of the modeling choice to use a graph, instead of a hypergraph, to represent real-world interconnected systems whose constituent relationships are of higher order by nature. Such a modeling choice typically involves an underlying projection process that maps the original hypergraph onto a graph, and is common in graph-based analysis. While hypergraph projection can potentially lead to loss of higher-order relations, there exists very limited studies on the consequences of doing so, as well as its remediation. This work fills this gap by doing two things: (1) we develop analysis based on graph and set theory, showing two ubiquitous patterns of hyperedges that are root to structural information loss in all hypergraph projections; we also quantify the combinatorial impossibility of recovering the lost higher-order structures if no extra help is provided; (2) we still seek to recover the lost higher-order structures in hypergraph projection, and in light of (1)'s findings we propose to relax the problem into a learning-based setting. Under this setting, we develop a learning-based hypergraph reconstruction method based on an important statistic of hyperedge distributions that we find. Our reconstruction method is evaluated on 8 real-world datasets under different settings, and exhibits consistently good performance. We also demonstrate benefits of the reconstructed hypergraphs via use cases of protein rankings and link predictions.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2512.21599.pdf' target='_blank'>https://arxiv.org/pdf/2512.21599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21599">GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding protein flexibility and its dynamic interactions with other molecules is essential for protein function study. Cryogenic electron microscopy (cryo-EM) provides an opportunity to directly observe macromolecular dynamics. However, analyzing datasets that contain both continuous motions and discrete states remains highly challenging. Here we present GaussianEM, a Gaussian pseudo-atomic framework that simultaneously models compositional and conformational heterogeneity from experimental cryo-EM images. GaussianEM employs a two-encoder-one-decoder architecture to map an image to its individual Gaussian components, and represent structural variability through changes in Gaussian parameters. This approach provides an intuitive and interpretable description of conformational changes, preserves local structural consistency along the transition trajectories, and naturally bridges the gap between density-based models and corresponding atomic models. We demonstrate the effectiveness of GaussianEM on both simulated and experimental datasets.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2512.10034.pdf' target='_blank'>https://arxiv.org/pdf/2512.10034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salomé Guilbert, Cassandra Masschelein, Jeremy Goumaz, Bohdan Naida, Philippe Schwaller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10034">DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2512.06496.pdf' target='_blank'>https://arxiv.org/pdf/2512.06496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stella Brown, Nicolas Preisig, Autumn Davis, Brian Hutchinson, Filip Jagodzinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06496">PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2512.02315.pdf' target='_blank'>https://arxiv.org/pdf/2512.02315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Teufel, Aaron W. Kollasch, Yining Huang, Ole Winther, Kevin K. Yang, Pascal Notin, Debora S. Marks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02315">Few-shot Protein Fitness Prediction via In-context Learning and Test-time Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting protein fitness with minimal experimental data is a persistent challenge in protein engineering. We introduce PRIMO (PRotein In-context Mutation Oracle), a transformer-based framework that leverages in-context learning and test-time training to adapt rapidly to new proteins and assays without large task-specific datasets. By encoding sequence information, auxiliary zero-shot predictions, and sparse experimental labels from many assays as a unified token set in a pre-training masked-language modeling paradigm, PRIMO learns to prioritize promising variants through a preference-based loss function. Across diverse protein families and properties-including both substitution and indel mutations-PRIMO outperforms zero-shot and fully supervised baselines. This work underscores the power of combining large-scale pre-training with efficient test-time adaptation to tackle challenging protein design tasks where data collection is expensive and label availability is limited.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2512.00379.pdf' target='_blank'>https://arxiv.org/pdf/2512.00379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anas Aziz Khan, Md Shah Fahad, Priyanka, Ramesh Chandra, Guransh Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00379">EnzyCLIP: A Cross-Attention Dual Encoder Framework with Contrastive Learning for Predicting Enzyme Kinetic Constants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of enzyme kinetic parameters is crucial for drug discovery, metabolic engineering, and synthetic biology applications. Current computational approaches face limitations in capturing complex enzyme-substrate interactions and often focus on single parameters while neglecting the joint prediction of catalytic turnover numbers (Kcat) and Michaelis-Menten constants (Km). We present EnzyCLIP, a novel dual-encoder framework that leverages contrastive learning and cross-attention mechanisms to predict enzyme kinetic parameters from protein sequences and substrate molecular structures. Our approach integrates ESM-2 protein language model embeddings with ChemBERTa chemical representations through a CLIP-inspired architecture enhanced with bidirectional cross-attention for dynamic enzyme-substrate interaction modeling. EnzyCLIP combines InfoNCE contrastive loss with Huber regression loss to learn aligned multimodal representations while predicting log10-transformed kinetic parameters. The model is trained on the CatPred-DB database containing 23,151 Kcat and 41,174 Km experimentally validated measurements, and achieved competitive performance with R2 scores of 0.593 for Kcat and 0.607 for Km prediction. XGBoost ensemble methods applied to the learned embeddings further improved Km prediction (R2 = 0.61) while maintaining robust Kcat performance.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2511.12588.pdf' target='_blank'>https://arxiv.org/pdf/2511.12588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuqi Huang, Mengxin Tian, Huan Liu, Wentao Li, Baobao Liang, Jie Wu, Fang Yan, Zhaoqing Tang, Zhongyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12588">Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2510.27040.pdf' target='_blank'>https://arxiv.org/pdf/2510.27040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dian Chen, Yunkai Chen, Tong Lin, Sijie Chen, Xiaolin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27040">GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal approaches that integrate protein structure and sequence have achieved remarkable success in protein-protein interface prediction. However, extending these methods to protein-peptide interactions remains challenging due to the inherent conformational flexibility of peptides and the limited availability of structural data that hinder direct training of structure-aware models. To address these limitations, we introduce GeoPep, a novel framework for peptide binding site prediction that leverages transfer learning from ESM3, a multimodal protein foundation model. GeoPep fine-tunes ESM3's rich pre-learned representations from protein-protein binding to address the limited availability of protein-peptide binding data. The fine-tuned model is further integrated with a parameter-efficient neural network architecture capable of learning complex patterns from sparse data. Furthermore, the model is trained using distance-based loss functions that exploit 3D structural information to enhance binding site prediction. Comprehensive evaluations demonstrate that GeoPep significantly outperforms existing methods in protein-peptide binding site prediction by effectively capturing sparse and heterogeneous binding patterns.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2510.25807.pdf' target='_blank'>https://arxiv.org/pdf/2510.25807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charlotte Claye, Pierre Marschall, Wassila Ouerdane, Céline Hudelot, Julien Duquesne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25807">Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-cell RNA-seq foundation models achieve strong performance on downstream tasks but remain black boxes, limiting their utility for biological discovery. Recent work has shown that sparse dictionary learning can extract concepts from deep learning models, with promising applications in biomedical imaging and protein models. However, interpreting biological concepts remains challenging, as biological sequences are not inherently human-interpretable. We introduce a novel concept-based interpretability framework for single-cell RNA-seq models with a focus on concept interpretation and evaluation. We propose an attribution method with counterfactual perturbations that identifies genes that influence concept activation, moving beyond correlational approaches like differential expression analysis. We then provide two complementary interpretation approaches: an expert-driven analysis facilitated by an interactive interface and an ontology-driven method with attribution-based biological pathway enrichment. Applying our framework to two well-known single-cell RNA-seq models from the literature, we interpret concepts extracted by Top-K Sparse Auto-Encoders trained on two immune cell datasets. With a domain expert in immunology, we show that concepts improve interpretability compared to individual neurons while preserving the richness and informativeness of the latent representations. This work provides a principled framework for interpreting what biological knowledge foundation models have encoded, paving the way for their use for hypothesis generation and discovery.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2510.23786.pdf' target='_blank'>https://arxiv.org/pdf/2510.23786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joohwan Ko, Aristofanis Rontogiannis, Yih-En Andrew Ban, Axel Elaldi, Nicholas Franklin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23786">Relaxed Sequence Sampling for Diverse Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein design using structure prediction models such as AlphaFold2 has shown remarkable success, but existing approaches like relaxed sequence optimization (RSO) rely on single-path gradient descent and ignore sequence-space constraints, limiting diversity and designability. We introduce Relaxed Sequence Sampling (RSS), a Markov chain Monte Carlo (MCMC) framework that integrates structural and evolutionary information for protein design. RSS operates in continuous logit space, combining gradient-guided exploration with protein language model-informed jumps. Its energy function couples AlphaFold2-derived structural objectives with ESM2-derived sequence priors, balancing accuracy and biological plausibility. In an in silico protein binder design task, RSS produces 5$\times$ more designable structures and 2-3$\times$ greater structural diversity than RSO baselines, at equal computational cost. These results highlight RSS as a principled approach for efficiently exploring the protein design landscape.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2510.22008.pdf' target='_blank'>https://arxiv.org/pdf/2510.22008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Saiful Islam Sajol, Magesh Rajasekaran, Hayden Gemeinhardt, Adam Bess, Chris Alvin, Supratik Mukhopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22008">A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computationally predicting protein-protein interactions (PPIs) is challenging due to the lack of integrated, multimodal protein representations. DPEB is a curated collection of 22,043 human proteins that integrates four embedding types: structural (AlphaFold2), transformer-based sequence (BioEmbeddings), contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), and sequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures are available through public databases (e.g., AlphaFold2 Protein Structure Database), but the internal neural network embeddings are not. DPEB addresses this gap by providing AlphaFold2-derived embeddings for computational modeling. Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highest PPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework also achieved 77.42% accuracy for enzyme classification and 86.04% accuracy for protein family classification. DPEB supports multiple graph neural network methods for PPI prediction, enabling applications in systems biology, drug target identification, pathway analysis, and disease mechanism studies.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2508.14844.pdf' target='_blank'>https://arxiv.org/pdf/2508.14844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Murat Isik, Mandeep Kaur Saggi, Humaira Gowher, Sabre Kais
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14844">Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting enzyme functionality remains one of the major challenges in computational biology, particularly for enzymes with limited structural annotations or sequence homology. We present a novel multimodal Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC) classification by integrating four complementary biochemical modalities: protein sequence embeddings, quantum-derived electronic descriptors, molecular graph structures, and 2D molecular image representations. Quantum Vision Transformer (QVT) backbone equipped with modality-specific encoders and a unified cross-attention fusion module. By integrating graph features and spatial patterns, our method captures key stereoelectronic interactions behind enzyme function. Experimental results demonstrate that our multimodal QVT model achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a substantial margin and achieving better performance results compared to other QML models.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2508.13255.pdf' target='_blank'>https://arxiv.org/pdf/2508.13255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahi Navelkar, Andrea Cosolo, Bogdan Bintu, Yubao Cheng, Vincent Gardeux, Silvia Gutnik, Taihei Fujimori, Antonina Hafner, Atishay Jay, Bojing Blair Jia, Adam Paul Jussila, Gerard Llimos, Antonios Lioutas, Nuno MC Martins, William J Moore, Yodai Takei, Frances Wong, Kaifu Yang, Huaiying Zhang, Quan Zhu, Magda Bienko, Lacramioara Bintu, Long Cai, Bart Deplancke, Marcelo Nollmann, Susan E Mango, Bing Ren, Peter J Park, Ahilya N Sawh, Andrew Schroeder, Jason R Swedlow, Golnaz Vahedi, Chao-Ting Wu, Sarah Aufmkolk, Alistair N Boettiger, Irene Farabella, Caterina Strambio-De-Castillia, Siyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13255">FAIR sharing of Chromatin Tracing datasets using the newly developed 4DN FISH Omics Format</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key output of the NIH Common Fund 4D Nucleome (4DN) project is the open publication of datasets on the structure of the human cell nucleus and genome. In recent years, multiplexed Fluorescence In Situ Hybridization (FISH) and FISH-omics methods have rapidly expanded, enabling quantification of chromatin organization in single cells, sometimes alongside RNA and protein measurements. These approaches have deepened our understanding of how 3D chromosome architecture relates to transcriptional activity and cell development in health and disease. However, results from Chromatin Tracing FISH-omics experiments remain difficult to share, reuse, and analyze due to the absence of standardized data-exchange specifications. Building on the recent release of microscopy metadata standards, we introduce the 4DN FISH Omics Format-Chromatin Tracing (FOF-CT), a community-developed standard for processed results from diverse imaging techniques. Current studies generally use one of two representations: ball-and-stick, where genomic segments appear as individual fluorescence spots, or volumetric, representing them as clouds of single-molecule localizations. This manuscript focuses on ball-and-stick methods, including those from the pioneering study of Wang et al. (2016) and related techniques. We describe the FOF-CT structure and present newly deposited datasets in the 4DN Data Portal and the OME Image Data Resource (IDR), highlighting their potential for reuse, integration, and modeling. We also outline example analysis pipelines and illustrate biological insights enabled by standardized, FAIR-compliant Chromatin Tracing datasets.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2508.02834.pdf' target='_blank'>https://arxiv.org/pdf/2508.02834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqi Feng, Peng Qiu, Mengchun Zhang, Yiran Tao, You Fan, Jingtao Xu, Barnabas Poczos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02834">Learning from B Cell Evolution: Adaptive Multi-Expert Diffusion for Antibody Design via Online Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have shown remarkable potential for antibody design, yet existing approaches apply uniform generation strategies that cannot adapt to each antigen's unique requirements. Inspired by B cell affinity maturation, where antibodies evolve through multi-objective optimization balancing affinity, stability, and self-avoidance, we propose the first biologically-motivated framework that leverages physics-based domain knowledge within an online meta-learning system. Our method employs multiple specialized experts (van der Waals, molecular recognition, energy balance, and interface geometry) whose parameters evolve during generation based on iterative feedback, mimicking natural antibody refinement cycles. Instead of fixed protocols, this adaptive guidance discovers personalized optimization strategies for each target. Our experiments demonstrate that this approach: (1) discovers optimal SE(3)-equivariant guidance strategies for different antigen classes without pre-training, preserving molecular symmetries throughout optimization; (2) significantly enhances hotspot coverage and interface quality through target-specific adaptation, achieving balanced multi-objective optimization characteristic of therapeutic antibodies; (3) establishes a paradigm for iterative refinement where each antibody-antigen system learns its unique optimization profile through online evaluation; (4) generalizes effectively across diverse design challenges, from small epitopes to large protein interfaces, enabling precision-focused campaigns for individual targets.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2507.19805.pdf' target='_blank'>https://arxiv.org/pdf/2507.19805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>FranÃ§ois Charih, James R. Green, Kyle K. Biggar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19805">Sequence-based protein-protein interaction prediction and its applications in drug discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aberrant protein-protein interactions (PPIs) underpin a plethora of human diseases, and disruption of these harmful interactions constitute a compelling treatment avenue. Advances in computational approaches to PPI prediction have closely followed progress in deep learning and natural language processing. In this review, we outline the state-of the-art for sequence-based PPI prediction methods and explore their impact on target identification and drug discovery. We begin with an overview of commonly used training data sources and techniques used to curate these data to enhance the quality of the training set. Subsequently, we survey various PPI predictor types, including traditional similarity-based approaches, and deep learning-based approaches with a particular emphasis on the transformer architecture. Finally, we provide examples of PPI prediction in systems-level proteomics analyses, target identification, and design of therapeutic peptides and antibodies. We also take the opportunity to showcase the potential of PPI-aware drug discovery models in accelerating therapeutic development.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2507.17876.pdf' target='_blank'>https://arxiv.org/pdf/2507.17876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rıza Özçelik, Sarah de Ruiter, Francesca Grisoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17876">Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scarcity of molecules with desirable properties (i.e., `positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' - without accessing any positively labeled data - and moving models in the opposite property directions to generate positive molecules. When analyzed on 33 design experiments with distinct molecular entities (small molecules, proteins), model architectures, and scales, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules in general. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable complex design properties, such as good docking scores to a protein. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the de facto transfer learning strategy for de novo molecule design.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2507.10953.pdf' target='_blank'>https://arxiv.org/pdf/2507.10953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Balu Bhasuran, Sabenabanu Abdulkadhar, Jeyakumar Natarajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10953">Unraveling the Biomarker Prospects of High-Altitude Diseases: Insights from Biomolecular Event Network Constructed using Text Mining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-altitude diseases (HAD), encompassing acute mountain sickness (AMS), high-altitude cerebral edema (HACE), and high-altitude pulmonary edema (HAPE), are triggered by hypobaric hypoxia at elevations above 2,500 meters. These conditions pose significant health risks, yet the molecular mechanisms remain insufficiently understood. In this study, we developed a biomolecular event extraction pipeline integrating supervised machine learning with feature-based and multiscale Laplacian graph kernels to analyze 7,847 curated HAD-related abstracts from PubMed. We extracted over 150 unique biomolecular events including gene expression, regulation, binding, and localization and constructed a weighted, undirected biomolecular event network comprising 97 nodes and 153 edges. Using the PageRank algorithm, we prioritized key biomolecules based on their centrality within the event network. The top-ranked proteins included Erythropoietin (EPO) (0.0163), Vascular endothelial growth factor (VEGF) (0.0148), Hypoxia-inducible factor 1 (HIF-1) alpha (0.0136), Endothelial PAS Domain Protein 1 (EPAS1) and Angiotensin-Converting Enzyme (ACE) (0.0119), Egl nine homolog 1 (EGLN1), Endothelin 1 (ET-1), and 70 kilodalton heat shock protein (Hsp70)(0.0118), all of which play crucial roles in oxygen sensing, vascular remodeling, erythropoiesis, and blood pressure regulation. Subnetwork analysis revealed three major functional clusters centered on hypoxia response, inflammation, and stress adaptation pathways. Our integrative approach demonstrates the utility of large-scale text mining and graph-based analysis to uncover mechanistic insights and prioritize potential biomarkers for high-altitude disease.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2506.20043.pdf' target='_blank'>https://arxiv.org/pdf/2506.20043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Sarigun, Bora Uyar, Vedran Franke, Altuna Akalin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20043">PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling physically valid ligand-binding poses remains a major challenge in molecular docking, particularly for unseen or structurally diverse targets. We introduce PocketVina, a fast and memory-efficient, search-based docking framework that combines pocket prediction with systematic multi-pocket exploration. We evaluate PocketVina across four established benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and PoseBusters--and observe consistently strong performance in sampling physically valid docking poses. PocketVina achieves state-of-the-art performance when jointly considering ligand RMSD and physical validity (PB-valid), while remaining competitive with deep learning-based approaches in terms of RMSD alone, particularly on structurally diverse and previously unseen targets. PocketVina also maintains state-of-the-art physically valid docking accuracy across ligands with varying degrees of flexibility. We further introduce TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000 protein-ligand pairs, and a partition of the dataset labeled with PubChem activity annotations. On this large-scale dataset, PocketVina successfully discriminates active from inactive targets, outperforming a deep learning baseline while requiring significantly less GPU memory and runtime. PocketVina offers a robust and scalable docking strategy that requires no task-specific training and runs efficiently on standard GPUs, making it well-suited for high-throughput virtual screening and structure-based drug discovery.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2506.17857.pdf' target='_blank'>https://arxiv.org/pdf/2506.17857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunan Liu, Aurelien Pelissier, Yanjun Shao, Lilian Denzler, Andrew C. R. Martin, Brooks Paige, MarÃ­a RodrÃ­guez MartÃ­nez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17857">AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential for therapeutic design and vaccine development, yet the performance of current models is limited by noisy experimental labels, heterogeneous assay conditions, and poor generalization across the vast antibody and antigen sequence space. We introduce AbRank, a large-scale benchmark and evaluation framework that reframes affinity prediction as a pairwise ranking problem. AbRank aggregates over 380,000 binding assays from nine heterogeneous sources, spanning diverse antibodies, antigens, and experimental conditions, and introduces standardized data splits that systematically increase distribution shift, from local perturbations such as point mutations to broad generalization across novel antigens and antibodies. To ensure robust supervision, AbRank defines an m-confident ranking framework by filtering out comparisons with marginal affinity differences, focusing training on pairs with at least an m-fold difference in measured binding strength. As a baseline for the benchmark, we introduce WALLE-Affinity, a graph-based approach that integrates protein language model embeddings with structural information to predict pairwise binding preferences. Our benchmarks reveal significant limitations in current methods under realistic generalization settings and demonstrate that ranking-based training improves robustness and transferability. In summary, AbRank offers a robust foundation for machine learning models to generalize across the antibody-antigen space, with direct relevance for scalable, structure-aware antibody therapeutic design.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2506.10015.pdf' target='_blank'>https://arxiv.org/pdf/2506.10015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuqiao Zhang, Sarath Chandra Dantu, Debarghya Mitra, Dalia Chakrabarty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10015">Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification of critical residues of a protein is actively pursued, since such residues are essential for protein function. We present three ways of recognising critical residues of an example protein, the evolution of which is tracked via molecular dynamical simulations. Our methods are based on learning a Random Geometric Graph (RGG) variable, where the state variable of each of 156 residues, is attached to a node of this graph, with the RGG learnt using the matrix of correlations between state variables of each residue-pair. Given the categorical nature of the state variable, correlation between a residue pair is computed using Cramer's V. We advance an organic thresholding to learn an RGG, and compare results against extant thresholding techniques, when parametrising criticality as the nodal degree in the learnt RGG. Secondly, we develop a criticality measure by ranking the computed differences between the posterior probability of the full graph variable defined on all 156 residues, and that of the graph with all but one residue omitted. A third parametrisation of criticality informs on the dynamical variation of nodal degrees as the protein evolves during the simulation. Finally, we compare results obtained with the three distinct criticality parameters, against experimentally-ascertained critical residues.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2506.05443.pdf' target='_blank'>https://arxiv.org/pdf/2506.05443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Lin, Yan Wang, You Zhou, Xinye Ni, Jiahui Wu, Sen Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05443">UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a "Master-Slave" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2505.21241.pdf' target='_blank'>https://arxiv.org/pdf/2505.21241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divya Nori, Anisha Parsan, Caroline Uhler, Wengong Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21241">BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein binder design has been transformed by hallucination-based methods that optimize structure prediction confidence metrics, such as the interface predicted TM-score (ipTM), via backpropagation. However, these metrics do not reflect the statistical likelihood of a binder-target complex under the learned distribution and yield sparse gradients for optimization. In this work, we propose a method to extract such likelihoods from structure predictors by reinterpreting their confidence outputs as an energy-based model (EBM). By leveraging the Joint Energy-based Modeling (JEM) framework, we introduce pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a design pipeline that maintains the same optimization framework as BindCraft but replaces ipTM with our energy-based objective. BECraft outperforms BindCraft, RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in silico binder success rates while reducing structural clashes. Furthermore, pTMEnergy establishes a new state-of-the-art in structure-based virtual screening tasks for miniprotein and RNA aptamer binders.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2505.16032.pdf' target='_blank'>https://arxiv.org/pdf/2505.16032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathryn Linehan, Radu Balan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16032">CUR Matrix Approximation through Convex Optimization for Feature Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The singular value decomposition (SVD) is commonly used in applications requiring a low rank matrix approximation. However, the singular vectors cannot be interpreted in terms of the original data. For applications requiring this type of interpretation, e.g., selection of important data matrix columns or rows, the approximate CUR matrix factorization can be used. Work on the CUR matrix approximation has generally focused on algorithm development, theoretical guarantees, and applications. In this work, we present a novel deterministic CUR formulation and algorithm with theoretical convergence guarantees. The algorithm utilizes convex optimization, finds important columns and rows separately, and allows the user to control the number of important columns and rows selected from the original data matrix. We present numerical results and demonstrate the effectiveness of our CUR algorithm as a feature selection method on gene expression data. These results are compared to those using the SVD and other CUR algorithms as the feature selection method. Lastly, we present a novel application of CUR as a feature selection method to determine discriminant proteins when clustering protein expression data in a self-organizing map (SOM), and compare the performance of multiple CUR algorithms in this application.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2505.15831.pdf' target='_blank'>https://arxiv.org/pdf/2505.15831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashley Wang, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15831">Ricci Matrix Comparison for Graph Alignment: A DMC Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The graph alignment problem explores the concept of node correspondence and its optimality. In this paper, we focus on purely geometric graph alignment methods, namely our newly proposed Ricci Matrix Comparison (RMC) and its original form, Degree Matrix Comparison (DMC). To formulate a Ricci-curvature-based graph alignment situation, we start with discussing different ideas of constructing one of the most typical and important topological objects, the torus, and then move on to introducing the RMC based on DMC with theoretical motivations. Lastly, we will present to the reader experimental results on a torus and a complex protein-protein interaction network that indicate the potential of applying a differential-geometric view to graph alignment. Results show that a direct variation of DMC using Ricci curvature can help with identifying holes in tori and aligning line graphs of a complex network at 80-90+% accuracy. This paper contributes a new perspective to the field of graph alignment and partially shows the validity of the previous DMC method.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2505.12913.pdf' target='_blank'>https://arxiv.org/pdf/2505.12913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom George Grigg, Mason Burlage, Oliver Brook Scott, Adam Taouil, Dominique Sydow, Liam Wilbraham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12913">Active Learning on Synthons for Molecular Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exhaustive virtual screening is highly informative but often intractable against the expensive objective functions involved in modern drug discovery. This problem is exacerbated in combinatorial contexts such as multi-vector expansion, where molecular spaces can quickly become ultra-large. Here, we introduce Scalable Active Learning via Synthon Acquisition (SALSA): a simple algorithm applicable to multi-vector expansion which extends pool-based active learning to non-enumerable spaces by factoring modeling and acquisition over synthon or fragment choices. Through experiments on ligand- and structure-based objectives, we highlight SALSA's sample efficiency, and its ability to scale to spaces of trillions of compounds. Further, we demonstrate application toward multi-parameter objective design tasks on three protein targets - finding SALSA-generated molecules have comparable chemical property profiles to known bioactives, and exhibit greater diversity and higher scores over an industry-leading generative approach.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2505.10545.pdf' target='_blank'>https://arxiv.org/pdf/2505.10545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amira Alakhdar, Barnabas Poczos, Newell Washburn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10545">Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing bioactive molecules remains a central, time- and cost-heavy challenge in drug discovery, particularly for novel targets lacking structural or functional data. Pharmacophore modeling presents an alternative for capturing the key features required for molecular bioactivity against a biological target. In this work, we present PharmaDiff, a pharmacophore-conditioned diffusion model for 3D molecular generation. PharmaDiff employs a transformer-based architecture to integrate an atom-based representation of the 3D pharmacophore into the generative process, enabling the precise generation of 3D molecular graphs that align with predefined pharmacophore hypotheses. Through comprehensive testing, PharmaDiff demonstrates superior performance in matching 3D pharmacophore constraints compared to ligand-based drug design methods. Additionally, it achieves higher docking scores across a range of proteins in structure-based drug design, without the need for target protein structures. By integrating pharmacophore modeling with 3D generative techniques, PharmaDiff offers a powerful and flexible framework for rational drug design.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2505.04823.pdf' target='_blank'>https://arxiv.org/pdf/2505.04823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Xiong, Hunter Nisonoff, Maria Lukarska, Ishan Gaur, Luke M. Oltrogge, David F. Savage, Jennifer Listgarten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04823">Guide your favorite protein sequence generative model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative machine learning models on sequences are transforming protein engineering. However, no principled framework exists for conditioning these models on auxiliary information, such as experimental data, in a plug-and-play manner. Herein, we present ProteinGuide -- a principled and general method for conditioning -- by unifying a broad class of protein generative models under a single framework. We demonstrate the applicability of ProteinGuide by guiding two protein generative models, ProteinMPNN and ESM3, to generate amino acid and structure token sequences, conditioned on several user-specified properties such as enhanced stability, enzyme classes, and CATH-labeled folds. We also used ProteinGuide with inverse folding models and our own experimental assay to design adenine base editor sequences for high activity.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2504.20102.pdf' target='_blank'>https://arxiv.org/pdf/2504.20102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingzhi Yu, Shuai Yan, Wenfeng Dai, Xiang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20102">HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) are fundamental for deciphering cellular functions,disease pathways,and drug discovery.Although existing neural networks and machine learning methods have achieved high accuracy in PPI prediction,their black-box nature leads to a lack of causal interpretation of the prediction results and difficulty in capturing hierarchical geometries and multi-scale dynamic interaction patterns among proteins.To address these challenges, we propose HyboWaveNet,a novel deep learning framework that collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale graphical wavelet transform for robust PPI prediction. Mapping protein features to Lorentz space simulates hierarchical topological relationships among biomolecules via a hyperbolic distance metric,enabling node feature representations that better fit biological a priori.HyboWaveNet inherently simulates hierarchical and scale-free biological relationships, while the integration of wavelet transforms enables adaptive extraction of local and global interaction features across different resolutions. Our framework generates node feature representations via a graph neural network under the Lorenz model and generates pairs of positive samples under multiple different views for comparative learning, followed by further feature extraction via multi-scale graph wavelet transforms to predict potential PPIs. Experiments on public datasets show that HyboWaveNet improves over both existing state-of-the-art methods. We also demonstrate through ablation experimental studies that the multi-scale graph wavelet transform module improves the predictive performance and generalization ability of HyboWaveNet. This work links geometric deep learning and signal processing to advance PPI prediction, providing a principled approach for analyzing complex biological systems
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2504.14274.pdf' target='_blank'>https://arxiv.org/pdf/2504.14274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxi Lu, Shizhuo Cheng, Yuru Jiang, Yan Zhang, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14274">ProtPainter: Draw or Drag Protein via Topology-guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein backbone generation have achieved promising results under structural, functional, or physical constraints. However, existing methods lack the flexibility for precise topology control, limiting navigation of the backbone space. We present ProtPainter, a diffusion-based approach for generating protein backbones conditioned on 3D curves. ProtPainter follows a two-stage process: curve-based sketching and sketch-guided backbone generation. For the first stage, we propose CurveEncoder, which predicts secondary structure annotations from a curve to parametrize sketch generation. For the second stage, the sketch guides the generative process in Denoising Diffusion Probabilistic Modeling (DDPM) to generate backbones. During this process, we further introduce a fusion scheduling scheme, Helix-Gating, to control the scaling factors. To evaluate, we propose the first benchmark for topology-conditioned protein generation, introducing Protein Restoration Task and a new metric, self-consistency Topology Fitness (scTF). Experiments demonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and designable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing its flexibility and versatility.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2504.13853.pdf' target='_blank'>https://arxiv.org/pdf/2504.13853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingfei Zhu, Chenyang Zhao, Haishi Zhao, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13853">GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-powered drug discovery typically relies on the successful prediction of compound-protein interactions, which are pivotal for the evaluation of designed compound molecules in structure-based drug design and represent a core challenge in the field.
  However, accurately predicting compound-protein affinity via regression models usually requires adequate-binding pose, which are derived from costly and complex experimental methods or time-consuming simulations with docking software. In response, we have introduced the GenShin model, which constructs a geometry-enhanced structural graph module that separately extracts additional features from proteins and compounds. Consequently, it attains an accuracy on par with mainstream models in predicting compound-protein affinities, while eliminating the need for adequate-binding pose as input. Our experimental findings demonstrate that the GenShin model vastly outperforms other models that rely on non-input docking conformations, achieving, or in some cases even exceeding, the performance of those requiring adequate-binding pose. Further experiments indicate that our GenShin model is more robust to inadequate-binding pose, affirming its higher suitability for real-world drug discovery scenarios. We hope our work will inspire more endeavors to bridge the gap between AI models and practical drug discovery challenges.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2504.08328.pdf' target='_blank'>https://arxiv.org/pdf/2504.08328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alice Driessen, Benedek Harsanyi, Marianna Rapsomaniki, Jannis Born
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08328">Towards generalizable single-cell perturbation modeling via the Conditional Monge Gap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning the response of single-cells to various treatments offers great potential to enable targeted therapies. In this context, neural optimal transport (OT) has emerged as a principled methodological framework because it inherently accommodates the challenges of unpaired data induced by cell destruction during data acquisition. However, most existing OT approaches are incapable of conditioning on different treatment contexts (e.g., time, drug treatment, drug dosage, or cell type) and we still lack methods that unanimously show promising generalization performance to unseen treatments. Here, we propose the Conditional Monge Gap which learns OT maps conditionally on arbitrary covariates. We demonstrate its value in predicting single-cell perturbation responses conditional to one or multiple drugs, a drug dosage, or combinations thereof. We find that our conditional models achieve results comparable and sometimes even superior to the condition-specific state-of-the-art on scRNA-seq as well as multiplexed protein imaging data. Notably, by aggregating data across conditions we perform cross-task learning which unlocks remarkable generalization abilities to unseen drugs or drug dosages, widely outperforming other conditional models in capturing heterogeneity (i.e., higher moments) in the perturbed population. Finally, by scaling to hundreds of conditions and testing on unseen drugs, we narrow the gap between structure-based and effect-based drug representations, suggesting a promising path to the successful prediction of perturbation effects for unseen treatments.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2504.07156.pdf' target='_blank'>https://arxiv.org/pdf/2504.07156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan van Eck, Dea Gogishvili, Wilson Silva, Sanne Abeln
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07156">PLM-eXplain: Divide and Conquer the Protein Embedding Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2503.17430.pdf' target='_blank'>https://arxiv.org/pdf/2503.17430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shun-Cai Zhao, Yi-Meng Huang, Yi-Fan Yang, Zi-Ran Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17430">Multi-timescale time encoding for CNN prediction of Fenna-Matthews-Olson energy-transfer dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning simulations of open quantum dynamics often rely on recursive predictors that accumulate error. We develop a non-recursive convolutional neural networks (CNNs) that maps system parameters and a redundant time encoding directly to excitation-energy-transfer populations in the Fenna-Matthews-Olson complex. The encoding-modified logistic plus $\tanh$ functions-normalizes time and resolves fast, transitional, and quasi-steady regimes, while physics-informed labels enforce population conservation and inter-site consistency. Trained only on $0\sim 7 ps$ reference trajectories generated with a Lindblad model in QuTiP, the network accurately predicts $0\sim100 ps$ dynamics across a range of reorganization energies, bath rates, and temperatures. Beyond $20 ps$, the absolute relative error remains below 0.05, demonstrating stable long-time extrapolation. By avoiding step-by-step recursion, the method suppresses error accumulation and generalizes across timescales. These results show that redundant time encoding enables data-efficient inference of long-time quantum dissipative dynamics in realistic pigment-protein complexes, and may aid the data-driven design of light-harvesting materials.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2503.13329.pdf' target='_blank'>https://arxiv.org/pdf/2503.13329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beatriz Costa-Gomes, Joel Greer, Nikolai Juraschko, James Parkhurst, Jola Mirecka, Marjan Famili, Camila Rangel-Smith, Oliver Strickson, Alan Lowe, Mark Basham, Tom Burnley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13329">PERC: a suite of software tools for the curation of cryoEM data with application to simulation, modelling and machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ease of access to data, tools and models expedites scientific research. In structural biology there are now numerous open repositories of experimental and simulated datasets. Being able to easily access and utilise these is crucial for allowing researchers to make optimal use of their research effort. The tools presented here are useful for collating existing public cryoEM datasets and/or creating new synthetic cryoEM datasets to aid the development of novel data processing and interpretation algorithms. In recent years, structural biology has seen the development of a multitude of machine-learning based algorithms for aiding numerous steps in the processing and reconstruction of experimental datasets and the use of these approaches has become widespread. Developing such techniques in structural biology requires access to large datasets which can be cumbersome to curate and unwieldy to make use of. In this paper we present a suite of Python software packages which we collectively refer to as PERC (profet, EMPIARreader and CAKED). These are designed to reduce the burden which data curation places upon structural biology research. The protein structure fetcher (profet) package allows users to conveniently download and cleave sequences or structures from the Protein Data Bank or Alphafold databases. EMPIARreader allows lazy loading of Electron Microscopy Public Image Archive datasets in a machine-learning compatible structure. The Class Aggregator for Key Electron-microscopy Data (CAKED) package is designed to seamlessly facilitate the training of machine learning models on electron microscopy data, including electron-cryo-microscopy-specific data augmentation and labelling. These packages may be utilised independently or as building blocks in workflows. All are available in open source repositories and designed to be easily extensible to facilitate more advanced workflows if required.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2503.08838.pdf' target='_blank'>https://arxiv.org/pdf/2503.08838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Burak Suyunu, Ãzdeniz Dolu, Arzucan ÃzgÃ¼r
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08838">evoBPE: Evolutionary Protein Sequence Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in computational biology have drawn compelling parallels between protein sequences and linguistic structures, highlighting the need for sophisticated tokenization methods that capture the intricate evolutionary dynamics of protein sequences. Current subword tokenization techniques, primarily developed for natural language processing, often fail to represent protein sequences' complex structural and functional properties adequately. This study introduces evoBPE, a novel tokenization approach that integrates evolutionary mutation patterns into sequence segmentation, addressing critical limitations in existing methods. By leveraging established substitution matrices, evoBPE transcends traditional frequency-based tokenization strategies. The method generates candidate token pairs through biologically informed mutations, evaluating them based on pairwise alignment scores and frequency thresholds. Extensive experiments on human protein sequences show that evoBPE performs better across multiple dimensions. Domain conservation analysis reveals that evoBPE consistently outperforms standard Byte-Pair Encoding, particularly as vocabulary size increases. Furthermore, embedding similarity analysis using ESM-2 suggests that mutation-based token replacements preserve biological sequence properties more effectively than arbitrary substitutions. The research contributes to protein sequence representation by introducing a mutation-aware tokenization method that better captures evolutionary nuances. By bridging computational linguistics and molecular biology, evoBPE opens new possibilities for machine learning applications in protein function prediction, structural modeling, and evolutionary analysis.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2503.03360.pdf' target='_blank'>https://arxiv.org/pdf/2503.03360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Afnan Sultan, Max Rausch-Dupont, Shahrukh Khan, Olga Kalinina, Dietrich Klakow, Andrea Volkamer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03360">Transformers for molecular property prediction: Domain adaptation efficiently improves performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past six years, molecular transformer models have become key tools in drug discovery. Most existing models are pre-trained on large, unlabeled datasets such as ZINC or ChEMBL. However, the extent to which large-scale pre-training improves molecular property prediction remains unclear. This study evaluates transformer models for this task while addressing their limitations. We explore how pre-training dataset size and chemically informed objectives impact performance. Our results show that increasing the dataset beyond approximately 400K to 800K molecules from large-scale unlabeled databases does not enhance performance across seven datasets covering five ADME endpoints: lipophilicity, permeability, solubility (two datasets), microsomal stability (two datasets), and plasma protein binding. In contrast, domain adaptation on a small, domain-specific dataset (less than or equal 4K molecules) using multi-task regression of physicochemical properties significantly boosts performance (P-value less than 0.001). A model pre-trained on 400K molecules and adapted with domain-specific data outperforms larger models such as MolFormer and performs comparably to MolBERT. Benchmarks against Random Forest (RF) baselines using descriptors and Morgan fingerprints show that chemically and physically informed features consistently yield better performance across model types. While RF remains a strong baseline, we identify concrete practices to enhance transformer performance. Aligning pre-training and adaptation with chemically meaningful tasks and domain-relevant data presents a promising direction for molecular property prediction. Our models are available on HuggingFace for easy use and adaptation.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2502.20632.pdf' target='_blank'>https://arxiv.org/pdf/2502.20632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoummo Ahsan Khandoker, Estelle M. Inack, Mohamed Hibat-Allah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20632">Lattice Protein Folding with Variational Annealing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the principles of protein folding is a cornerstone of computational biology, with implications for drug design, bioengineering, and the understanding of fundamental biological processes. Lattice protein folding models offer a simplified yet powerful framework for studying the complexities of protein folding, enabling the exploration of energetically optimal folds under constrained conditions. However, finding these optimal folds is a computationally challenging combinatorial optimization problem. In this work, we introduce a novel upper-bound training scheme that employs masking to identify the lowest-energy folds in two-dimensional Hydrophobic-Polar (HP) lattice protein folding. By leveraging Dilated Recurrent Neural Networks (RNNs) integrated with an annealing process driven by temperature-like fluctuations, our method accurately predicts optimal folds for benchmark systems of up to 60 beads. Our approach also effectively masks invalid folds from being sampled without compromising the autoregressive sampling properties of RNNs. This scheme is generalizable to three spatial dimensions and can be extended to lattice protein models with larger alphabets. Our findings emphasize the potential of advanced machine learning techniques in tackling complex protein folding problems and a broader class of constrained combinatorial optimization challenges.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2502.10828.pdf' target='_blank'>https://arxiv.org/pdf/2502.10828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amey P. Pasarkar, Adji Bousso Dieng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10828">The Vendiscope: An Algorithmic Microscope For Data Collections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of microscopy, beginning with its invention in the late 16th century, has continuously enhanced our ability to explore and understand the microscopic world, enabling increasingly detailed observations of structures and phenomena. In parallel, the rise of data-driven science has underscored the need for sophisticated methods to explore and understand the composition of complex data collections. This paper introduces the Vendiscope, the first algorithmic microscope designed to extend traditional microscopy to computational analysis. The Vendiscope leverages the Vendi scores -- a family of differentiable diversity metrics rooted in ecology and quantum mechanics -- and assigns weights to data points based on their contribution to the overall diversity of the collection. These weights enable high-resolution data analysis at scale. We demonstrate this across biology, materials science, and machine learning (ML). We analyzed the $250$ million protein sequences in the protein universe, discovering that over $200$ million are near-duplicates and that AlphaFold fails on proteins with Gene Ontology (GO) functions that contribute most to diversity. Applying the Vendiscope to the Materials Project database led to similar findings: more than $85\%$ of the crystals with formation energy data are near-duplicates and ML models perform poorly on materials that enhance diversity. Additionally, the Vendiscope can be used to study phenomena such as memorization in generative models. We used the Vendiscope to identify memorized training samples from $13$ different generative models and found that the best-performing ones often memorize the training samples that contribute least to diversity. Our findings demonstrate that the Vendiscope can serve as a powerful tool for data-driven science.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2502.02182.pdf' target='_blank'>https://arxiv.org/pdf/2502.02182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis-Alexandre Leger, Maxine Leonardi, Andrea Salati, Felix Naef, Martin Weigert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02182">Sequence models for continuous cell cycle stage prediction from brightfield images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the Fucci system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous Fucci signals using non-fluorescence brightfield imaging, a widely available label-free modality. To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2502.01461.pdf' target='_blank'>https://arxiv.org/pdf/2502.01461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitay Sicherman, Kira Radinsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01461">Docking-Aware Attention: Dynamic Protein Representations through Molecular Context Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational prediction of enzymatic reactions represents a crucial challenge in sustainable chemical synthesis across various scientific domains, ranging from drug discovery to materials science and green chemistry. These syntheses rely on proteins that selectively catalyze complex molecular transformations. These protein catalysts exhibit remarkable substrate adaptability, with the same protein often catalyzing different chemical transformations depending on its molecular partners. Current approaches to protein representation in reaction prediction either ignore protein structure entirely or rely on static embeddings, failing to capture how proteins dynamically adapt their behavior to different substrates. We present Docking-Aware Attention (DAA), a novel architecture that generates dynamic, context-dependent protein representations by incorporating molecular docking information into the attention mechanism. DAA combines physical interaction scores from docking predictions with learned attention patterns to focus on protein regions most relevant to specific molecular interactions. We evaluate our method on enzymatic reaction prediction, where it outperforms previous state-of-the-art methods, achieving 62.2\% accuracy versus 56.79\% on complex molecules and 55.54\% versus 49.45\% on innovative reactions. Through detailed ablation studies and visualizations, we demonstrate how DAA generates interpretable attention patterns that adapt to different molecular contexts. Our approach represents a general framework for context-aware protein representation in biocatalysis prediction, with potential applications across enzymatic synthesis planning. We open-source our implementation and pre-trained models to facilitate further research.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2501.18456.pdf' target='_blank'>https://arxiv.org/pdf/2501.18456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Rosset, Roberto Netti, Anna Paola Muntoni, Martin Weigt, Francesco Zamponi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18456">adabmDCA 2.0 -- a flexible but easy-to-use package for Direct Coupling Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this methods article, we provide a flexible but easy-to-use implementation of Direct Coupling Analysis (DCA) based on Boltzmann machine learning, together with a tutorial on how to use it. The package \texttt{adabmDCA 2.0} is available in different programming languages (C++, Julia, Python) usable on different architectures (single-core and multi-core CPU, GPU) using a common front-end interface. In addition to several learning protocols for dense and sparse generative DCA models, it allows to directly address common downstream tasks like residue-residue contact prediction, mutational-effect prediction, scoring of sequence libraries and generation of artificial sequences for sequence design. It is readily applicable to protein and RNA sequence data.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2501.09571.pdf' target='_blank'>https://arxiv.org/pdf/2501.09571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Laird, Circe Hsu, Asilata Bapat, Robin Walters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09571">MatrixNet: Learning over symmetry groups using learned group representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2501.07747.pdf' target='_blank'>https://arxiv.org/pdf/2501.07747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Bianchin de Oliveira, Helio Pedrini, Zanoni Dias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07747">Scaling Up ESM2 Architectures for Long Protein Sequences Analysis: Long and Quantized Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various approaches utilizing Transformer architectures have achieved state-of-the-art results in Natural Language Processing (NLP). Based on this success, numerous architectures have been proposed for other types of data, such as in biology, particularly for protein sequences. Notably among these are the ESM2 architectures, pre-trained on billions of proteins, which form the basis of various state-of-the-art approaches in the field. However, the ESM2 architectures have a limitation regarding input size, restricting it to 1,022 amino acids, which necessitates the use of preprocessing techniques to handle sequences longer than this limit. In this paper, we present the long and quantized versions of the ESM2 architectures, doubling the input size limit to 2,048 amino acids.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2501.02824.pdf' target='_blank'>https://arxiv.org/pdf/2501.02824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Jiang, Long Chen, Yueying Zhu, Yazhou Shi, Huahai Qiu, Bengong Zhang, Tianshou Zhou, Guo-Wei Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02824">Proteomic Learning of Gamma-Aminobutyric Acid (GABA) Receptor-Mediated Anesthesia</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anesthetics are crucial in surgical procedures and therapeutic interventions, but they come with side effects and varying levels of effectiveness, calling for novel anesthetic agents that offer more precise and controllable effects. Targeting Gamma-aminobutyric acid (GABA) receptors, the primary inhibitory receptors in the central nervous system, could enhance their inhibitory action, potentially reducing side effects while improving the potency of anesthetics. In this study, we introduce a proteomic learning of GABA receptor-mediated anesthesia based on 24 GABA receptor subtypes by considering over 4000 proteins in protein-protein interaction (PPI) networks and over 1.5 millions known binding compounds. We develop a corresponding drug-target interaction network to identify potential lead compounds for novel anesthetic design. To ensure robust proteomic learning predictions, we curated a dataset comprising 136 targets from a pool of 980 targets within the PPI networks. We employed three machine learning algorithms, integrating advanced natural language processing (NLP) models such as pretrained transformer and autoencoder embeddings. Through a comprehensive screening process, we evaluated the side effects and repurposing potential of over 180,000 drug candidates targeting the GABRA5 receptor. Additionally, we assessed the ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties of these candidates to identify those with near-optimal characteristics. This approach also involved optimizing the structures of existing anesthetics. Our work presents an innovative strategy for the development of new anesthetic drugs, optimization of anesthetic use, and deeper understanding of potential anesthesia-related side effects.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2501.02680.pdf' target='_blank'>https://arxiv.org/pdf/2501.02680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-ran Li, Xavier F. Cadet, David Medina-Ortiz, Mehdi D. Davari, Ramanathan Sowdhamini, Cedric Damour, Yu Li, Alain Miranville, Frederic Cadet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02680">From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein design with desirable properties has been a significant challenge for many decades. Generative artificial intelligence is a promising approach and has achieved great success in various protein generation tasks. Notably, diffusion models stand out for their robust mathematical foundations and impressive generative capabilities, offering unique advantages in certain applications such as protein design. In this review, we first give the definition and characteristics of diffusion models and then focus on two strategies: Denoising Diffusion Probabilistic Models and Score-based Generative Models, where DDPM is the discrete form of SGM. Furthermore, we discuss their applications in protein design, peptide generation, drug discovery, and protein-ligand interaction. Finally, we outline the future perspectives of diffusion models to advance autonomous protein design and engineering. The E(3) group consists of all rotations, reflections, and translations in three-dimensions. The equivariance on the E(3) group can keep the physical stability of the frame of each amino acid as much as possible, and we reflect on how to keep the diffusion model E(3) equivariant for protein generation.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2412.19812.pdf' target='_blank'>https://arxiv.org/pdf/2412.19812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Conghao Wang, Jagath C. Rajapakse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19812">Pharmacophore-guided de novo drug design with diffusion bridge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>De novo design of bioactive drug molecules with potential to treat desired biological targets is a profound task in the drug discovery process. Existing approaches tend to leverage the pocket structure of the target protein to condition the molecule generation. However, even the pocket area of the target protein may contain redundant information since not all atoms in the pocket is responsible for the interaction with the ligand. In this work, we propose PharmacoBridge, a phamacophore-guided de novo design approach to generate drug candidates inducing desired bioactivity via diffusion bridge. Our method adapts the diffusion bridge to effectively convert pharmacophore arrangements in the spatial space into molecular structures under the manner of SE(3)-equivariant transformation, providing sophisticated control over optimal biochemical feature arrangements on the generated molecules. PharmacoBridge is demonstrated to generate hit candidates that exhibit high binding affinity with potential protein targets.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2412.18633.pdf' target='_blank'>https://arxiv.org/pdf/2412.18633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars L. Schaaf, Ilyes Batatia, Christoph Brunken, Thomas D. Barrett, Jules Tilly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18633">BoostMD: Accelerating molecular sampling by leveraging ML force field features from previous time-steps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating atomic-scale processes, such as protein dynamics and catalytic reactions, is crucial for advancements in biology, chemistry, and materials science. Machine learning force fields (MLFFs) have emerged as powerful tools that achieve near quantum mechanical accuracy, with promising generalization capabilities. However, their practical use is often limited by long inference times compared to classical force fields, especially when running extensive molecular dynamics (MD) simulations required for many biological applications. In this study, we introduce BoostMD, a surrogate model architecture designed to accelerate MD simulations. BoostMD leverages node features computed at previous time steps to predict energies and forces based on positional changes. This approach reduces the complexity of the learning task, allowing BoostMD to be both smaller and significantly faster than conventional MLFFs. During simulations, the computationally intensive reference MLFF is evaluated only every $N$ steps, while the lightweight BoostMD model handles the intermediate steps at a fraction of the computational cost. Our experiments demonstrate that BoostMD achieves an eight-fold speedup compared to the reference model and generalizes to unseen dipeptides. Furthermore, we find that BoostMD accurately samples the ground-truth Boltzmann distribution when running molecular dynamics. By combining efficient feature reuse with a streamlined architecture, BoostMD offers a robust solution for conducting large-scale, long-timescale molecular simulations, making high-accuracy ML-driven modeling more accessible and practical.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2412.12101.pdf' target='_blank'>https://arxiv.org/pdf/2412.12101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elana Simon, James Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12101">InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have demonstrated remarkable success in protein modeling and design, yet their internal mechanisms for predicting structure and function remain poorly understood. Here we present a systematic approach to extract and analyze interpretable features from PLMs using sparse autoencoders (SAEs). By training SAEs on embeddings from the PLM ESM-2, we identify up to 2,548 human-interpretable latent features per layer that strongly correlate with up to 143 known biological concepts such as binding sites, structural motifs, and functional domains. In contrast, examining individual neurons in ESM-2 reveals up to 46 neurons per layer with clear conceptual alignment across 15 known concepts, suggesting that PLMs represent most concepts in superposition. Beyond capturing known annotations, we show that ESM-2 learns coherent concepts that do not map onto existing annotations and propose a pipeline using language models to automatically interpret novel latent features learned by the SAEs. As practical applications, we demonstrate how these latent features can fill in missing annotations in protein databases and enable targeted steering of protein sequence generation. Our results demonstrate that PLMs encode rich, interpretable representations of protein biology and we propose a systematic framework to extract and analyze these latent features. In the process, we recover both known biology and potentially new protein motifs. As community resources, we introduce InterPLM (interPLM.ai), an interactive visualization platform for exploring and analyzing learned PLM features, and release code for training and analysis at github.com/ElanaPearl/interPLM.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2412.05776.pdf' target='_blank'>https://arxiv.org/pdf/2412.05776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azwad Tamir, Jiann-Shiun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05776">ProtGO: A Transformer based Fusion Model for accurately predicting Gene Ontology (GO) Terms from full scale Protein Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in next generation sequencing technology have led to the creation of extensive, open-source protein databases consisting of hundreds of millions of sequences. To render these sequences applicable in biomedical applications, they must be meticulously annotated by wet lab testing or extracting them from existing literature. Over the last few years, researchers have developed numerous automatic annotation systems, particularly deep learning models based on machine learning and artificial intelligence, to address this issue. In this work, we propose a transformer-based fusion model capable of predicting Gene Ontology (GO) terms from full-scale protein sequences, achieving state-of-the-art accuracy compared to other contemporary machine learning annotation systems. The approach performs particularly well on clustered split datasets, which comprise training and testing samples originating from distinct distributions that are structurally diverse. This demonstrates that the model is able to understand both short and long term dependencies within the enzyme's structure and can precisely identify the motifs associated with the various GO terms. Furthermore, the technique is lightweight and less computationally expensive compared to the benchmark methods, while at the same time not unaffected by sequence length, rendering it appropriate for diverse applications with varying sequence lengths.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2411.17669.pdf' target='_blank'>https://arxiv.org/pdf/2411.17669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Burak Suyunu, Enes Taylan, Arzucan ÃzgÃ¼r
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17669">Linguistic Laws Meet Protein Sequences: A Comparative Analysis of Subword Tokenization Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tokenization is a crucial step in processing protein sequences for machine learning models, as proteins are complex sequences of amino acids that require meaningful segmentation to capture their functional and structural properties. However, existing subword tokenization methods, developed primarily for human language, may be inadequate for protein sequences, which have unique patterns and constraints. This study evaluates three prominent tokenization approaches, Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varying vocabulary sizes (400-6400), analyzing their effectiveness in protein sequence representation, domain boundary preservation, and adherence to established linguistic laws. Our comprehensive analysis reveals distinct behavioral patterns among these tokenizers, with vocabulary size significantly influencing their performance. BPE demonstrates better contextual specialization and marginally better domain boundary preservation at smaller vocabularies, while SentencePiece achieves better encoding efficiency, leading to lower fertility scores. WordPiece offers a balanced compromise between these characteristics. However, all tokenizers show limitations in maintaining protein domain integrity, particularly as vocabulary size increases. Analysis of linguistic law adherence shows partial compliance with Zipf's and Brevity laws but notable deviations from Menzerath's law, suggesting that protein sequences may follow distinct organizational principles from natural languages. These findings highlight the limitations of applying traditional NLP tokenization methods to protein sequences and emphasize the need for developing specialized tokenization strategies that better account for the unique characteristics of proteins.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2411.15618.pdf' target='_blank'>https://arxiv.org/pdf/2411.15618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian B. Hinz, Matthew R. Masters, Julia N. Kieu, Amr H. Mahmoud, Markus A. Lill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15618">Accelerated Hydration Site Localization and Thermodynamic Profiling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Water plays a fundamental role in the structure and function of proteins and other biomolecules. The thermodynamic profile of water molecules surrounding a protein are critical for ligand binding and recognition. Therefore, identifying the location and thermodynamic behavior of relevant water molecules is important for generating and optimizing lead compounds for affinity and selectivity to a given target. Computational methods have been developed to identify these hydration sites, but are largely limited to simplified models that fail to capture multi-body interactions, or dynamics-based methods that rely on extensive sampling. Here we present a method for fast and accurate localization and thermodynamic profiling of hydration sites for protein structures. The method is based on a geometric deep neural network trained on a large, novel dataset of explicit water molecular dynamics simulations. We confirm the accuracy and robustness of our model on experimental data and demonstrate it's utility on several case studies.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2411.11530.pdf' target='_blank'>https://arxiv.org/pdf/2411.11530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Zhang, Jian K. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11530">SeqProFT: Applying LoRA Finetuning for Sequence-only Protein Property Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) are capable of learning the relationships between protein sequences and functions by treating amino acid sequences as textual data in a self-supervised manner. However, fine-tuning these models typically demands substantial computational resources and time, with results that may not always be optimized for specific tasks. To overcome these challenges, this study employs the LoRA method to perform end-to-end fine-tuning of the ESM-2 model specifically for protein property prediction tasks, utilizing only sequence information. Additionally, a multi-head attention mechanism is integrated into the downstream network to combine sequence features with contact map information, thereby enhancing the model's comprehension of protein sequences. Experimental results of extensive classification and regression tasks demonstrate that the fine-tuned model achieves strong performance and faster convergence across multiple regression and classification tasks.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2410.21335.pdf' target='_blank'>https://arxiv.org/pdf/2410.21335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Yu Liang, Jun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21335">E(3)-invariant diffusion model for pocket-aware peptide generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biologists frequently desire protein inhibitors for a variety of reasons, including use as research tools for understanding biological processes and application to societal problems in agriculture, healthcare, etc. Immunotherapy, for instance, relies on immune checkpoint inhibitors to block checkpoint proteins, preventing their binding with partner proteins and boosting immune cell function against abnormal cells. Inhibitor discovery has long been a tedious process, which in recent years has been accelerated by computational approaches. Advances in artificial intelligence now provide an opportunity to make inhibitor discovery smarter than ever before. While extensive research has been conducted on computer-aided inhibitor discovery, it has mainly focused on either sequence-to-structure mapping, reverse mapping, or bio-activity prediction, making it unrealistic for biologists to utilize such tools. Instead, our work proposes a new method of computer-assisted inhibitor discovery: de novo pocket-aware peptide structure and sequence generation network. Our approach consists of two sequential diffusion models for end-to-end structure generation and sequence prediction. By leveraging angle and dihedral relationships between backbone atoms, we ensure an E(3)-invariant representation of peptide structures. Our results demonstrate that our method achieves comparable performance to state-of-the-art models, highlighting its potential in pocket-aware peptide design. This work offers a new approach for precise drug discovery using receptor-specific peptide generation.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2410.21069.pdf' target='_blank'>https://arxiv.org/pdf/2410.21069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqi Ling, Cheng Cai, Demin Kong, Zhisheng Wei, Jing Wu, Lei Wang, Zhaohong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21069">EMOCPD: Efficient Attention-based Models for Computational Protein Design Using Amino Acid Microenvironment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational protein design (CPD) refers to the use of computational methods to design proteins. Traditional methods relying on energy functions and heuristic algorithms for sequence design are inefficient and do not meet the demands of the big data era in biomolecules, with their accuracy limited by the energy functions and search algorithms. Existing deep learning methods are constrained by the learning capabilities of the networks, failing to extract effective information from sparse protein structures, which limits the accuracy of protein design. To address these shortcomings, we developed an Efficient attention-based Models for Computational Protein Design using amino acid microenvironment (EMOCPD). It aims to predict the category of each amino acid in a protein by analyzing the three-dimensional atomic environment surrounding the amino acids, and optimize the protein based on the predicted high-probability potential amino acid categories. EMOCPD employs a multi-head attention mechanism to focus on important features in the sparse protein microenvironment and utilizes an inverse residual structure to optimize the network architecture. The proposed EMOCPD achieves over 80% accuracy on the training set and 68.33% and 62.32% accuracy on two independent test sets, respectively, surpassing the best comparative methods by over 10%. In protein design, the thermal stability and protein expression of the predicted mutants from EMOCPD show significant improvements compared to the wild type, effectively validating EMOCPD's potential in designing superior proteins. Furthermore, the predictions of EMOCPD are influenced positively, negatively, or have minimal impact based on the content of the 20 amino acids, categorizing amino acids as positive, negative, or neutral. Research findings indicate that EMOCPD is more suitable for designing proteins with lower contents of negative amino acids.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2410.18101.pdf' target='_blank'>https://arxiv.org/pdf/2410.18101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhi Xu, Haowei Ni, Qinhui Gao, Chia-Hua Chang, Yanran Huo, Fanyu Zhao, Shiyu Hu, Wei Xia, Yike Zhang, Radu Grovu, Min He, John. Z. H. Zhang, Yuanqing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18101">Molecular Dynamics and Machine Learning Unlock Possibilities in Beauty Design -- A Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational molecular design -- the endeavor to design molecules, with various missions, aided by machine learning and molecular dynamics approaches, has been widely applied to create valuable new molecular entities, from small molecule therapeutics to protein biologics. In the small data regime, physics-based approaches model the interaction between the molecule being designed and proteins of key physiological functions, providing structural insights into the mechanism. When abundant data has been collected, a quantitative structure-activity relationship (QSAR) can be more directly constructed from experimental data, from which machine learning can distill key insights to guide the design of the next round of experiment design. Machine learning methodologies can also facilitate physical modeling, from improving the accuracy of force fields and extending them to unseen chemical spaces, to more directly enhancing the sampling on the conformational spaces. We argue that these techniques are mature enough to be applied to not just extend the longevity of life, but the beauty it manifests. In this perspective, we review the current frontiers in the research \& development of skin care products, as well as the statistical and physical toolbox applicable to addressing the challenges in this industry. Feasible interdisciplinary research projects are proposed to harness the power of machine learning tools to design innovative, effective, and inexpensive skin care products.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2410.15849.pdf' target='_blank'>https://arxiv.org/pdf/2410.15849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shikhar Vashistha, Neetesh Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15849">Focus Where It Matters: Graph Selective State Focused Attention Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional graph neural networks (GNNs) lack scalability and lose individual node characteristics due to over-smoothing, especially in the case of deeper networks. This results in sub-optimal feature representation, affecting the model's performance on tasks involving dynamically changing graphs. To address this issue, we present Graph Selective States Focused Attention Networks (GSANs) based neural network architecture for graph-structured data. The GSAN is enabled by multi-head masked self-attention (MHMSA) and selective state space modeling (S3M) layers to overcome the limitations of GNNs. In GSAN, the MHMSA allows GSAN to dynamically emphasize crucial node connections, particularly in evolving graph environments. The S3M layer enables the network to adjust dynamically in changing node states and improving predictions of node behavior in varying contexts without needing primary knowledge of the graph structure. Furthermore, the S3M layer enhances the generalization of unseen structures and interprets how node states influence link importance. With this, GSAN effectively outperforms inductive and transductive tasks and overcomes the issues that traditional GNNs experience. To analyze the performance behavior of GSAN, a set of state-of-the-art comparative experiments are conducted on graphs benchmark datasets, including $Cora$, $Citeseer$, $Pubmed$ network citation, and $protein-protein-interaction$ datasets, as an outcome, GSAN improved the classification accuracy by $1.56\%$, $8.94\%$, $0.37\%$, and $1.54\%$ on $F1-score$ respectively.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2410.15128.pdf' target='_blank'>https://arxiv.org/pdf/2410.15128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haibo Wang, Yuxuan Qiu, Yanze Wang, Rob Brekelmans, Yuanqi Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15128">Generalized Flow Matching for Transition Dynamics Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating transition dynamics between metastable states is a fundamental challenge in dynamical systems and stochastic processes with wide real-world applications in understanding protein folding, chemical reactions and neural activities. However, the computational challenge often lies on sampling exponentially many paths in which only a small fraction ends in the target metastable state due to existence of high energy barriers. To amortize the cost, we propose a data-driven approach to warm-up the simulation by learning nonlinear interpolations from local dynamics. Specifically, we infer a potential energy function from local dynamics data. To find plausible paths between two metastable states, we formulate a generalized flow matching framework that learns a vector field to sample propable paths between the two marginal densities under the learned energy function. Furthermore, we iteratively refine the model by assigning importance weights to the sampled paths and buffering more likely paths for training. We validate the effectiveness of the proposed method to sample probable paths on both synthetic and real-world molecular systems.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2410.03634.pdf' target='_blank'>https://arxiv.org/pdf/2410.03634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Yang, Aadyot Bhatnagar, Jeffrey A. Ruffolo, Ali Madani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03634">Function-Guided Conditional Generation Using Protein Language Models with Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The conditional generation of proteins with desired functions is a key goal for generative models. Existing methods based on prompting of protein language models (PLMs) can generate proteins conditioned on a target functionality, such as a desired enzyme family. However, these methods are limited to simple, tokenized conditioning and have not been shown to generalize to unseen functions. In this study, we propose ProCALM (Protein Conditionally Adapted Language Model), an approach for the conditional generation of proteins using adapters to PLMs. While previous methods have used adapters for structure-conditioned generation from PLMs, our implementation of ProCALM involves finetuning ProGen2 to condition generation based on versatile representations of protein function-e.g. enzyme family, taxonomy, or natural language descriptions. ProCALM matches or exceeds the performance of existing methods at conditional sequence generation from target functions. Impressively, it can also generalize to rare and unseen functions. Overall, ProCALM is a flexible and computationally efficient approach, and we expect that it can be extended to a wide range of generative language models.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2410.00833.pdf' target='_blank'>https://arxiv.org/pdf/2410.00833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erik Jansson, Jonathan Krook, Klas Modin, Ozan Ãktem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00833">Geometric shape matching for recovering protein conformations from single-particle Cryo-EM data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address recovery of the three-dimensional backbone structure of single polypeptide proteins from single-particle cryo-electron microscopy (Cryo-SPA) data. Cryo-SPA produces noisy tomographic projections of electrostatic potentials of macromolecules. From these projections, we use methods from shape analysis to recover the three-dimensional backbone structure. Thus, we view the reconstruction problem as an indirect matching problem, where a point cloud representation of the protein backbone is deformed to match 2D tomography data. The deformations are obtained via the action of a matrix Lie group. By selecting a deformation energy, the optimality conditions are obtained, which lead to computational algorithms for optimal deformations. We showcase our approach on synthetic data, for which we recover the three-dimensional structure of the backbone.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2410.00523.pdf' target='_blank'>https://arxiv.org/pdf/2410.00523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernd Ulmann, Shrish Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00523">Building a simple oscillator based Ising machine for research and education</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oscillator based Ising machines are non-von-Neumann machines ideally suited for solving combinatorial problems otherwise intractable on classic stored-program digital computers due to their run-time complexity. Possible future applications are manifold ranging from quantum simulations to protein folding and are of high academic and commercial interest as well. Described in the following is a very simple such machine aimed at educational and research applications.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2409.13057.pdf' target='_blank'>https://arxiv.org/pdf/2409.13057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Michels, Ramya Bandarupalli, Amin Ahangar Akbari, Thai Le, Hong Xiao, Jing Li, Erik F. Y. Hom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13057">Natural Language Processing Methods for the Study of Protein-Ligand Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Natural Language Processing (NLP) have ignited interest in developing effective methods for predicting protein-ligand interactions (PLIs) given their relevance to drug discovery and protein engineering efforts and the ever-growing volume of biochemical sequence and structural data available. The parallels between human languages and the "languages" used to represent proteins and ligands have enabled the use of NLP machine learning approaches to advance PLI studies. In this review, we explain where and how such approaches have been applied in the recent literature and discuss useful mechanisms such as long short-term memory, transformers, and attention. We conclude with a discussion of the current limitations of NLP methods for the study of PLIs as well as key challenges that need to be addressed in future work.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2408.08341.pdf' target='_blank'>https://arxiv.org/pdf/2408.08341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Yu Liang, Xueting Huang, Tibo Duran, Andrew J. Wiemer, Jun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08341">Exploring Latent Space for Generating Peptide Analogs Using Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating peptides with desired properties is crucial for drug discovery and biotechnology. Traditional sequence-based and structure-based methods often require extensive datasets, which limits their effectiveness. In this study, we proposed a novel method that utilized autoencoder shaped models to explore the protein embedding space, and generate novel peptide analogs by leveraging protein language models. The proposed method requires only a single sequence of interest, avoiding the need for large datasets. Our results show significant improvements over baseline models in similarity indicators of peptide structures, descriptors and bioactivities. The proposed method validated through Molecular Dynamics simulations on TIGIT inhibitors, demonstrates that our method produces peptide analogs with similar yet distinct properties, highlighting its potential to enhance peptide screening processes.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2408.06402.pdf' target='_blank'>https://arxiv.org/pdf/2408.06402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaojiao Guan, Yongxin Ji, Cheng Peng, Wei Zou, Xubo Tang, Jiayu Shang, Yanni Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06402">PhaGO: Protein function annotation for bacteriophages by integrating the genomic context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bacteriophages are viruses that target bacteria, playing a crucial role in microbial ecology. Phage proteins are important in understanding phage biology, such as virus infection, replication, and evolution. Although a large number of new phages have been identified via metagenomic sequencing, many of them have limited protein function annotation. Accurate function annotation of phage proteins presents several challenges, including their inherent diversity and the scarcity of annotated ones. Existing tools have yet to fully leverage the unique properties of phages in annotating protein functions. In this work, we propose a new protein function annotation tool for phages by leveraging the modular genomic structure of phage genomes. By employing embeddings from the latest protein foundation models and Transformer to capture contextual information between proteins in phage genomes, PhaGO surpasses state-of-the-art methods in annotating diverged proteins and proteins with uncommon functions by 6.78% and 13.05% improvement, respectively. PhaGO can annotate proteins lacking homology search results, which is critical for characterizing the rapidly accumulating phage genomes. We demonstrate the utility of PhaGO by identifying 688 potential holins in phages, which exhibit high structural conservation with known holins. The results show the potential of PhaGO to extend our understanding of newly discovered phages.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2408.00220.pdf' target='_blank'>https://arxiv.org/pdf/2408.00220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Su, Yiying Tong, Guo-Wei Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00220">Persistent de Rham-Hodge Laplacians in Eulerian representation for manifold topological learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, topological data analysis has become a trending topic in data science and engineering. However, the key technique of topological data analysis, i.e., persistent homology, is defined on point cloud data, which does not work directly for data on manifolds. Although earlier evolutionary de Rham-Hodge theory deals with data on manifolds, it is inconvenient for machine learning applications because of the numerical inconsistency caused by remeshing the involving manifolds in the Lagrangian representation. In this work, we introduce persistent de Rham-Hodge Laplacian, or persistent Hodge Laplacian (PHL) as an abbreviation, for manifold topological learning. Our PHLs are constructed in the Eulerian representation via structure-persevering Cartesian grids, avoiding the numerical inconsistency over the multiscale manifolds. To facilitate the manifold topological learning, we propose a persistent Hodge Laplacian learning algorithm for data on manifolds or volumetric data. As a proof-of-principle application of the proposed manifold topological learning model, we consider the prediction of protein-ligand binding affinities with two benchmark datasets. Our numerical experiments highlight the power and promise of the proposed method.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2408.00057.pdf' target='_blank'>https://arxiv.org/pdf/2408.00057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Kalifa, Uriel Singer, Kira Radinsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00057">GOProteinGNN: Leveraging Protein Knowledge Graphs for Protein Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins play a vital role in biological processes and are indispensable for living organisms. Accurate representation of proteins is crucial, especially in drug development. Recently, there has been a notable increase in interest in utilizing machine learning and deep learning techniques for unsupervised learning of protein representations. However, these approaches often focus solely on the amino acid sequence of proteins and lack factual knowledge about proteins and their interactions, thus limiting their performance. In this study, we present GOProteinGNN, a novel architecture that enhances protein language models by integrating protein knowledge graph information during the creation of amino acid level representations. Our approach allows for the integration of information at both the individual amino acid level and the entire protein level, enabling a comprehensive and effective learning process through graph-based learning. By doing so, we can capture complex relationships and dependencies between proteins and their functional annotations, resulting in more robust and contextually enriched protein representations. Unlike previous methods, GOProteinGNN uniquely learns the entire protein knowledge graph during training, which allows it to capture broader relational nuances and dependencies beyond mere triplets as done in previous work. We perform a comprehensive evaluation on several downstream tasks demonstrating that GOProteinGNN consistently outperforms previous methods, showcasing its effectiveness and establishing it as a state-of-the-art solution for protein representation learning.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2407.18336.pdf' target='_blank'>https://arxiv.org/pdf/2407.18336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Musa Azeem, Homayoun Valafar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18336">Dihedral Angle Adherence: Evaluating Protein Structure Predictions in the Absence of Experimental Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the 3D structures of proteins is essential in understanding their behavior in the cellular environment. Computational methods of predicting protein structures have advanced, but assessing prediction accuracy remains a challenge. The traditional method, RMSD, relies on experimentally determined structures and lacks insight into improvement areas of predictions. We propose an alternative: analyzing dihedral angles, bypassing the need for the reference structure of an evaluated protein. Our method segments proteins into amino acid subsequences and searches for matches, comparing dihedral angles across numerous proteins to compute a metric using Mahalanobis distance. Evaluated on many predictions, our approach correlates with RMSD and identifies areas for prediction enhancement. This method offers a promising route for accurate protein structure prediction assessment and improvement.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2407.13780.pdf' target='_blank'>https://arxiv.org/pdf/2407.13780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ulrich A. Mbou Sob, Qiulin Li, Miguel ArbesÃº, Oliver Bent, Andries P. Smit, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13780">Generative Model for Small Molecules with Latent Space RL Fine-Tuning to Protein Targets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A specific challenge with deep learning approaches for molecule generation is generating both syntactically valid and chemically plausible molecular string representations. To address this, we propose a novel generative latent-variable transformer model for small molecules that leverages a recently proposed molecular string representation called SAFE. We introduce a modification to SAFE to reduce the number of invalid fragmented molecules generated during training and use this to train our model. Our experiments show that our model can generate novel molecules with a validity rate > 90% and a fragmentation rate < 1% by sampling from a latent space. By fine-tuning the model using reinforcement learning to improve molecular docking, we significantly increase the number of hit candidates for five specific protein targets compared to the pre-trained model, nearly doubling this number for certain targets. Additionally, our top 5% mean docking scores are comparable to the current state-of-the-art (SOTA), and we marginally outperform SOTA on three of the five targets.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2406.16821.pdf' target='_blank'>https://arxiv.org/pdf/2406.16821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Jian, Curtis Wu, Danny Reidenbach, Aditi S. Krishnapriyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16821">General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-Based Drug Design (SBDD) focuses on generating valid ligands that strongly and specifically bind to a designated protein pocket. Several methods use machine learning for SBDD to generate these ligands in 3D space, conditioned on the structure of a desired protein pocket. Recently, diffusion models have shown success here by modeling the underlying distributions of atomic positions and types. While these methods are effective in considering the structural details of the protein pocket, they often fail to explicitly consider the binding affinity. Binding affinity characterizes how tightly the ligand binds to the protein pocket, and is measured by the change in free energy associated with the binding process. It is one of the most crucial metrics for benchmarking the effectiveness of the interaction between a ligand and protein pocket. To address this, we propose BADGER: Binding Affinity Diffusion Guidance with Enhanced Refinement. BADGER is a general guidance method to steer the diffusion sampling process towards improved protein-ligand binding, allowing us to adjust the distribution of the binding affinity between ligands and proteins. Our method is enabled by using a neural network (NN) to model the energy function, which is commonly approximated by AutoDock Vina (ADV). ADV's energy function is non-differentiable, and estimates the affinity based on the interactions between a ligand and target protein receptor. By using a NN as a differentiable energy function proxy, we utilize the gradient of our learned energy function as a guidance method on top of any trained diffusion model. We show that our method improves the binding affinity of generated ligands to their protein receptors by up to 60\%, significantly surpassing previous machine learning methods. We also show that our guidance method is flexible and can be easily applied to other diffusion-based SBDD frameworks.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2406.08511.pdf' target='_blank'>https://arxiv.org/pdf/2406.08511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amira Alakhdar, Barnabas Poczos, Newell Washburn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08511">Diffusion Models in $\textit{De Novo}$ Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as powerful tools for molecular generation, particularly in the context of 3D molecular structures. Inspired by non-equilibrium statistical physics, these models can generate 3D molecular structures with specific properties or requirements crucial to drug discovery. Diffusion models were particularly successful at learning 3D molecular geometries' complex probability distributions and their corresponding chemical and physical properties through forward and reverse diffusion processes. This review focuses on the technical implementation of diffusion models tailored for 3D molecular generation. It compares the performance, evaluation methods, and implementation details of various diffusion models used for molecular generation tasks. We cover strategies for atom and bond representation, architectures of reverse diffusion denoising networks, and challenges associated with generating stable 3D molecular structures. This review also explores the applications of diffusion models in $\textit{de novo}$ drug design and related areas of computational chemistry, such as structure-based drug design, including target-specific molecular generation, molecular docking, and molecular dynamics of protein-ligand complexes. We also cover conditional generation on physical properties, conformation generation, and fragment-based drug design. By summarizing the state-of-the-art diffusion models for 3D molecular generation, this review sheds light on their role in advancing drug discovery as well as their current limitations.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2406.07770.pdf' target='_blank'>https://arxiv.org/pdf/2406.07770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Liu, Saee Gopal Paliwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07770">DualBind: A Dual-Loss Framework for Protein-Ligand Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding affinities is crucial for drug development. Recent advances in machine learning show promising results on this task. However, these methods typically rely heavily on labeled data, which can be scarce or unreliable, or they rely on assumptions like Boltzmann-distributed data that may not hold true in practice. Here, we present DualBind, a novel framework that integrates supervised mean squared error (MSE) with unsupervised denoising score matching (DSM) to accurately learn the binding energy function. DualBind not only addresses the limitations of DSM-only models by providing more accurate absolute affinity predictions but also improves generalizability and reduces reliance on labeled data compared to MSE-only models. Our experimental results demonstrate that DualBind excels in predicting binding affinities and can effectively utilize both labeled and unlabeled data to enhance performance.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2406.05738.pdf' target='_blank'>https://arxiv.org/pdf/2406.05738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Le Menestrel, Manuel Rivas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05738">Smiles2Dock: an open large-scale multi-task dataset for ML-based molecular docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Docking is a crucial component in drug discovery aimed at predicting the binding conformation and affinity between small molecules and target proteins. ML-based docking has recently emerged as a prominent approach, outpacing traditional methods like DOCK and AutoDock Vina in handling the growing scale and complexity of molecular libraries. However, the availability of comprehensive and user-friendly datasets for training and benchmarking ML-based docking algorithms remains limited. We introduce Smiles2Dock, an open large-scale multi-task dataset for molecular docking. We created a framework combining P2Rank and AutoDock Vina to dock 1.7 million ligands from the ChEMBL database against 15 AlphaFold proteins, giving us more than 25 million protein-ligand binding scores. The dataset leverages a wide range of high-accuracy AlphaFold protein models, encompasses a diverse set of biologically relevant compounds and enables researchers to benchmark all major approaches for ML-based docking such as Graph, Transformer and CNN-based methods. We also introduce a novel Transformer-based architecture for docking scores prediction and set it as an initial benchmark for our dataset. Our dataset and code are publicly available to support the development of novel ML-based methods for molecular docking to advance scientific research in this field.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2406.04929.pdf' target='_blank'>https://arxiv.org/pdf/2406.04929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oscar Lao, Konstantinos Zacharopoulos, Apostolos Fournaris, Rossano Schifanella, Ioannis Arapakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04929">Protein pathways as a catalyst to directed evolution of the topology of artificial neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the present article, we propose a paradigm shift on evolving Artificial Neural Networks (ANNs) towards a new bio-inspired design that is grounded on the structural properties, interactions, and dynamics of protein networks (PNs): the Artificial Protein Network (APN). This introduces several advantages previously unrealized by state-of-the-art approaches in NE: (1) We can draw inspiration from how nature, thanks to millions of years of evolution, efficiently encodes protein interactions in the DNA to translate our APN to silicon DNA. This helps bridge the gap between syntax and semantics observed in current NE approaches. (2) We can learn from how nature builds networks in our genes, allowing us to design new and smarter networks through EA evolution. (3) We can perform EA crossover/mutation operations and evolution steps, replicating the operations observed in nature directly on the genotype of networks, thus exploring and exploiting the phenotypic space, such that we avoid getting trapped in sub-optimal solutions. (4) Our novel definition of APN opens new ways to leverage our knowledge about different living things and processes from biology. (5) Using biologically inspired encodings, we can model more complex demographic and ecological relationships (e.g., virus-host or predator-prey interactions), allowing us to optimise for multiple, often conflicting objectives.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2406.03103.pdf' target='_blank'>https://arxiv.org/pdf/2406.03103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawid Zamojski, Agnieszka Gogler, Dorota Scieglinska, Michal Marczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03103">EpidermaQuant: Unsupervised detection and quantification of epidermal differentiation markers on H-DAB-stained images of reconstructed human epidermis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrity of the reconstructed human epidermis generated in vitro could be assessed using histological analyses combined with immunohistochemical staining of keratinocyte differentiation markers. Computer-based analysis of scanned tissue saves the expert time and may improve the accuracy of quantification by eliminating interrater reliability issues. However, technical differences during the preparation and capture of stained images and the presence of multiple artifacts may influence the outcome of computational methods. Using a dataset with 598 unannotated images showing cross-sections of in vitro reconstructed human epidermis stained with DAB-based immunohistochemistry reaction to visualize 4 different keratinocyte differentiation marker proteins (filaggrin, keratin 10, Ki67, HSPA2) and counterstained with hematoxylin, we developed an unsupervised method for the detection and quantification of immunohistochemical staining. The proposed pipeline includes the following steps: (i) color normalization to reduce the variability of pixel intensity values in different samples; (ii) color deconvolution to acquire color channels of the stains used; (iii) morphological operations to find the background area of the image; (iv) automatic image rotation; and (v) finding markers of human epidermal differentiation with clustering. Also, we created a method to exclude images without DAB-stained areas. The most effective combination of methods includes: (i) Reinhard's normalization; (ii) Ruifrok and Johnston color deconvolution method; (iii) proposed image rotation method based on boundary distribution of image intensity; (iv) k-means clustering using DAB stain intensity. These results should enhance the performance of quantitative analysis of protein markers in reconstructed human epidermis samples and enable comparison of their spatial distribution between different experimental conditions.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2406.01572.pdf' target='_blank'>https://arxiv.org/pdf/2406.01572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, Jennifer Listgarten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01572">Unlocking Guidance for Discrete State-Space Diffusion and Flow Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models on discrete state-spaces have a wide range of potential applications, particularly in the domain of natural sciences. In continuous state-spaces, controllable and flexible generation of samples with desired properties has been realized using guidance on diffusion and flow models. However, these guidance approaches are not readily amenable to discrete state-space models. Consequently, we introduce a general and principled method for applying guidance on such models. Our method depends on leveraging continuous-time Markov processes on discrete state-spaces, which unlocks computational tractability for sampling from a desired guided distribution. We demonstrate the utility of our approach, Discrete Guidance, on a range of applications including guided generation of small-molecules, DNA sequences and protein sequences.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2405.18768.pdf' target='_blank'>https://arxiv.org/pdf/2405.18768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divya Nori, Wengong Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18768">RNAFlow: RNA Structure & Sequence Design via Inverse Folding-Based Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing significance of RNA engineering in diverse biological applications has spurred interest in developing AI methods for structure-based RNA design. While diffusion models have excelled in protein design, adapting them for RNA presents new challenges due to RNA's conformational flexibility and the computational cost of fine-tuning large structure prediction models. To this end, we propose RNAFlow, a flow matching model for protein-conditioned RNA sequence-structure design. Its denoising network integrates an RNA inverse folding model and a pre-trained RosettaFold2NA network for generation of RNA sequences and structures. The integration of inverse folding in the structure denoising process allows us to simplify training by fixing the structure prediction network. We further enhance the inverse folding model by conditioning it on inferred conformational ensembles to model dynamic RNA conformations. Evaluation on protein-conditioned RNA structure and sequence generation tasks demonstrates RNAFlow's advantage over existing RNA design methods.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2405.18749.pdf' target='_blank'>https://arxiv.org/pdf/2405.18749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hirofumi Tsuruta, Hiroyuki Yamazaki, Ryota Maeda, Ryotaro Tamura, Akihiro Imura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18749">A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases. To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences. However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets. To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins. AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants. Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences. We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models. These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery. The datasets are available at https://datasets.cognanous.com.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2405.15928.pdf' target='_blank'>https://arxiv.org/pdf/2405.15928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dea Gogishvili, Emmanuel Minois-Genin, Jan van Eck, Sanne Abeln
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15928">PatchProt: Hydrophobic patch prediction using protein foundation models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hydrophobic patches on protein surfaces play important functional roles in protein-protein and protein-ligand interactions. Large hydrophobic surfaces are also involved in the progression of aggregation diseases. Predicting exposed hydrophobic patches from a protein sequence has been shown to be a difficult task. Fine-tuning foundation models allows for adapting a model to the specific nuances of a new task using a much smaller dataset. Additionally, multi-task deep learning offers a promising solution for addressing data gaps, simultaneously outperforming single-task methods. In this study, we harnessed a recently released leading large language model ESM-2. Efficient fine-tuning of ESM-2 was achieved by leveraging a recently developed parameter-efficient fine-tuning method. This approach enabled comprehensive training of model layers without excessive parameters and without the need to include a computationally expensive multiple sequence analysis. We explored several related tasks, at local (residue) and global (protein) levels, to improve the representation of the model. As a result, our fine-tuned ESM-2 model, PatchProt, cannot only predict hydrophobic patch areas but also outperforms existing methods at predicting primary tasks, including secondary structure and surface accessibility predictions. Importantly, our analysis shows that including related local tasks can improve predictions on more difficult global tasks. This research sets a new standard for sequence-based protein property prediction and highlights the remarkable potential of fine-tuning foundation models enriching the model representation by training over related tasks.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2405.15840.pdf' target='_blank'>https://arxiv.org/pdf/2405.15840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benoit Gaujac, JÃ©rÃ©mie DonÃ, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, Thomas D. Barrett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15840">Learning the Language of Protein Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Representation learning and \emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-5 Ã. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2405.08031.pdf' target='_blank'>https://arxiv.org/pdf/2405.08031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Gharizadeh, Karim Abbasi, Amin Ghareyazi, Mohammad R. K. Mofrad, Hamid R. Rabiee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08031">HGTDR: Advancing Drug Repurposing with Heterogeneous Graph Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: Drug repurposing is a viable solution for reducing the time and cost associated with drug development. However, thus far, the proposed drug repurposing approaches still need to meet expectations. Therefore, it is crucial to offer a systematic approach for drug repurposing to achieve cost savings and enhance human lives. In recent years, using biological network-based methods for drug repurposing has generated promising results. Nevertheless, these methods have limitations. Primarily, the scope of these methods is generally limited concerning the size and variety of data they can effectively handle. Another issue arises from the treatment of heterogeneous data, which needs to be addressed or converted into homogeneous data, leading to a loss of information. A significant drawback is that most of these approaches lack end-to-end functionality, necessitating manual implementation and expert knowledge in certain stages. Results: We propose a new solution, HGTDR (Heterogeneous Graph Transformer for Drug Repurposing), to address the challenges associated with drug repurposing. HGTDR is a three-step approach for knowledge graph-based drug re-purposing: 1) constructing a heterogeneous knowledge graph, 2) utilizing a heterogeneous graph transformer network, and 3) computing relationship scores using a fully connected network. By leveraging HGTDR, users gain the ability to manipulate input graphs, extract information from diverse entities, and obtain their desired output. In the evaluation step, we demonstrate that HGTDR performs comparably to previous methods. Furthermore, we review medical studies to validate our method's top ten drug repurposing suggestions, which have exhibited promising results. We also demon-strated HGTDR's capability to predict other types of relations through numerical and experimental validation, such as drug-protein and disease-protein inter-relations.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2405.07452.pdf' target='_blank'>https://arxiv.org/pdf/2405.07452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karim Abbasi, Parvin Razzaghi, Amin Ghareyazi, Hamid R. Rabiee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07452">PLA-SGCN: Protein-Ligand Binding Affinity Prediction by Integrating Similar Pairs and Semi-supervised Graph Convolutional Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The protein-ligand binding affinity (PLA) prediction goal is to predict whether or not the ligand could bind to a protein sequence. Recently, in PLA prediction, deep learning has received much attention. Two steps are involved in deep learning-based approaches: feature extraction and task prediction step. Many deep learning-based approaches concentrate on introducing new feature extraction networks or integrating auxiliary knowledge like protein-protein interaction networks or gene ontology knowledge. Then, a task prediction network is designed simply using some fully connected layers. This paper aims to integrate retrieved similar hard protein-ligand pairs in PLA prediction (i.e., task prediction step) using a semi-supervised graph convolutional network (GCN). Hard protein-ligand pairs are retrieved for each input query sample based on the manifold smoothness constraint. Then, a graph is learned automatically in which each node is a protein-ligand pair, and each edge represents the similarity between pairs. In other words, an end-to-end framework is proposed that simultaneously retrieves hard similar samples, learns protein-ligand descriptor, learns the graph topology of the input sample with retrieved similar hard samples (learn adjacency matrix), and learns a semi-supervised GCN to predict the binding affinity (as task predictor). The training step adjusts the parameter values, and in the inference step, the learned model is fine-tuned for each input sample. To evaluate the proposed approach, it is applied to the four well-known PDBbind, Davis, KIBA, and BindingDB datasets. The results show that the proposed method significantly performs better than the comparable approaches.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2405.06729.pdf' target='_blank'>https://arxiv.org/pdf/2405.06729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleix Lafita, Ferran Gonzalez, Mahmoud Hossam, Paul Smyth, Jacob Deasy, Ari Allyn-Feuer, Daniel Seaton, Stephen Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06729">Fine-tuning Protein Language Models with Deep Mutational Scanning improves Variant Effect Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein Language Models (PLMs) have emerged as performant and scalable tools for predicting the functional impact and clinical significance of protein-coding variants, but they still lag experimental accuracy. Here, we present a novel fine-tuning approach to improve the performance of PLMs with experimental maps of variant effects from Deep Mutational Scanning (DMS) assays using a Normalised Log-odds Ratio (NLR) head. We find consistent improvements in a held-out protein test set, and on independent DMS and clinical variant annotation benchmarks from ProteinGym and ClinVar. These findings demonstrate that DMS is a promising source of sequence diversity and supervised training data for improving the performance of PLMs for variant effect prediction.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2405.06654.pdf' target='_blank'>https://arxiv.org/pdf/2405.06654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Qiang, Wenxian Shi, Yuxuan Song, Menghua Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06654">PROflow: An iterative refinement model for PROTAC-induced structure prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteolysis targeting chimeras (PROTACs) are small molecules that trigger the breakdown of traditionally ``undruggable'' proteins by binding simultaneously to their targets and degradation-associated proteins. A key challenge in their rational design is understanding their structural basis of activity. Due to the lack of crystal structures (18 in the PDB), existing PROTAC docking methods have been forced to simplify the problem into a distance-constrained protein-protein docking task. To address the data issue, we develop a novel pseudo-data generation scheme that requires only binary protein-protein complexes. This new dataset enables PROflow, an iterative refinement model for PROTAC-induced structure prediction that models the full PROTAC flexibility during constrained protein-protein docking. PROflow outperforms the state-of-the-art across docking metrics and runtime. Its inference speed enables the large-scale screening of PROTAC designs, and computed properties of predicted structures achieve statistically significant correlations with published degradation activities.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2405.06301.pdf' target='_blank'>https://arxiv.org/pdf/2405.06301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Lindsay, Sian Lindsay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06301">Learning from String Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Universal Similarity Metric (USM) has been demonstrated to give practically useful measures of "similarity" between sequence data. Here we have used the USM as an alternative distance metric in a K-Nearest Neighbours (K-NN) learner to allow effective pattern recognition of variable length sequence data. We compare this USM approach with the commonly used string-to-word vector approach. Our experiments have used two data sets of divergent domains: (1) spam email filtering and (2) protein subcellular localization. Our results with this data reveal that the USM-based K-NN learner (1) gives predictions with higher classification accuracy than those output by techniques that use the string-to-word vector approach, and (2) can be used to generate reliable probability forecasts.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2404.17144.pdf' target='_blank'>https://arxiv.org/pdf/2404.17144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon J. Ward, Muhamed Baljevic, Sharon M. Weiss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17144">Sensor Response-Time Reduction using Long-Short Term Memory Network Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The response time of a biosensor is a crucial metric in safety-critical applications such as medical diagnostics where an earlier diagnosis can markedly improve patient outcomes. However, the speed at which a biosensor reaches a final equilibrium state can be limited by poor mass transport and long molecular diffusion times that increase the time it takes target molecules to reach the active sensing region of a biosensor. While optimization of system and sensor design can promote molecules reaching the sensing element faster, a simpler and complementary approach for response time reduction that is widely applicable across all sensor platforms is to use time-series forecasting to predict the ultimate steady-state sensor response. In this work, we show that ensembles of long short-term memory (LSTM) networks can accurately predict equilibrium biosensor response from a small quantity of initial time-dependent biosensor measurements, allowing for significant reduction in response time by a mean and median factor of improvement of 18.6 and 5.1 respectively. The ensemble of models simultaneously estimates uncertainty, which is vital for ensuring confidence in the predictions and subsequent safety-related decisions that are made. This approach is demonstrated on real-time experimental data collected by exposing porous silicon biosensors to buffered protein solutions using a multi-channel fluidic cell that enables the automated measurement of 100 porous silicon biosensors in parallel. The dramatic improvement in sensor response time achieved using LSTM network ensembles and associated uncertainty quantification opens the door to trustworthy and faster responding biosensors, enabling more rapid medical diagnostics for faster clinical decision making that can lead to improved patient outcomes and healthcare access, as well as quicker identification of toxins in food and the environment.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2404.10178.pdf' target='_blank'>https://arxiv.org/pdf/2404.10178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chentianye Xu, Xueying Zhan, Min Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10178">CryoMAE: Few-Shot Cryo-EM Particle Picking with Masked Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryo-electron microscopy (cryo-EM) emerges as a pivotal technology for determining the architecture of cells, viruses, and protein assemblies at near-atomic resolution. Traditional particle picking, a key step in cryo-EM, struggles with manual effort and automated methods' sensitivity to low signal-to-noise ratio (SNR) and varied particle orientations. Furthermore, existing neural network (NN)-based approaches often require extensive labeled datasets, limiting their practicality. To overcome these obstacles, we introduce cryoMAE, a novel approach based on few-shot learning that harnesses the capabilities of Masked Autoencoders (MAE) to enable efficient selection of single particles in cryo-EM images. Contrary to conventional NN-based techniques, cryoMAE requires only a minimal set of positive particle images for training yet demonstrates high performance in particle detection. Furthermore, the implementation of a self-cross similarity loss ensures distinct features for particle and background regions, thereby enhancing the discrimination capability of cryoMAE. Experiments on large-scale cryo-EM datasets show that cryoMAE outperforms existing state-of-the-art (SOTA) methods, improving 3D reconstruction resolution by up to 22.4%.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2404.02003.pdf' target='_blank'>https://arxiv.org/pdf/2404.02003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02003">AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2404.00766.pdf' target='_blank'>https://arxiv.org/pdf/2404.00766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puneet Mehrotra, Vaastav Anand, Daniel Margo, Milad Rezaei Hajidehi, Margo Seltzer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00766">SoK: The Faults in our Graph Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph-structured data is prevalent in domains such as social networks, financial transactions, brain networks, and protein interactions. As a result, the research community has produced new databases and analytics engines to process such data. Unfortunately, there is not yet widespread benchmark standardization in graph processing, and the heterogeneity of evaluations found in the literature can lead researchers astray. Evaluations frequently ignore datasets' statistical idiosyncrasies, which significantly affect system performance. Scalability studies often use datasets that fit easily in memory on a modest desktop. Some studies rely on synthetic graph generators, but these generators produce graphs with unnatural characteristics that also affect performance, producing misleading results. Currently, the community has no consistent and principled manner with which to compare systems and provide guidance to developers who wish to select the system most suited to their application.
  We provide three different systematizations of benchmarking practices. First, we present a 12-year literary review of graph processing benchmarking, including a summary of the prevalence of specific datasets and benchmarks used in these papers. Second, we demonstrate the impact of two statistical properties of datasets that drastically affect benchmark performance. We show how different assignments of IDs to vertices, called vertex orderings, dramatically alter benchmark performance due to the caching behavior they induce. We also show the impact of zero-degree vertices on the runtime of benchmarks such as breadth-first search and single-source shortest path. We show that these issues can cause performance to change by as much as 38% on several popular graph processing systems. Finally, we suggest best practices to account for these issues when evaluating graph systems.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2403.04106.pdf' target='_blank'>https://arxiv.org/pdf/2403.04106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro LiÃ², Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04106">Understanding Biology in the Age of Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2402.01481.pdf' target='_blank'>https://arxiv.org/pdf/2402.01481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01481">Pre-Training Protein Bi-level Representation Through Span Mask Strategy On 3D Protein Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein chains to learn meaningful representations of both residues and atoms. This leads to a simple yet effective approach to learning protein representation suitable for diverse downstream tasks. Extensive experimental results on binding site prediction and function prediction tasks demonstrate our proposed pre-training approach significantly outperforms other methods. Our code will be made public.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2401.06936.pdf' target='_blank'>https://arxiv.org/pdf/2401.06936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinru Hua, Rasool Ahmad, Jose Blanchet, Wei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06936">Accelerated Sampling of Rare Events using a Neural Network Bias Potential</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of computational physics and material science, the efficient sampling of rare events occurring at atomic scale is crucial. It aids in understanding mechanisms behind a wide range of important phenomena, including protein folding, conformal changes, chemical reactions and materials diffusion and deformation. Traditional simulation methods, such as Molecular Dynamics and Monte Carlo, often prove inefficient in capturing the timescale of these rare events by brute force. In this paper, we introduce a practical approach by combining the idea of importance sampling with deep neural networks (DNNs) that enhance the sampling of these rare events. In particular, we approximate the variance-free bias potential function with DNNs which is trained to maximize the probability of rare event transition under the importance potential function. This method is easily scalable to high-dimensional problems and provides robust statistical guarantees on the accuracy of the estimated probability of rare event transition. Furthermore, our algorithm can actively generate and learn from any successful samples, which is a novel improvement over existing methods. Using a 2D system as a test bed, we provide comparisons between results obtained from different training strategies, traditional Monte Carlo sampling and numerically solved optimal bias potential function under different temperatures. Our numerical results demonstrate the efficacy of the DNN-based importance sampling of rare events.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2401.02124.pdf' target='_blank'>https://arxiv.org/pdf/2401.02124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeynep Hilal Kilimci, Mustafa Yalcin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02124">ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify anticancer peptides for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBert, BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and 88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2512.23175.pdf' target='_blank'>https://arxiv.org/pdf/2512.23175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungeon Lee, Takuto Koyama, Itsuki Maeda, Shigeyuki Matsumoto, Yasushi Okuno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23175">HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2512.22335.pdf' target='_blank'>https://arxiv.org/pdf/2512.22335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22335">Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2512.22007.pdf' target='_blank'>https://arxiv.org/pdf/2512.22007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22007">DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the binding affinity between antigens and antibodies is fundamental to drug discovery and vaccine development. Traditional computational approaches often rely on experimentally determined 3D structures, which are scarce and computationally expensive to obtain. This paper introduces DuaDeep-SeqAffinity, a novel sequence-only deep learning framework that predicts affinity scores solely from their amino acid sequences using a dual-stream hybrid architecture. Our approach leverages pre-trained ESM-2 protein language model embeddings, combining 1D Convolutional Neural Networks (CNNs) for local motif detection with Transformer encoders for global contextual representation. A subsequent fusion module integrates these multi-faceted features, which are then passed to a fully connected network for final score regression. Experimental results demonstrate that DuaDeep-SeqAffinity significantly outperforms individual architectural components and existing state-of-the-art (SOTA) methods. DuaDeep achieved a superior Pearson correlation of 0.688, an R^2 of 0.460, and a Root Mean Square Error (RMSE) of 0.737, surpassing single-branch variants ESM-CNN and ESM-Transformer. Notably, the model achieved an Area Under the Curve (AUC) of 0.890, outperforming sequence-only benchmarks and even surpassing structure-sequence hybrid models. These findings prove that high-fidelity sequence embeddings can capture essential binding patterns typically reserved for structural modeling. By eliminating the reliance on 3D structures, DuaDeep-SeqAffinity provides a highly scalable and efficient solution for high-throughput screening of vast sequence libraries, significantly accelerating the therapeutic discovery pipeline.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2512.20924.pdf' target='_blank'>https://arxiv.org/pdf/2512.20924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew D. Blevins, Ian K. Quigley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20924">Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can machine learning models identify which chemist made a molecule from structure alone? If so, models trained on literature data may exploit chemist intent rather than learning causal structure-activity relationships. We test this by linking CHEMBL assays to publication authors and training a 1,815-class classifier to predict authors from molecular fingerprints, achieving 60% top-5 accuracy under scaffold-based splitting. We then train an activity model that receives only a protein identifier and an author-probability vector derived from structure, with no direct access to molecular descriptors. This author-only model achieves predictive power comparable to a simple baseline that has access to structure. This reveals a "Clever Hans" failure mode: models can predict bioactivity largely by inferring chemist goals and favorite targets without requiring a lab-independent understanding of chemistry. We analyze the sources of this leakage, propose author-disjoint splits, and recommend dataset practices to decouple chemist intent from biological outcomes.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2512.12134.pdf' target='_blank'>https://arxiv.org/pdf/2512.12134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>La Ode Aman, A Mu'thi Andy Suryadi, Dizky Ramadani Putri Papeo, Hamsidar Hasan, Ariani H Hutuba, Netty Ino Ischak, Yuszda K. Salimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12134">Modeling Dabrafenib Response Using Multi-Omics Modality Fusion and Protein Network Embeddings Based on Graph Convolutional Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer cell response to targeted therapy arises from complex molecular interactions, making single omics insufficient for accurate prediction. This study develops a model to predict Dabrafenib sensitivity by integrating multiple omics layers (genomics, transcriptomics, proteomics, epigenomics, and metabolomics) with protein network embeddings generated using Graph Convolutional Networks (GCN). Each modality is encoded into low dimensional representations through neural network preprocessing. Protein interaction information from STRING is incorporated using GCN to capture biological topology. An attention based fusion mechanism assigns adaptive weights to each modality according to its relevance. Using GDSC cancer cell line data, the model shows that selective integration of two modalities, especially proteomics and transcriptomics, achieves the best test performance (R2 around 0.96), outperforming all single omics and full multimodal settings. Genomic and epigenomic data were less informative, while proteomic and transcriptomic layers provided stronger phenotypic signals related to MAPK inhibitor activity. These results show that attention guided multi omics fusion combined with GCN improves drug response prediction and reveals complementary molecular determinants of Dabrafenib sensitivity. The approach offers a promising computational framework for precision oncology and predictive modeling of targeted therapies.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2512.10066.pdf' target='_blank'>https://arxiv.org/pdf/2512.10066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongkai Chen, Samuel WK Wong, SC Kou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10066">Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2512.09894.pdf' target='_blank'>https://arxiv.org/pdf/2512.09894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengren, Liu, Yixiang Zhang, Yiming, Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09894">Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2512.09152.pdf' target='_blank'>https://arxiv.org/pdf/2512.09152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter W Fields, Vudtiwat Ngampruetikorn, David J Schwab, Stephanie E Palmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09152">Understanding temperature tuning in energy-based models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models of complex systems often require post-hoc parameter adjustments to produce useful outputs. For example, energy-based models for protein design are sampled at an artificially low ''temperature'' to generate novel, functional sequences. This temperature tuning is a common yet poorly understood heuristic used across machine learning contexts to control the trade-off between generative fidelity and diversity. Here, we develop an interpretable, physically motivated framework to explain this phenomenon. We demonstrate that in systems with a large ''energy gap'' - separating a small fraction of meaningful states from a vast space of unrealistic states - learning from sparse data causes models to systematically overestimate high-energy state probabilities, a bias that lowering the sampling temperature corrects. More generally, we characterize how the optimal sampling temperature depends on the interplay between data size and the system's underlying energy landscape. Crucially, our results show that lowering the sampling temperature is not always desirable; we identify the conditions where \emph{raising} it results in better generative performance. Our framework thus casts post-hoc temperature tuning as a diagnostic tool that reveals properties of the true data distribution and the limits of the learned model.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2512.06592.pdf' target='_blank'>https://arxiv.org/pdf/2512.06592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James King, Lewis Cornwall, Andrei Cristian Nica, James Day, Aaron Sim, Neil Dalchau, Lilly Wollman, Joshua Meyers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06592">On fine-tuning Boltz-2 for protein-protein affinity prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2512.04311.pdf' target='_blank'>https://arxiv.org/pdf/2512.04311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Manuel Cantarero Angulo, Matthew Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04311">Real-time Cricket Sorting By Sex</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2512.03825.pdf' target='_blank'>https://arxiv.org/pdf/2512.03825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aingeru Ramos, Jose A Pascual, Javier Navaridas, Ivan Coluzza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03825">Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2512.03124.pdf' target='_blank'>https://arxiv.org/pdf/2512.03124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Souza, Júlio Araújo, John Kesley Costa, Carlile Lavor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03124">On the Complexity of the Ordered Covering Problem in Distance Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Ordered Covering Problem (OCP) arises in the context of the Discretizable Molecular Distance Geometry Problem (DMDGP), where the ordering of pruning edges significantly impacts the performance of the SBBU algorithm for protein structure determination. In recent work, Souza et al. (2023) formalized OCP as a hypergraph covering problem with ordered, exponential costs, and proposed a greedy heuristic that outperforms the original SBBU ordering by orders of magnitude. However, the computational complexity of finding optimal solutions remained open. In this paper, we prove that OCP is NP-complete through a polynomial-time reduction from the strongly NP-complete 3-Partition problem. Our reduction constructs a tight budget that forces optimal solutions to correspond exactly to valid 3-partitions. This result establishes a computational barrier for optimal edge ordering and provides theoretical justification for the heuristic approaches currently used in practice.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2512.01287.pdf' target='_blank'>https://arxiv.org/pdf/2512.01287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmitry Zankov, Pavlo Polishchuk, Michal Sobieraj, Mario Barbatti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01287">milearn: A Python Package for Multi-Instance Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce milearn, a Python package for multi-instance learning (MIL) that follows the familiar scikit-learn fit/predict interface while providing a unified framework for both classical and neural-network-based MIL algorithms for regression and classification. The package also includes built-in hyperparameter optimization designed specifically for small MIL datasets, enabling robust model selection in data-scarce scenarios. We demonstrate the versatility of milearn across a broad range of synthetic MIL benchmark datasets, including digit classification and regression, molecular property prediction, and protein-protein interaction (PPI) prediction. Special emphasis is placed on the key instance detection (KID) problem, for which the package provides dedicated support.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2512.00376.pdf' target='_blank'>https://arxiv.org/pdf/2512.00376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajit Kumar, IndraPrakash Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00376">Layer Probing Improves Kinase Functional Prediction with Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have transformed sequence-based protein analysis, yet most applications rely only on final-layer embeddings, which may overlook biologically meaningful information encoded in earlier layers. We systematically evaluate all 33 layers of ESM-2 for kinase functional prediction using both unsupervised clustering and supervised classification. We show that mid-to-late transformer layers (layers 20-33) outperform the final layer by 32 percent in unsupervised Adjusted Rand Index and improve homology-aware supervised accuracy to 75.7 percent. Domain-level extraction, calibrated probability estimates, and a reproducible benchmarking pipeline further strengthen reliability. Our results demonstrate that transformer depth contains functionally distinct biological signals and that principled layer selection significantly improves kinase function prediction.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2511.22239.pdf' target='_blank'>https://arxiv.org/pdf/2511.22239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Somnath Mondal, Tinkal Mondal, Soumajit Pramanik, Rukmankesh Mehra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22239">DeepPNI: Language- and graph-based model for mutation-driven protein-nucleic acid energetics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The interaction between proteins and nucleic acids is crucial for processes that sustain cellular function, including DNA maintenance and the regulation of gene expression and translation. Amino acid mutations in protein-nucleic acid complexes often lead to vital diseases. Experimental techniques have their own specific limitations in predicting mutational effects in protein-nucleic acid complexes. In this study, we compiled a large dataset of 1951 mutations including both protein-DNA and protein-RNA complexes and integrated structural and sequential features to build a deep learning-based regression model named DeepPNI. This model estimates mutation-induced binding free energy changes in protein-nucleic acid complexes. The structural features are encoded via edge-aware RGCN and the sequential features are extracted using protein language model ESM-2. We have achieved a high average Pearson correlation coefficient (PCC) of 0.76 in the large dataset via five-fold cross-validation. Consistent performance across individual dataset of protein-DNA, protein-RNA complexes, and different experimental temperature split dataset make the model generalizable. Our model showed good performance in complex-based five-fold cross-validation, which proved its robustness. In addition, DeepPNI outperformed in external dataset validation, and comparison with existing tools
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2511.20783.pdf' target='_blank'>https://arxiv.org/pdf/2511.20783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anderson E. Schwertner, Francisco N. C. Sobral
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20783">A derivative-free trust-region approach for Low Order-Value Optimization problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Low Order-Value Optimization (LOVO) problem involves minimizing the minimum among a finite number of function values within a feasible set. LOVO has several practical applications such as robust parameter estimation, protein alignment, portfolio optimization, among others. In this work, we are interested in the constrained nonlinear optimization LOVO problem of minimizing the minimum between a finite number of function values subject to a nonempty closed convex set where each function is a black-box and continuously differentiable, but the derivatives are not available. We develop the first derivative-free trust-region algorithm for constrained LOVO problems with convergence to weakly critical points. Under suitable conditions, we establish the global convergence of the algorithm and also its worst-case iteration complexity analysis. An initial open-source implementation using only linear interpolation models is developed. Extensive numerical experiments and comparison with existing alternatives show the properties and the efficiency of the proposed approach when solving LOVO problems.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2511.19423.pdf' target='_blank'>https://arxiv.org/pdf/2511.19423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19423">Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2511.19184.pdf' target='_blank'>https://arxiv.org/pdf/2511.19184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lakshaditya Singh, Adwait Shelke, Divyansh Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19184">Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2511.16675.pdf' target='_blank'>https://arxiv.org/pdf/2511.16675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanlue Li, Xufeng Zhao, Fang Wu, Sören Laue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16675">Joint Design of Protein Surface and Structure Using a Diffusion Bridge Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interactions (PPIs) are governed by surface complementarity and hydrophobic interactions at protein interfaces. However, designing diverse and physically realistic protein structure and surfaces that precisely complement target receptors remains a significant challenge in computational protein design. In this work, we introduce PepBridge, a novel framework for the joint design of protein surface and structure that seamlessly integrates receptor surface geometry and biochemical properties. Starting with a receptor surface represented as a 3D point cloud, PepBridge generates complete protein structures through a multi-step process. First, it employs denoising diffusion bridge models (DDBMs) to map receptor surfaces to ligand surfaces. Next, a multi-model diffusion model predicts the corresponding structure, while Shape-Frame Matching Networks ensure alignment between surface geometry and backbone architecture. This integrated approach facilitates surface complementarity, conformational stability, and chemical feasibility. Extensive validation across diverse protein design scenarios demonstrates PepBridge's efficacy in generating structurally viable proteins, representing a significant advancement in the joint design of top-down protein structure.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2511.14781.pdf' target='_blank'>https://arxiv.org/pdf/2511.14781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler L. Hayes, Giri P. Krishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14781">Quantifying the Role of OpenFold Components in Protein Structure Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Models such as AlphaFold2 and OpenFold have transformed protein structure prediction, yet their inner workings remain poorly understood. We present a methodology to systematically evaluate the contribution of individual OpenFold components to structure prediction accuracy. We identify several components that are critical for most proteins, while others vary in importance across proteins. We further show that the contribution of several components is correlated with protein length. These findings provide insight into how OpenFold achieves accurate predictions and highlight directions for interpreting protein prediction networks more broadly.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2511.14056.pdf' target='_blank'>https://arxiv.org/pdf/2511.14056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marios Papamichals, Regina Ruane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14056">Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2511.13791.pdf' target='_blank'>https://arxiv.org/pdf/2511.13791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratik Chakraborty, Aryan Bhargava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13791">XAI-Driven Deep Learning for Protein Sequence Functional Group Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins perform essential biological functions, and accurate classification of their sequences is critical for understanding structure-function relationships, enzyme mechanisms, and molecular interactions. This study presents a deep learning-based framework for functional group classification of protein sequences derived from the Protein Data Bank (PDB). Four architectures were implemented: Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), CNN-BiLSTM hybrid, and CNN with Attention. Each model was trained using k-mer integer encoding to capture both local and long-range dependencies. Among these, the CNN achieved the highest validation accuracy of 91.8%, demonstrating the effectiveness of localized motif detection. Explainable AI techniques, including Grad-CAM and Integrated Gradients, were applied to interpret model predictions and identify biologically meaningful sequence motifs. The discovered motifs, enriched in histidine, aspartate, glutamate, and lysine, represent amino acid residues commonly found in catalytic and metal-binding regions of transferase enzymes. These findings highlight that deep learning models can uncover functionally relevant biochemical signatures, bridging the gap between predictive accuracy and biological interpretability in protein sequence analysis.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2511.13758.pdf' target='_blank'>https://arxiv.org/pdf/2511.13758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun-Hyoung Park, Ho-Jun Song, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13758">ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2511.09529.pdf' target='_blank'>https://arxiv.org/pdf/2511.09529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samyak Sanghvi, Nishant Ranjan, Tarak Karmakar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09529">SiDGen: Structure-informed Diffusion for Generative modeling of Ligands for Proteins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing ligands that are both chemically valid and structurally compatible with protein binding pockets is a key bottleneck in computational drug discovery. Existing approaches either ignore structural context or rely on expensive, memory-intensive encoding that limits throughput and scalability. We present SiDGen (Structure-informed Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness. To balance expressivity with efficiency, SiDGen supports two conditioning pathways: a streamlined mode that pools coarse structural signals from protein embeddings and a full mode that injects localized pairwise biases for stronger coupling. A coarse-stride folding mechanism with nearest-neighbor upsampling alleviates the quadratic memory costs of pair tensors, enabling training on realistic sequence lengths. Learning stability is maintained through in-loop chemical validity checks and an invalidity penalty, while large-scale training efficiency is restored \textit{via} selective compilation, dataloader tuning, and gradient accumulation. In automated benchmarks, SiDGen generates ligands with high validity, uniqueness, and novelty, while achieving competitive performance in docking-based evaluations and maintaining reasonable molecular properties. These results demonstrate that SiDGen can deliver scalable, pocket-aware molecular design, providing a practical route to conditional generation for high-throughput drug discovery.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2511.09216.pdf' target='_blank'>https://arxiv.org/pdf/2511.09216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erik Hartman, Jonas Wallin, Johan Malmström, Jimmy Olsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09216">Controllable protein design through Feynman-Kac steering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based models have recently enabled the generation of realistic and diverse protein structures, yet they remain limited in their ability to steer outcomes toward specific functional or biochemical objectives, such as binding affinity or sequence composition. Here we extend the Feynman-Kac (FK) steering framework, an inference-time control approach, to diffusion-based protein design. By coupling FK steering with structure generation, the method guides sampling toward desirable structural or energetic features while maintaining the diversity of the underlying diffusion process. To enable simultaneous generation of both sequence and structure properties, rewards are computed on models refined through ProteinMPNN and all-atom relaxation. Applied to binder design, FK steering consistently improves predicted interface energetics across diverse targets with minimal computational overhead. More broadly, this work demonstrates that inference-time FK control generalizes diffusion-based protein design to arbitrary, non-differentiable, and reward-agnostic objectives, providing a unified and model-independent framework for guided molecular generation.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2511.05726.pdf' target='_blank'>https://arxiv.org/pdf/2511.05726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Gao, Annie Cheung, Yihao Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05726">GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of protein-ligand binding affinity plays a pivotal role in accelerating the discovery of novel drugs and vaccines, particularly for gastrointestinal (GI) diseases such as gastric ulcers, Crohn's disease, and ulcerative colitis. Traditional computational models often rely on structural information alone and thus fail to capture the genetic determinants that influence disease mechanisms and therapeutic responses. To address this gap, we propose GastroDL-Fusion, a dual-modal deep learning framework that integrates protein-ligand complex data with disease-associated gene sequence information for drug and vaccine development. In our approach, protein-ligand complexes are represented as molecular graphs and modeled using a Graph Isomorphism Network (GIN), while gene sequences are encoded into biologically meaningful embeddings via a pre-trained Transformer (ProtBERT/ESM). These complementary modalities are fused through a multi-layer perceptron to enable robust cross-modal interaction learning. We evaluate the model on benchmark datasets of GI disease-related targets, demonstrating that GastroDL-Fusion significantly improves predictive performance over conventional methods. Specifically, the model achieves a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75, outperforming CNN, BiLSTM, GIN, and Transformer-only baselines. These results confirm that incorporating both structural and genetic features yields more accurate predictions of binding affinities, providing a reliable computational tool for accelerating the design of targeted therapies and vaccines in the context of gastrointestinal diseases.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2511.03032.pdf' target='_blank'>https://arxiv.org/pdf/2511.03032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James C. Bowden, Sergey Levine, Jennifer Listgarten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03032">Leveraging Discrete Function Decomposability for Scientific Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of AI-driven science and engineering, we often want to design discrete objects in silico according to user-specified properties. For example, we may wish to design a protein to bind its target, arrange components within a circuit to minimize latency, or find materials with certain properties. Given a property predictive model, in silico design typically involves training a generative model over the design space (e.g., protein sequence space) to concentrate on designs with the desired properties. Distributional optimization -- which can be formalized as an estimation of distribution algorithm or as reinforcement learning policy optimization -- finds the generative model that maximizes an objective function in expectation. Optimizing a distribution over discrete-valued designs is in general challenging because of the combinatorial nature of the design space. However, many property predictors in scientific applications are decomposable in the sense that they can be factorized over design variables in a way that could in principle enable more effective optimization. For example, amino acids at a catalytic site of a protein may only loosely interact with amino acids of the rest of the protein to achieve maximal catalytic activity. Current distributional optimization algorithms are unable to make use of such decomposability structure. Herein, we propose and demonstrate use of a new distributional optimization algorithm, Decomposition-Aware Distributional Optimization (DADO), that can leverage any decomposability defined by a junction tree on the design variables, to make optimization more efficient. At its core, DADO employs a soft-factorized "search distribution" -- a learned generative model -- for efficient navigation of the search space, invoking graph message-passing to coordinate optimization across linked factors.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2511.01277.pdf' target='_blank'>https://arxiv.org/pdf/2511.01277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Annabelle Martin, Daphne Kontogiorgos-Heintz, Jeff Nivala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01277">Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nanopore protein sequencing produces long, noisy ionic current traces in which key molecular phases, such as protein capture and translocation, are embedded. Capture phases mark the successful entry of a protein into the pore and serve as both a checkpoint and a signal that a channel merits further analysis. However, manual identification of capture phases is time-intensive, often requiring several days for expert reviewers to annotate the data due to the need for domain-specific interpretation of complex signal patterns. To address this, a lightweight one-dimensional convolutional neural network (1D CNN) was developed and trained to detect capture phases in down-sampled signal windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids, histogram-based classifiers, and other CNN variants using run-level data splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and precision of 93.39% on held-out test data. The model supports low-latency inference and is integrated into a dashboard for Oxford Nanopore experiments, reducing the total analysis time from several days to under thirty minutes. These results show that efficient, real-time capture detection is possible using simple, interpretable architectures and suggest a broader role for lightweight ML models in sequencing workflows.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2508.18891.pdf' target='_blank'>https://arxiv.org/pdf/2508.18891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18891">pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or sparse data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates LLM-inspired architectures for the alignment-free fusion of sparse data sources and offers native sparse metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2508.18693.pdf' target='_blank'>https://arxiv.org/pdf/2508.18693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitong Cheng, Yiran Jiang, Yulong Ge, Yufeng Li, Zhongheng Qin, Rongzhi Lin, Jianwei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18693">Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift, characterized by degraded model performance during transition from labeled source domains to unlabeled target domains, poses a persistent challenge for deploying deep learning systems. Current unsupervised domain adaptation (UDA) methods predominantly rely on fine-tuning feature extractors - an approach limited by inefficiency, reduced interpretability, and poor scalability to modern architectures.
  Our analysis reveals that models pretrained on large-scale data exhibit domain-invariant geometric patterns in their feature space, characterized by intra-class clustering and inter-class separation, thereby preserving transferable discriminative structures. These findings indicate that domain shifts primarily manifest as boundary misalignment rather than feature degradation.
  Unlike fine-tuning entire pre-trained models - which risks introducing unpredictable feature distortions - we propose the Feature-space Planes Searcher (FPS): a novel domain adaptation framework that optimizes decision boundaries by leveraging these geometric patterns while keeping the feature encoder frozen. This streamlined approach enables interpretative analysis of adaptation while substantially reducing memory and computational costs through offline feature extraction, permitting full-dataset optimization in a single computation cycle.
  Evaluations on public benchmarks demonstrate that FPS achieves competitive or superior performance to state-of-the-art methods. FPS scales efficiently with multimodal large models and shows versatility across diverse domains including protein structure prediction, remote sensing classification, and earthquake detection. We anticipate FPS will provide a simple, effective, and generalizable paradigm for transfer learning, particularly in domain adaptation tasks. .
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2508.18446.pdf' target='_blank'>https://arxiv.org/pdf/2508.18446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Abbaszadeh, Armita Shahlaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18446">From Prediction to Simulation: AlphaFold 3 as a Differentiable Framework for Structural Biology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AlphaFold 3 represents a transformative advancement in computational biology, enhancing protein structure prediction through novel multi-scale transformer architectures, biologically informed cross-attention mechanisms, and geometry-aware optimization strategies. These innovations dramatically improve predictive accuracy and generalization across diverse protein families, surpassing previous methods. Crucially, AlphaFold 3 embodies a paradigm shift toward differentiable simulation, bridging traditional static structural modeling with dynamic molecular simulations. By reframing protein folding predictions as a differentiable process, AlphaFold 3 serves as a foundational framework for integrating deep learning with physics-based molecular
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2508.16587.pdf' target='_blank'>https://arxiv.org/pdf/2508.16587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rakesh Thakur, Riya Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16587">HemePLM-Diffuse: A Scalable Generative Framework for Protein-Ligand Dynamics in Large Biomolecular System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehending the long-timescale dynamics of protein-ligand complexes is very important for drug discovery and structural biology, but it continues to be computationally challenging for large biomolecular systems. We introduce HemePLM-Diffuse, an innovative generative transformer model that is designed for accurate simulation of protein-ligand trajectories, inpaints the missing ligand fragments, and sample transition paths in systems with more than 10,000 atoms. HemePLM-Diffuse has features of SE(3)-Invariant tokenization approach for proteins and ligands, that utilizes time-aware cross-attentional diffusion to effectively capture atomic motion. We also demonstrate its capabilities using the 3CQV HEME system, showing enhanced accuracy and scalability compared to leading models such as TorchMD-Net, MDGEN, and Uni-Mol.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2508.13421.pdf' target='_blank'>https://arxiv.org/pdf/2508.13421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabrielle Wehr, Reuben Rideaux, Amaya J. Fox, David R. Lightfoot, Jason Tangen, Jason B. Mattingley, Shane E. Ehrhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13421">Virtuous Machines: Towards Artificial General Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2508.12575.pdf' target='_blank'>https://arxiv.org/pdf/2508.12575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zohra Yagoub, Hafida Bouziane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12575">Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of amyloidogenicity in peptides and proteins remains a focal point of ongoing bioinformatics. The crucial step in this field is to apply advanced computational methodologies. Many recent approaches to predicting amyloidogenicity within proteins are highly based on evolutionary motifs and the individual properties of amino acids. It is becoming increasingly evident that the sequence information-based features show high predictive performance. Consequently, our study evaluated the contextual features of protein sequences obtained from a pretrained protein large language model leveraging bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and protein sequences. Our method achieved an accuracy of 84.5% on 10-fold cross-validation and an accuracy of 83% in the test dataset. Our results demonstrate competitive performance, highlighting the potential of LLMs in enhancing the accuracy of amyloid prediction.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2508.10541.pdf' target='_blank'>https://arxiv.org/pdf/2508.10541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Shing-Hei Wong, Joshua Mincheol Kim, Sin-Hang Fung, Qing Xiong, Kelvin Fu-Kiu Ao, Junkang Wei, Ran Wang, Dan Michelle Wang, Jingying Zhou, Bo Feng, Alfred Sze-Lok Cheng, Kevin Y. Yip, Stephen Kwok-Wing Tsui, Qin Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10541">Driving Accurate Allergen Prediction with Protein Language Models and Generalization-Focused Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Allergens, typically proteins capable of triggering adverse immune responses, represent a significant public health challenge. To accurately identify allergen proteins, we introduce Applm (Allergen Prediction with Protein Language Models), a computational framework that leverages the 100-billion parameter xTrimoPGLM protein language model. We show that Applm consistently outperforms seven state-of-the-art methods in a diverse set of tasks that closely resemble difficult real-world scenarios. These include identifying novel allergens that lack similar examples in the training set, differentiating between allergens and non-allergens among homologs with high sequence similarity, and assessing functional consequences of mutations that create few changes to the protein sequences. Our analysis confirms that xTrimoPGLM, originally trained on one trillion tokens to capture general protein sequence characteristics, is crucial for Applm's performance by detecting important differences among protein sequences. In addition to providing Applm as open-source software, we also provide our carefully curated benchmark datasets to facilitate future research.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2508.10117.pdf' target='_blank'>https://arxiv.org/pdf/2508.10117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Manh Son, Pham Huu Vang, Nguyen Thi Dung, Nguyen Manh Ha. Ta Thi Thao, Tran Thi Thu Thuy, Phan Minh Giang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10117">In silico study on the cytotoxicity against Hela cancer cells of xanthones bioactive compounds from Garcinia cowa: QSAR based on Graph Deep Learning, Network Pharmacology, and Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cancer is recognized as a complex group of diseases, contributing to the highest global mortality rates, with increasing prevalence and a trend toward affecting younger populations. It is characterized by uncontrolled proliferation of abnormal cells, invasion of adjacent tissues, and metastasis to distant organs. Garcinia cowa, a traditional medicinal plant widely used in Southeast Asia, including Vietnam, is employed to treat fever, cough, indigestion, as a laxative, and for parasitic diseases. Numerous xanthone compounds isolated from this species exhibit a broad spectrum of biological activities, with some showing promise as anti cancer and antimalarial agents. Network pharmacology analysis successfully identified key bioactive compounds Rubraxanthone, Garcinone D, Norcowanin, Cowanol, and Cowaxanthone alongside their primary protein targets (TNF, CTNNB1, SRC, NFKB1, and MTOR), providing critical insights into the molecular mechanisms underlying their anti-cancer effects. The Graph Attention Network algorithm demonstrated superior predictive performance, achieving an R2 of 0.98 and an RMSE of 0.02 after data augmentation, highlighting its accuracy in predicting pIC50 values for xanthone based compounds. Additionally, molecular docking revealed MTOR as a potential target for inducing cytotoxicity in HeLa cancer cells from Garcinia cowa.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2508.09659.pdf' target='_blank'>https://arxiv.org/pdf/2508.09659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes F. Hevler, Shivam Verma, Mirat Soijtra, Carolyn R. Bertozzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09659">Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal Tracks is a Python-based statistical framework for analyzing protein thermal stability data that overcomes key limitations of existing thermal proteome profiling (TPP) work-flows. Unlike standard approaches that assume sigmoidal melting curves and are constrained by empirical null distributions (limiting significant hits to approximately 5 % of data), Thermal Tracks uses Gaussian Process (GP) models with squared-exponential kernels to flexibly model any melting curve shape while generating unbiased null distributions through kernel priors. This framework is particularly valuable for analyzing proteome-wide perturbations that significantly alter protein thermal stability, such as pathway inhibitions, genetic modifications, or environmental stresses, where conventional TPP methods may miss biologically relevant changes due to their statistical constraints. Furthermore, Thermal Tracks excels at analyzing proteins with un-conventional melting profiles, including phase-separating proteins and membrane proteins, which often exhibit complex, non-sigmoidal thermal stability behaviors. Thermal Tracks is freely available from GitHub and is implemented in Python, providing an accessible and flexible tool for proteome-wide thermal profiling studies.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2508.07887.pdf' target='_blank'>https://arxiv.org/pdf/2508.07887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabrina Namazova, Alessandra Brondetta, Younes Strittmatter, Matthew Nassar, Sebastian Musslick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07887">Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulators have revolutionized scientific practice across the natural sciences. By generating data that reliably approximate real-world phenomena, they enable scientists to accelerate hypothesis testing and optimize experimental designs. This is perhaps best illustrated by AlphaFold, a Nobel-prize winning simulator in chemistry that predicts protein structures from amino acid sequences, enabling rapid prototyping of molecular interactions, drug targets, and protein functions. In the behavioral sciences, a reliable participant simulator - a system capable of producing human-like behavior across cognitive tasks - would represent a similarly transformative advance. Recently, Binz et al. introduced Centaur, a large language model (LLM) fine-tuned on human data from 160 experiments, proposing its use not only as a model of cognition but also as a participant simulator for "in silico prototyping of experimental studies", e.g., to advance automated cognitive science. Here, we review the core criteria for a participant simulator and assess how well Centaur meets them. Although Centaur demonstrates strong predictive accuracy, its generative behavior - a critical criterion for a participant simulator - systematically diverges from human data. This suggests that, while Centaur is a significant step toward predicting human behavior, it does not yet meet the standards of a reliable participant simulator or an accurate model of cognition.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2508.07345.pdf' target='_blank'>https://arxiv.org/pdf/2508.07345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samiha Afaf Neha, Abir Ahammed Bhuiyan, Md. Ishrak Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07345">ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is essential for genomic studies due to their crucial role as structural elements in bacteriophages. Computational tools, particularly machine learning, have emerged for annotating phage protein sequences from high-throughput sequencing. However, effective annotation requires specialized sequence encodings. Our paper introduces ProteoKnight, a new image-based encoding method that addresses spatial constraints in existing techniques, yielding competitive performance in PVP classification using pre-trained convolutional neural networks. Additionally, our study evaluates prediction uncertainty in binary PVP classification through Monte Carlo Dropout (MCD). \textbf{Methods:} ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences were classified using multiple pre-trained CNNs. Variance and entropy measures assessed prediction uncertainty across proteins of various classes and lengths. \textbf{Results:} Our experiments achieved 90.8% accuracy in binary classification, comparable to state-of-the-art methods. Multi-class classification accuracy remains suboptimal. Our uncertainty analysis unveils variability in prediction confidence influenced by protein class and sequence length. \textbf{Conclusions:} Our study surpasses frequency chaos game representation (FCGR) by introducing novel image encoding that mitigates spatial information loss limitations. Our classification technique yields accurate and robust PVP predictions while identifying low-confidence predictions.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2508.07326.pdf' target='_blank'>https://arxiv.org/pdf/2508.07326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Polina V. Banushkina, Sergei V. Krivov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07326">Nonparametric Reaction Coordinate Optimization with Histories: A Framework for Rare Event Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare but critical events in complex systems, such as protein folding, chemical reactions, disease progression, and extreme weather or climate phenomena, are governed by complex, high-dimensional, stochastic dynamics. Identifying an optimal reaction coordinate (RC) that accurately captures the progress of these dynamics is crucial for understanding and simulating such processes. This work introduces a nonparametric RC optimization framework that incorporates trajectory histories, enabling robust analysis even for irregular or incomplete data. The power of the method is demonstrated through increasingly challenging analyses of protein folding dynamics, where it provides accurate committor estimates that pass a stringent validation test and yield high-resolution free energy profiles. Its generality is further illustrated through applications to dynamics in phase space, a conceptual ocean circulation model, and a longitudinal clinical dataset. These results demonstrate that rare event dynamics can be accurately characterized without exhaustive sampling of the configuration space, establishing a general, flexible, and robust framework for analyzing complex dynamical systems and longitudinal datasets.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2508.04724.pdf' target='_blank'>https://arxiv.org/pdf/2508.04724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timothy Fei Truong, Tristan Bepler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04724">Understanding protein function with a multimodal retrieval-augmented foundation model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2508.03709.pdf' target='_blank'>https://arxiv.org/pdf/2508.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mhd Hussein Murtada, Z. Faidon Brotzakis, Michele Vendruscolo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03709">MD-LLM-1: A Large Language Model for Molecular Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular dynamics (MD) is a powerful approach for modelling molecular systems, but it remains computationally intensive on spatial and time scales of many macromolecular systems of biological interest. To explore the opportunities offered by deep learning to address this problem, we introduce a Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how LLMs can be leveraged to learn protein dynamics and discover states not seen in training. By applying MD-LLM-1, the first implementation of this approach, obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein systems, we show that training on one conformational state enables the prediction of other conformational states. These results indicate that MD-LLM-1 can learn the principles for the exploration of the conformational landscapes of proteins, although it is not yet modeling explicitly their thermodynamics and kinetics.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2508.03446.pdf' target='_blank'>https://arxiv.org/pdf/2508.03446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erico Souza Teixeira, Lucas Barros Fernandes, Yara Rodrigues InÃ¡cio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03446">Quantum Neural Network applications to Protein Binding Affinity Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Binding energy is a fundamental thermodynamic property that governs molecular interactions, playing a crucial role in fields such as healthcare and the natural sciences. It is particularly relevant in drug development, vaccine design, and other biomedical applications. Over the years, various methods have been developed to estimate protein binding energy, ranging from experimental techniques to computational approaches, with machine learning making significant contributions to this field. Although classical computing has demonstrated strong results in constructing predictive models, the variation of quantum computing for machine learning has emerged as a promising alternative. Quantum neural networks (QNNs) have gained traction as a research focus, raising the question of their potential advantages in predicting binding energies. To investigate this potential, this study explored the feasibility of QNNs for this task by proposing thirty variations of multilayer perceptron-based quantum neural networks. These variations span three distinct architectures, each incorporating ten different quantum circuits to configure their quantum layers. The performance of these quantum models was compared with that of a state-of-the-art classical multilayer perceptron-based artificial neural network, evaluating both accuracy and training time. A primary dataset was used for training, while two additional datasets containing entirely unseen samples were employed for testing. Results indicate that the quantum models achieved approximately 20% higher accuracy on one unseen dataset, although their accuracy was lower on the other datasets. Notably, quantum models exhibited training times several orders of magnitude shorter than their classical counterparts, highlighting their potential for efficient protein binding energy prediction.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2508.03444.pdf' target='_blank'>https://arxiv.org/pdf/2508.03444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atabey ÃnlÃ¼, Phil Rohr, Ahmet Celebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03444">An Auditable Agent Platform For Automated Molecular Optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug discovery frequently loses momentum when data, expertise, and tools are scattered, slowing design cycles. To shorten this loop we built a hierarchical, tool using agent framework that automates molecular optimisation. A Principal Researcher defines each objective, a Database agent retrieves target information, an AI Expert generates de novo scaffolds with a sequence to molecule deep learning model, a Medicinal Chemist edits them while invoking a docking tool, a Ranking agent scores the candidates, and a Scientific Critic polices the logic. Each tool call is summarised and stored causing the full reasoning path to remain inspectable. The agents communicate through concise provenance records that capture molecular lineage, to build auditable, molecule centered reasoning trajectories and reuse successful transformations via in context learning. Three cycle research loops were run against AKT1 protein using five large language models. After ranking the models by mean docking score, we ran 20 independent scale ups on the two top performers. We then compared the leading LLMs' binding affinity results across three configurations, LLM only, single agent, and multi agent. Our results reveal an architectural trade off, the multi agent setting excelled at focused binding optimization, improving average predicted binding affinity by 31%. In contrast, single agent runs generated molecules with superior drug like properties at the cost of less potent binding scores. Unguided LLM runs finished fastest, yet their lack of transparent tool signals left the validity of their reasoning paths unverified. These results show that test time scaling, focused feedback loops and provenance convert general purpose LLMs into auditable systems for molecular design, and suggest that extending the toolset to ADMET and selectivity predictors could push research workflows further along the discovery pipeline.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2507.22186.pdf' target='_blank'>https://arxiv.org/pdf/2507.22186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ambarish Singh, Romila Pradhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22186">SourceSplice: Source Selection for Machine Learning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data quality plays a pivotal role in the predictive performance of machine learning (ML) tasks - a challenge amplified by the deluge of data sources available in modern organizations. Prior work in data discovery largely focus on metadata matching, semantic similarity or identifying tables that should be joined to answer a particular query, but do not consider source quality for high performance of the downstream ML task. This paper addresses the problem of determining the best subset of data sources that must be combined to construct the underlying training dataset for a given ML task. We propose SourceGrasp and SourceSplice, frameworks designed to efficiently select a suitable subset of sources that maximizes the utility of the downstream ML model. Both the algorithms rely on the core idea that sources (or their combinations) contribute differently to the task utility, and must be judiciously chosen. While SourceGrasp utilizes a metaheuristic based on a greediness criterion and randomization, the SourceSplice framework presents a source selection mechanism inspired from gene splicing - a core concept used in protein synthesis. We empirically evaluate our algorithms on three real-world datasets and synthetic datasets and show that, with significantly fewer subset explorations, SourceSplice effectively identifies subsets of data sources leading to high task utility. We also conduct studies reporting the sensitivity of SourceSplice to the decision choices under several settings.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2507.20520.pdf' target='_blank'>https://arxiv.org/pdf/2507.20520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Praneeth Narisetty, Uday Kumar Reddy Kattamanchi, Lohit Akshant Nimma, Sri Ram Kaushik Karnati, Shiva Nagendra Babu Kore, Mounika Golamari, Tejashree Nageshreddy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20520">AQUA: A Large Language Model for Aquaculture & Fisheries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2507.16915.pdf' target='_blank'>https://arxiv.org/pdf/2507.16915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>April Herwig, Matthew J. Colbrook, Oliver Junge, PÃ©ter Koltai, Julia Slipantschuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16915">Avoiding spectral pollution for transfer operators using residuals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Koopman operator theory enables linear analysis of nonlinear dynamical systems by lifting their evolution to infinite-dimensional function spaces. However, finite-dimensional approximations of Koopman and transfer (Frobenius--Perron) operators are prone to spectral pollution, introducing spurious eigenvalues that can compromise spectral computations. While recent advances have yielded provably convergent methods for Koopman operators, analogous tools for general transfer operators remain limited. In this paper, we present algorithms for computing spectral properties of transfer operators without spectral pollution, including extensions to the Hardy-Hilbert space. Case studies--ranging from families of Blaschke maps with known spectrum to a molecular dynamics model of protein folding--demonstrate the accuracy and flexibility of our approach. Notably, we demonstrate that spectral features can arise even when the corresponding eigenfunctions lie outside the chosen space, highlighting the functional-analytic subtleties in defining the "true" Koopman spectrum. Our methods offer robust tools for spectral estimation across a broad range of applications.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2507.16801.pdf' target='_blank'>https://arxiv.org/pdf/2507.16801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Lin, Yaxue Fang, Zehong Zhang, Zhouwu Liu, Siyun Zhong, Fulong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16801">Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation is critical for controlling protein expression and designing effective therapeutic mRNAs. While recent deep learning models have shown promise in predicting translational efficiency from 5'UTR sequences, most are constrained by fixed input lengths and limited interpretability. We introduce UTR-STCNet, a Transformer-based architecture for flexible and biologically grounded modeling of variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token Clustering (SATC) module that iteratively aggregates nucleotide tokens into multi-scale, semantically meaningful units based on saliency scores. A Saliency-Guided Transformer (SGT) block then captures both local and distal regulatory dependencies using a lightweight attention mechanism. This combined architecture achieves efficient and interpretable modeling without input truncation or increased computational cost. Evaluated across three benchmark datasets, UTR-STCNet consistently outperforms state-of-the-art baselines in predicting mean ribosome load (MRL), a key proxy for translational efficiency. Moreover, the model recovers known functional elements such as upstream AUGs and Kozak motifs, highlighting its potential for mechanistic insight into translation regulation.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2507.14639.pdf' target='_blank'>https://arxiv.org/pdf/2507.14639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saleh Alwer, Ronan Fleming
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14639">KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis constant ($K_{\mathrm{M}}$) are essential for modelling enzymatic activity but experimental data remains limited in scale and diversity. Previous methods for predicting enzyme kinetics typically use mean-pooled residue embeddings from a single protein language model to represent the protein. We present KinForm, a machine learning framework designed to improve predictive accuracy and generalisation for kinetic parameters by optimising protein feature representations. KinForm combines several residue-level embeddings (Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and ProtT5-XL-UniRef50), taken from empirically selected intermediate transformer layers and applies weighted pooling based on per-residue binding-site probability. To counter the resulting high dimensionality, we apply dimensionality reduction using principal--component analysis (PCA) on concatenated protein features, and rebalance the training data via a similarity-based oversampling strategy. KinForm outperforms baseline methods on two benchmark datasets. Improvements are most pronounced in low sequence similarity bins. We observe improvements from binding-site probability pooling, intermediate-layer selection, PCA, and oversampling of low-identity proteins. We also find that removing sequence overlap between folds provides a more realistic evaluation of generalisation and should be the standard over random splitting when benchmarking kinetic prediction models.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2507.13950.pdf' target='_blank'>https://arxiv.org/pdf/2507.13950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbo Liang, Bruna Jacobson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13950">MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extensively exploring protein conformational landscapes remains a major challenge in computational biology due to the high computational cost involved in dynamic physics-based simulations. In this work, we propose a novel pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and generative adversarial networks (GANs) to explore protein conformational spaces. MoDyGAN contains a generator that maps Gaussian distributions into MD-derived protein trajectories, and a refinement module that combines ensemble learning with a dual-discriminator to further improve the plausibility of generated conformations. Central to our approach is an innovative representation technique that reversibly transforms 3D protein structures into 2D matrices, enabling the use of advanced image-based GAN architectures. We use three rigid proteins to demonstrate that MoDyGAN can generate plausible new conformations. We also use deca-alanine as a case study to show that interpolations within the latent space closely align with trajectories obtained from steered molecular dynamics (SMD) simulations. Our results suggest that representing proteins as image-like data unlocks new possibilities for applying advanced deep learning techniques to biomolecular simulation, leading to an efficient sampling of conformational states. Additionally, the proposed framework holds strong potential for extension to other complex 3D structures.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2507.11950.pdf' target='_blank'>https://arxiv.org/pdf/2507.11950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauren Lui, Torben Nielsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11950">RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Functional annotation of microbial genomes is often biased toward protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs (ncRNAs) that are critical for regulating bacterial and archaeal physiology, stress response and metabolism. Identifying ncRNAs directly from genomic sequence is a paramount challenge in bioinformatics and biology, essential for understanding the complete regulatory potential of an organism. This paper presents RNAMunin, a machine learning (ML) model that is capable of finding ncRNAs using genomic sequence alone. It is also computationally viable for large sequence datasets such as long read metagenomic assemblies with contigs totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary samples. We know of no other model that can detect ncRNAs based solely on genomic sequence at this scale. Since RNAMunin only requires genomic sequence as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do not need transcriptomics data. We wrote this manuscript in a narrative style in order to best convey how RNAMunin was developed and how it works in detail. Unlike almost all current ML models, at approximately 1M parameters, RNAMunin is very small and very fast.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2507.11176.pdf' target='_blank'>https://arxiv.org/pdf/2507.11176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Li, Xingye Cheng, Ziyang Huang, Jingyuan Luo, Qianqian Xu, Qiguang Zhao, Tianchen Guo, Yumeng Zhang, Linda Lidan Zhong, Zhaoxiang Bian, Leihan Tang, Aiping Lyu, Liang Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11176">An Interpretable AI framework Quantifying Traditional Chinese Medicine Principles Towards Enhancing and Integrating with Modern Biomedicine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Chinese Medicine diagnosis and treatment principles, established through centuries of trial-and-error clinical practice, directly maps patient-specific symptom patterns to personalised herbal therapies. These empirical holistic mapping principles offer valuable strategies to address remaining challenges of reductionism methodologies in modern biomedicine. However, the lack of a quantitative framework and molecular-level evidence has limited their interpretability and reliability. Here, we present an AI framework trained on ancient and classical TCM formula records to quantify the symptom pattern-herbal therapy mappings. Interestingly, we find that empirical TCM diagnosis and treatment are consistent with the encoding-decoding processes in the AI model. This enables us to construct an interpretable TCM embedding space (TCM-ES) using the model's quantitative representation of TCM principles. Validated through broad and extensive TCM patient data, the TCM-ES offers universal quantification of the TCM practice and therapeutic efficacy. We further map biomedical entities into the TCM-ES through correspondence alignment. We find that the principal directions of the TCM-ES are significantly associated with key biological functions (such as metabolism, immune, and homeostasis), and that the disease and herb embedding proximity aligns with their genetic relationships in the human protein interactome, which demonstrate the biological significance of TCM principles. Moreover, the TCM-ES uncovers latent disease relationships, and provides alternative metric to assess clinical efficacy for modern disease-drug pairs. Finally, we construct a comprehensive and integrative TCM knowledge graph, which predicts potential associations between diseases and targets, drugs, herbal compounds, and herbal therapies, providing TCM-informed opportunities for disease analysis and drug development.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2507.10273.pdf' target='_blank'>https://arxiv.org/pdf/2507.10273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Zhu, Emmanuel Noutahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10273">Conditional Chemical Language Models are Versatile Tools in Drug Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative chemical language models (CLMs) have demonstrated strong capabilities in molecular design, yet their impact in drug discovery remains limited by the absence of reliable reward signals and the lack of interpretability in their outputs. We present SAFE-T, a generalist chemical modeling framework that conditions on biological context -- such as protein targets or mechanisms of action -- to prioritize and design molecules without relying on structural information or engineered scoring functions. SAFE-T models the conditional likelihood of fragment-based molecular sequences given a biological prompt, enabling principled scoring of molecules across tasks such as virtual screening, drug-target interaction prediction, and activity cliff detection. Moreover, it supports goal-directed generation by sampling from this learned distribution, aligning molecular design with biological objectives. In comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA, ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves performance comparable to or better than existing approaches while being significantly faster. Fragment-level attribution further reveals that SAFE-T captures known structure-activity relationships, supporting interpretable and biologically grounded design. Together with its computational efficiency, these results demonstrate that conditional generative CLMs can unify scoring and generation to accelerate early-stage drug discovery.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2507.06458.pdf' target='_blank'>https://arxiv.org/pdf/2507.06458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Banerjee, David Martinez, Camille Dang, Ethan Tam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06458">Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) encode rich biological information, yet their internal neuron representations are poorly understood. We introduce the first automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation, our method scales to hundreds of thousands of neurons, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. We then develop a novel neuron activation-guided steering method to generate proteins with desired traits, enabling convergence to target biochemical properties like molecular weight and instability index as well as secondary and tertiary structural motifs, including alpha helices and canonical Zinc Fingers. We finally show that analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2507.05540.pdf' target='_blank'>https://arxiv.org/pdf/2507.05540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, Ehsan Irajizad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05540">Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) often struggle with noisy edges. We propose Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate external "clean" links and guide embeddings of a noisy target graph. We train two encoders--one on the full graph (target plus external edges) and another on a regularization graph excluding the target's potentially noisy links--then penalize discrepancies between their latent representations. This constraint steers the model away from overfitting spurious edges. Experiments on benchmark datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and validate it on a small protein-metabolite network, where metabolite-protein interactions reduce noise in protein co-occurrence data. Our results highlight LSC-GNN's potential to boost predictive performance and interpretability in settings with noisy relational structures.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2507.02624.pdf' target='_blank'>https://arxiv.org/pdf/2507.02624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine HonorÃ©, Borja RodrÃ­guez GÃ¡lvez, Yoomi Park, Yitian Zhou, Volker M. Lauschke, Ming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02624">A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variant effect predictors (VEPs) aim to assess the functional impact of protein variants, traditionally relying on multiple sequence alignments (MSAs). This approach assumes that naturally occurring variants are fit, an assumption challenged by pharmacogenomics, where some pharmacogenes experience low evolutionary pressure. Deep mutational scanning (DMS) datasets provide an alternative by offering quantitative fitness scores for variants. In this work, we propose a transformer-based matrix variational auto-encoder (matVAE) with a structured prior and evaluate its performance on 33 DMS datasets corresponding to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence model in zero-shot prediction on DMS datasets, despite using an order of magnitude fewer parameters and requiring less computation at inference time. We also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on DMS data, and find that the latter performs better on supervised prediction tasks. Additionally, incorporating AlphaFold-generated structures into our transformer model further improves performance, achieving results comparable to DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the potential of DMS datasets to replace MSAs without significant loss in predictive performance, motivating further development of DMS datasets and exploration of their relationships to enhance variant effect prediction.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2506.13006.pdf' target='_blank'>https://arxiv.org/pdf/2506.13006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunna Huh, Hyeonsu Lee, Hyunjin Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13006">Antibody Foundational Model : Ab-RoBERTa</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing prominence of antibody-based therapeutics, antibody engineering has gained increasing attention as a critical area of research and development. Recent progress in transformer-based protein large language models (LLMs) has demonstrated promising applications in protein sequence design and structural prediction. Moreover, the availability of large-scale antibody datasets such as the Observed Antibody Space (OAS) database has opened new avenues for the development of LLMs specialized for processing antibody sequences. Among these, RoBERTa has demonstrated improved performance relative to BERT, while maintaining a smaller parameter count (125M) compared to the BERT-based protein model, ProtBERT (420M). This reduced model size enables more efficient deployment in antibody-related applications. However, despite the numerous advantages of the RoBERTa architecture, antibody-specific foundational models built upon it have remained inaccessible to the research community. In this study, we introduce Ab-RoBERTa, a RoBERTa-based antibody-specific LLM, which is publicly available at https://huggingface.co/mogam-ai/Ab-RoBERTa. This resource is intended to support a wide range of antibody-related research applications including paratope prediction or humanness assessment.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2506.12455.pdf' target='_blank'>https://arxiv.org/pdf/2506.12455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqin Qiu, Xinyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12455">A Transfer Learning Framework for Multilayer Networks via Model Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Link prediction in multilayer networks is a key challenge in applications such as recommendation systems and protein-protein interaction prediction. While many techniques have been developed, most rely on assumptions about shared structures and require access to raw auxiliary data, limiting their practicality. To address these issues, we propose a novel transfer learning framework for multilayer networks using a bi-level model averaging method. A $K$-fold cross-validation criterion based on edges is used to automatically weight inter-layer and intra-layer candidate models. This enables the transfer of information from auxiliary layers while mitigating model uncertainty, even without prior knowledge of shared structures. Theoretically, we prove the optimality and weight convergence of our method under mild conditions. Computationally, our framework is efficient and privacy-preserving, as it avoids raw data sharing and supports parallel processing across multiple servers. Simulations show our method outperforms others in predictive accuracy and robustness. We further demonstrate its practical value through two real-world recommendation system applications.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2506.02212.pdf' target='_blank'>https://arxiv.org/pdf/2506.02212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ella Rannon, David Burstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02212">Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2505.23879.pdf' target='_blank'>https://arxiv.org/pdf/2505.23879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caio Cheohen, VinnÃ­cius M. S. Gomes, Manuela L. da Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23879">CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2505.22926.pdf' target='_blank'>https://arxiv.org/pdf/2505.22926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sylvey Lin, Zhi-Yi Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22926">Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate whether synthetic images generated by diffusion models can enhance multi-label classification of protein subcellular localization. Specifically, we implement a simplified class-conditional denoising diffusion probabilistic model (DDPM) to produce label-consistent samples and explore their integration with real data via two hybrid training strategies: Mix Loss and Mix Representation. While these approaches yield promising validation performance, our proposed MixModel exhibits poor generalization to unseen test data, underscoring the challenges of leveraging synthetic data effectively. In contrast, baseline classifiers built on ResNet backbones with conventional loss functions demonstrate greater stability and test-time performance. Our findings highlight the importance of realistic data generation and robust supervision when incorporating generative augmentation into biomedical image classification.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2505.19763.pdf' target='_blank'>https://arxiv.org/pdf/2505.19763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Hamelryck, Kanti V. Mardia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19763">Unfolding AlphaFold's Bayesian Roots in Probability Kinematics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel theoretical interpretation of AlphaFold1 that reveals the potential of generalized Bayesian updating for probabilistic deep learning. The seminal breakthrough of AlphaFold1 in protein structure prediction by deep learning relied on a learned potential energy function, in contrast to the later end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was originally justified by referring to physical potentials of mean force (PMFs), we reinterpret AlphaFold1's potential as an instance of {\em probability kinematics} -- also known as {\em Jeffrey conditioning} -- a principled but under-recognised generalization of conventional Bayesian updating. Probability kinematics accommodates uncertain or {\em soft} evidence in the form of updated probabilities over a partition. This perspective reveals AlphaFold1's potential as a form of generalized Bayesian updating, rather than a thermodynamic potential. To confirm our probabilistic framework's scope and precision, we analyze a synthetic 2D model in which an angular random walk prior is updated with evidence on distances via probability kinematics, mirroring AlphaFold1's approach. This theoretical contribution connects AlphaFold1 to a broader class of well-justified Bayesian methods, allowing precise quantification, surpassing merely qualitative heuristics based on PMFs. Our contribution is theoretical: we replace AlphaFold1's heuristic analogy with a principled probabilistic framework, tested in a controlled synthetic setting where correctness can be assessed. More broadly, our results point to the considerable promise of probability kinematics for probabilistic deep learning, by allowing the formulation of complex models from a few simpler components.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2505.15747.pdf' target='_blank'>https://arxiv.org/pdf/2505.15747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15747">Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2505.11610.pdf' target='_blank'>https://arxiv.org/pdf/2505.11610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asher Moldwin, Amarda Shehu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11610">Foundation Models for AI-Enabled Biological Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2505.11529.pdf' target='_blank'>https://arxiv.org/pdf/2505.11529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Luo, Jinyu Zhou, Le Xu, Sisi Yuan, Xuan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11529">DynamicDTA: Drug-Target Binding Affinity Prediction Using Dynamic Descriptors and Graph Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting drug-target binding affinity (DTA) is essential for identifying potential therapeutic candidates in drug discovery. However, most existing models rely heavily on static protein structures, often overlooking the dynamic nature of proteins, which is crucial for capturing conformational flexibility that will be beneficial for protein binding interactions. We introduce DynamicDTA, an innovative deep learning framework that incorporates static and dynamic protein features to enhance DTA prediction. The proposed DynamicDTA takes three types of inputs, including drug sequence, protein sequence, and dynamic descriptors. A molecular graph representation of the drug sequence is generated and subsequently processed through graph convolutional network, while the protein sequence is encoded using dilated convolutions. Dynamic descriptors, such as root mean square fluctuation, are processed through a multi-layer perceptron. These embedding features are fused with static protein features using cross-attention, and a tensor fusion network integrates all three modalities for DTA prediction. Extensive experiments on three datasets demonstrate that DynamicDTA achieves by at least 3.4% improvement in RMSE score with comparison to seven state-of-the-art baseline methods. Additionally, predicting novel drugs for Human Immunodeficiency Virus Type 1 and visualizing the docking complexes further demonstrates the reliability and biological relevance of DynamicDTA.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2505.10848.pdf' target='_blank'>https://arxiv.org/pdf/2505.10848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10848">Foundation model for mass spectrometry proteomics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mass spectrometry is the dominant technology in the field of proteomics, enabling high-throughput analysis of the protein content of complex biological samples. Due to the complexity of the instrumentation and resulting data, sophisticated computational methods are required for the processing and interpretation of acquired mass spectra. Machine learning has shown great promise to improve the analysis of mass spectrometry data, with numerous purpose-built methods for improving specific steps in the data acquisition and analysis pipeline reaching widespread adoption. Here, we propose unifying various spectrum prediction tasks under a single foundation model for mass spectra. To this end, we pre-train a spectrum encoder using de novo sequencing as a pre-training task. We then show that using these pre-trained spectrum representations improves our performance on the four downstream tasks of spectrum quality prediction, chimericity prediction, phosphorylation prediction, and glycosylation status prediction. Finally, we perform multi-task fine-tuning and find that this approach improves the performance on each task individually. Overall, our work demonstrates that a foundation model for tandem mass spectrometry proteomics trained on de novo sequencing learns generalizable representations of spectra, improves performance on downstream tasks where training data is limited, and can ultimately enhance data acquisition and analysis in proteomics experiments.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2505.10711.pdf' target='_blank'>https://arxiv.org/pdf/2505.10711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SebestyÃ©n Kamp, Giovanni Stracquadanio, T. Ian Simpson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10711">GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GNN-Suite, a robust modular framework for constructing and benchmarking Graph Neural Network (GNN) architectures in computational biology. GNN-Suite standardises experimentation and reproducibility using the Nextflow workflow to evaluate GNN performance. We demonstrate its utility in identifying cancer-driver genes by constructing molecular networks from protein-protein interaction (PPI) data from STRING and BioGRID and annotating nodes with features from the PCAWG, PID, and COSMIC-CGC repositories.
  Our design enables fair comparisons among diverse GNN architectures including GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline Logistic Regression (LR) model. All GNNs were configured as standardised two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss to address class imbalance) over an 80/20 train-test split for 300 epochs. Each model was evaluated over 10 independent runs with different random seeds to yield statistically robust performance metrics, with balanced accuracy (BACC) as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/- 0.035) on a STRING-based network, although all GNN types outperformed the LR baseline, highlighting the advantage of network-based learning over feature-only approaches.
  Our results show that a common framework for implementing and evaluating GNN architectures aids in identifying not only the best model but also the most effective means of incorporating complementary data. By making GNN-Suite publicly available, we aim to foster reproducible research and promote improved benchmarking standards in computational biology. Future work will explore additional omics datasets and further refine network architectures to enhance predictive accuracy and interpretability in biomedical applications.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2505.09087.pdf' target='_blank'>https://arxiv.org/pdf/2505.09087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>He Wang, Yikun Zhang, Jie Chen, Jian Zhan, Yaoqi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09087">A Comparative Review of RNA Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given usefulness of protein language models (LMs) in structure and functional inference, RNA LMs have received increased attentions in the last few years. However, these RNA models are often not compared against the same standard. Here, we divided RNA LMs into three classes (pretrained on multiple RNA types (especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein LMs as controls in zero-shot prediction of RNA secondary structure and functional classification. Results shows that the models doing well on secondary structure prediction often perform worse in function classification or vice versa, suggesting that more balanced unsupervised training is needed.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2505.06753.pdf' target='_blank'>https://arxiv.org/pdf/2505.06753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhamed Amin, Bernard R. Brooks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06753">Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Boltzmann classifier, a novel distance based probabilistic classification algorithm inspired by the Boltzmann distribution. Unlike traditional classifiers that produce hard decisions or uncalibrated probabilities, the Boltzmann classifier assigns class probabilities based on the average distance to the nearest neighbors within each class, providing interpretable, physically meaningful outputs. We evaluate the performance of the method across three application domains: molecular activity prediction, oxidation state classification of transition metal complexes, and breast cancer diagnosis. In the molecular activity task, the classifier achieved the highest accuracy in predicting active compounds against two protein targets, with strong correlations observed between the predicted probabilities and experimental pIC50 values. For metal complexes, the classifier accurately distinguished between oxidation states II and III for Fe, Mn, and Co, using only metal-ligand bond lengths extracted from crystallographic data, and demonstrated high consistency with known chemical trends. In the breast cancer dataset, the classifier achieved 97% accuracy, with low confidence predictions concentrated in inherently ambiguous cases. Across all tasks, the Boltzmann classifier performed competitively or better than standard models such as logistic regression, support vector machines, random forests, and k-nearest neighbors. Its probabilistic outputs were found to correlate with continuous physical or biological properties, highlighting its potential utility in both classification and regression contexts. The results suggest that the Boltzmann classifier is a robust and interpretable alternative to conventional machine learning approaches, particularly in scientific domains where underlying structure property relationships are important.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2505.02022.pdf' target='_blank'>https://arxiv.org/pdf/2505.02022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhang, Koji Tsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02022">NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nanobodies -- single-domain antibody fragments derived from camelid heavy-chain-only antibodies -- exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models -- including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs -- in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2504.19034.pdf' target='_blank'>https://arxiv.org/pdf/2504.19034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samantha Petti, Carlos MartÃ­-GÃ³mez, Justin B. Kinney, Juannan Zhou, David M. McCandlish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19034">On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mappings from biological sequences (DNA, RNA, protein) to quantitative measures of sequence functionality play an important role in contemporary biology. We are interested in the related tasks of (i) inferring predictive sequence-to-function maps and (ii) decomposing sequence-function maps to elucidate the contributions of individual subsequences. Because each sequence-function map can be written as a weighted sum over subsequences in multiple ways, meaningfully interpreting these weights requires ``gauge-fixing,'' i.e., defining a unique representation for each map. Recent work has established that most existing gauge-fixed representations arise as the unique solutions to $L_2$-regularized regression in an overparameterized ``weight space'' where the choice of regularizer defines the gauge. Here, we establish the relationship between regularized regression in overparameterized weight space and Gaussian process approaches that operate in ``function space,'' i.e.~the space of all real-valued functions on a finite set of sequences. We disentangle how weight space regularizers both impose an implicit prior on the learned function and restrict the optimal weights to a particular gauge. We show how to construct regularizers that correspond to arbitrary explicit Gaussian process priors combined with a wide variety of gauges and characterize the implicit function space priors associated with the most common weight space regularizers. Finally, we derive the posterior distribution of a broad class of sequence-to-function statistics, including gauge-fixed weights and multiple systems for expressing higher-order epistatic coefficients. We show that such distributions can be efficiently computed for product-kernel priors using a kernel trick.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2504.17162.pdf' target='_blank'>https://arxiv.org/pdf/2504.17162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cece Zhang, Xuehuan Zhu, Nick Peterson, Jieqiong Wang, Shibiao Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17162">A Comprehensive Review on RNA Subcellular Localization Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The subcellular localization of RNAs, including long non-coding RNAs (lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs, plays a critical role in determining their biological functions. For instance, lncRNAs are predominantly associated with chromatin and act as regulators of gene transcription and chromatin structure, while mRNAs are distributed across the nucleus and cytoplasm, facilitating the transport of genetic information for protein synthesis. Understanding RNA localization sheds light on processes like gene expression regulation with spatial and temporal precision. However, traditional wet lab methods for determining RNA localization, such as in situ hybridization, are often time-consuming, resource-demanding, and costly. To overcome these challenges, computational methods leveraging artificial intelligence (AI) and machine learning (ML) have emerged as powerful alternatives, enabling large-scale prediction of RNA subcellular localization. This paper provides a comprehensive review of the latest advancements in AI-based approaches for RNA subcellular localization prediction, covering various RNA types and focusing on sequence-based, image-based, and hybrid methodologies that combine both data types. We highlight the potential of these methods to accelerate RNA research, uncover molecular pathways, and guide targeted disease treatments. Furthermore, we critically discuss the challenges in AI/ML approaches for RNA subcellular localization, such as data scarcity and lack of benchmarks, and opportunities to address them. This review aims to serve as a valuable resource for researchers seeking to develop innovative solutions in the field of RNA subcellular localization and beyond.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2504.17068.pdf' target='_blank'>https://arxiv.org/pdf/2504.17068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Kantroo, GÃ¼nter P. Wagner, Benjamin B. Machta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17068">In-Context Learning can distort the relationship between sequence likelihoods and biological fitness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models have emerged as powerful predictors of the viability of biological sequences. During training these models learn the rules of the grammar obeyed by sequences of amino acids or nucleotides. Once trained, these models can take a sequence as input and produce a likelihood score as an output; a higher likelihood implies adherence to the learned grammar and correlates with experimental fitness measurements. Here we show that in-context learning can distort the relationship between fitness and likelihood scores of sequences. This phenomenon most prominently manifests as anomalously high likelihood scores for sequences that contain repeated motifs. We use protein language models with different architectures trained on the masked language modeling objective for our experiments, and find transformer-based models to be particularly vulnerable to this effect. This behavior is mediated by a look-up operation where the model seeks the identity of the masked position by using the other copy of the repeated motif as a reference. This retrieval behavior can override the model's learned priors. This phenomenon persists for imperfectly repeated sequences, and extends to other kinds of biologically relevant features such as reversed complement motifs in RNA sequences that fold into hairpin structures.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2504.16886.pdf' target='_blank'>https://arxiv.org/pdf/2504.16886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Sharma, Anthony Gitter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16886">Exploring zero-shot structure-based protein fitness prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to make zero-shot predictions about the fitness consequences of protein sequence changes with pre-trained machine learning models enables many practical applications. Such models can be applied for downstream tasks like genetic variant interpretation and protein engineering without additional labeled data. The advent of capable protein structure prediction tools has led to the availability of orders of magnitude more precomputed predicted structures, giving rise to powerful structure-based fitness prediction models. Through our experiments, we assess several modeling choices for structure-based models and their effects on downstream fitness prediction. Zero-shot fitness prediction models can struggle to assess the fitness landscape within disordered regions of proteins, those that lack a fixed 3D structure. We confirm the importance of matching protein structures to fitness assays and find that predicted structures for disordered regions can be misleading and affect predictive performance. Lastly, we evaluate an additional structure-based model on the ProteinGym substitution benchmark and show that simple multi-modal ensembles are strong baselines.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2504.16479.pdf' target='_blank'>https://arxiv.org/pdf/2504.16479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Qin, Ming He, Changyong Yu, Ming Ni, Xian Liu, Xiaochen Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16479">The Dance of Atoms-De Novo Protein Design with Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The de novo design of proteins refers to creating proteins with specific structures and functions that do not naturally exist. In recent years, the accumulation of high-quality protein structure and sequence data and technological advancements have paved the way for the successful application of generative artificial intelligence (AI) models in protein design. These models have surpassed traditional approaches that rely on fragments and bioinformatics. They have significantly enhanced the success rate of de novo protein design, and reduced experimental costs, leading to breakthroughs in the field. Among various generative AI models, diffusion models have yielded the most promising results in protein design. In the past two to three years, more than ten protein design models based on diffusion models have emerged. Among them, the representative model, RFDiffusion, has demonstrated success rates in 25 protein design tasks that far exceed those of traditional methods, and other AI-based approaches like RFjoint and hallucination. This review will systematically examine the application of diffusion models in generating protein backbones and sequences. We will explore the strengths and limitations of different models, summarize successful cases of protein design using diffusion models, and discuss future development directions.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2504.15634.pdf' target='_blank'>https://arxiv.org/pdf/2504.15634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizheng Liu, Hitoshi Iba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15634">Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based architectures have recently propelled advances in sequence modeling across domains, but their application to the hydrophobic-hydrophilic (H-P) model for protein folding remains relatively unexplored. In this work, we adapt a Deep Q-Network (DQN) integrated with attention mechanisms (Transformers) to address the 3D H-P protein folding problem. Our system formulates folding decisions as a self-avoiding walk in a reinforced environment, and employs a specialized reward function based on favorable hydrophobic interactions. To improve performance, the method incorporates validity check including symmetry-breaking constraints, dueling and double Q-learning, and prioritized replay to focus learning on critical transitions. Experimental evaluations on standard benchmark sequences demonstrate that our approach achieves several known best solutions for shorter sequences, and obtains near-optimal results for longer chains. This study underscores the promise of attention-based reinforcement learning for protein folding, and created a prototype of Transformer-based Q-network structure for 3-dimensional lattice models.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2504.13978.pdf' target='_blank'>https://arxiv.org/pdf/2504.13978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Liu, Meng Zhao, Guanlan Hu, Yuchen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13978">Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background. Diet and inflammation are critical factors influencing cancer risk. However, the combined impact of nutritional status and inflammatory biomarkers on cancer status and type, using machine learning (ML), remains underexplored.
  Objectives. This study investigates the association between nutritional factors, inflammatory biomarkers, and cancer status, and whether these relationships differ across cancer types using National Health and Nutrition Examination Survey (NHANES) data.
  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP), and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES participants (2,120 with cancer). Multivariable logistic regression assessed associations with cancer prevalence. We also examined whether these features differed across the five most common cancer types. To evaluate predictive value, we applied three ML models - Logistic Regression, Random Forest, and XGBoost - on the full feature set.
  Results. The cohort's mean age was 49.1 years; 34.7% were obese. Comorbidities such as anemia and liver conditions, along with nutritional factors like protein and several vitamins, were key predictors of cancer status. Among the models, Random Forest performed best, achieving an accuracy of 0.72.
  Conclusions. Higher-quality nutritional intake and lower levels of inflammation may offer protective effects against cancer. These findings highlight the potential of combining nutritional and inflammatory markers with ML to inform cancer prevention strategies.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2504.13863.pdf' target='_blank'>https://arxiv.org/pdf/2504.13863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snigdha Tiwari, Sahil Sharma, Arvind Bagga, Aditi Sinha, Deepak Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13863">Utsarjan: A smartphone App for providing kidney care and real-time assistance to children with nephrotic syndrome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background Telemedicine has the potential to provide secure and cost-effective healthcare at the touch of a button. Nephrotic syndrome is a chronic childhood illness involving frequent relapses and demands long/complex treatment. Hence, developing a remote means of doctor-patient interface will ensure the provision of quality healthcare to patients. Methods The Utsarjan mobile App framework was built with Flutter that enables cross-platform development (Android, iOS, Windows) with speed, smoothness, and open-source benefits. The frontend uses Dart for user interaction, while the backend employs Node.js, Express, and NGINX for APIs, load balancing and high performance. MongoDB ensures a flexible database, Bcrypt secures passwords, PM2 handles deployment, uptime and logs, while Firebase Cloud Messaging powers free push notifications. Results Utsarjan (means excretion) is a multi-functional smartphone application for giving nephrotic care and real-time assistance to all patients (especially those in rural regions and/or who do not have access to specialists). It helps patients and doctors by ensuring opportune visits, recording each clinical test/parameter and improving medication adherence. It gives a graphical visualization of relapses, medicine dosage as well as different anthropometric parameters (urine protein, BP, height and weight). This is the first nephrotic care App that enables prompt access to doctor's advice. Conclusions Utsarjan is a mobile App to provide kidney care and real-time assistance to children with nephrotic syndrome. It gives a graphical overview of changes in a patient's health over the long course of treatment. This will assist doctors in appropriately modifying the treatment regimen. Consequently, it will (hopefully) lead to the prevention of relapses and/or complications.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2504.10388.pdf' target='_blank'>https://arxiv.org/pdf/2504.10388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Rijal, Caroline M. Holmes, Samantha Petti, Gautam Reddy, Michael M. Desai, Pankaj Mehta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10388">Inferring genotype-phenotype maps using attention models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for "transfer learning" - predicting phenotypes in novel environments with limited training data.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2504.06282.pdf' target='_blank'>https://arxiv.org/pdf/2504.06282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub VaÅ¡Ã­Äek, Dafni Skiadopoulou, Ksenia G. Kuznetsova, Lukas KÃ¤ll, Marc Vaudel, Stefan Bruckner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06282">ProHap Explorer: Visualizing Haplotypes in Proteogenomic Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mass spectrometry-based proteomics, experts usually project data onto a single set of reference sequences, overlooking the influence of common haplotypes (combinations of genetic variants inherited together from a parent). We recently introduced ProHap, a tool for generating customized protein haplotype databases. Here, we present ProHap Explorer, a visualization interface designed to investigate the influence of common haplotypes on the human proteome. It enables users to explore haplotypes, their effects on protein sequences, and the identification of non-canonical peptides in public mass spectrometry datasets. The design builds on well-established representations in biological sequence analysis, ensuring familiarity for domain experts while integrating novel interactive elements tailored to proteogenomic data exploration. User interviews with proteomics experts confirmed the tool's utility, highlighting its ability to reveal whether haplotypes affect proteins of interest. By facilitating the intuitive exploration of proteogenomic variation, ProHap Explorer supports research in personalized medicine and the development of targeted therapies.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2504.02839.pdf' target='_blank'>https://arxiv.org/pdf/2504.02839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Lombard, Sergei Grudinin, Elodie Laine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02839">PETIMOT: A Novel Framework for Inferring Protein Motions from Sparse Data Using SE(3)-Equivariant Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins move and deform to ensure their biological functions. Despite significant progress in protein structure prediction, approximating conformational ensembles at physiological conditions remains a fundamental open problem. This paper presents a novel perspective on the problem by directly targeting continuous compact representations of protein motions inferred from sparse experimental observations. We develop a task-specific loss function enforcing data symmetries, including scaling and permutation operations. Our method PETIMOT (Protein sEquence and sTructure-based Inference of MOTions) leverages transfer learning from pre-trained protein language models through an SE(3)-equivariant graph neural network. When trained and evaluated on the Protein Data Bank, PETIMOT shows superior performance in time and accuracy, capturing protein dynamics, particularly large/slow conformational changes, compared to state-of-the-art flow-matching approaches and traditional physics-based models.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2504.02014.pdf' target='_blank'>https://arxiv.org/pdf/2504.02014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiannuo Li, Lan Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02014">HCAF-DTA: drug-target binding affinity prediction with cross-attention fused hypergraph neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of the binding affinity between drugs and target proteins is a core task in computer-aided drug design. Existing deep learning methods tend to ignore the information of internal sub-structural features of drug molecules and drug-target interactions, resulting in limited prediction performance. In this paper, we propose a drug-target association prediction model HCAF-DTA based on cross-attention fusion hypergraph neural network. The model innovatively introduces hypergraph representation in the feature extraction stage: drug molecule hypergraphs are constructed based on the tree decomposition algorithm, and the sub-structural and global features extracted by fusing the hypergraph neural network with the graphical neural network through hopping connections, in which the hyper edges can efficiently characterise the functional functional groups and other key chemical features; for the protein feature extraction, a weighted graph is constructed based on the residues predicted by the ESM model contact maps to construct weighted graphs, and multilayer graph neural networks were used to capture spatial dependencies. In the prediction stage, a bidirectional multi-head cross-attention mechanism is designed to model intermolecular interactions from the dual viewpoints of atoms and amino acids, and cross-modal features with correlated information are fused by attention. Experiments on benchmark datasets such as Davis and KIBA show that HCAF-DTA outperforms state of the arts in all three performance evaluation metrics, with the MSE metrics reaching 0.198 and 0.122, respectively, with an improvement of up to 4% from the optimal baseline.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2504.00146.pdf' target='_blank'>https://arxiv.org/pdf/2504.00146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tudor-Stefan Cotet, Igor Krawczuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00146">Why risk matters for protein binder design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian optimization (BO) has recently become more prevalent in protein engineering applications and hence has become a fruitful target of benchmarks. However, current BO comparisons often overlook real-world considerations like risk and cost constraints. In this work, we compare 72 model combinations of encodings, surrogate models, and acquisition functions on 11 protein binder fitness landscapes, specifically from this perspective. Drawing from the portfolio optimization literature, we adopt metrics to quantify the cold-start performance relative to a random baseline, to assess the risk of an optimization campaign, and to calculate the overall budget required to reach a fitness threshold. Our results suggest the existence of Pareto-optimal models on the risk-performance axis, the shift of this preference depending on the landscape explored, and the robust correlation between landscape properties such as epistasis with the average and worst-case model performance. They also highlight that rigorous model selection requires substantial computational and statistical efforts.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2503.20767.pdf' target='_blank'>https://arxiv.org/pdf/2503.20767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clara Fannjiang, Ji Won Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20767">Reliable algorithm selection for machine learning-guided design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Algorithms for machine learning-guided design, or design algorithms, use machine learning-based predictions to propose novel objects with desired property values. Given a new design task -- for example, to design novel proteins with high binding affinity to a therapeutic target -- one must choose a design algorithm and specify any hyperparameters and predictive and/or generative models involved. How can these decisions be made such that the resulting designs are successful? This paper proposes a method for design algorithm selection, which aims to select design algorithms that will produce a distribution of design labels satisfying a user-specified success criterion -- for example, that at least ten percent of designs' labels exceed a threshold. It does so by combining designs' predicted property values with held-out labeled data to reliably forecast characteristics of the label distributions produced by different design algorithms, building upon techniques from prediction-powered inference. The method is guaranteed with high probability to return design algorithms that yield successful label distributions (or the null set if none exist), if the density ratios between the design and labeled data distributions are known. We demonstrate the method's effectiveness in simulated protein and RNA design tasks, in settings with either known or estimated density ratios.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2503.18213.pdf' target='_blank'>https://arxiv.org/pdf/2503.18213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delower Hossain, Jake Y Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18213">A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few decades, Artificial Intelligence (AI) scientists have been conducting investigations to attain human-level performance by a machine in accomplishing a cognitive task. Within machine learning, the ultimate aspiration is to attain Artificial General Intelligence (AGI) through a machine. This pursuit has led to the exploration of two distinct AI paradigms. Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability, and knowledge representation but faces challenges in processing complex real-world data with noise. Conversely, deep learning (Black-Box systems) research breakthroughs in neural networks are notable, yet they lack reasoning and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI research, attempts to bridge this gap by integrating logical reasoning into neural networks, enabling them to learn and reason with symbolic representations. While a long path, this strategy has made significant progress towards achieving common sense reasoning by systems. This article conducts an extensive review of over 977 studies from prominent scientific databases (DBLP, ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its healthcare applications, particularly in drug discovery, and Protein engineering research. The survey addresses vital themes, including reasoning, explainability, integration strategies, 41 healthcare-related use cases, benchmarking, datasets, current approach limitations from both healthcare and broader perspectives, and proposed novel approaches for future experiments.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2503.16351.pdf' target='_blank'>https://arxiv.org/pdf/2503.16351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krithik Ramesh, Sameed M. Siddiqui, Albert Gu, Michael D. Mitzenmacher, Pardis C. Sabeti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16351">Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2503.13352.pdf' target='_blank'>https://arxiv.org/pdf/2503.13352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ewan R. S. Wallace, Nathan C. Frey, Joshua A. Rackers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13352">Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate Tool for Ligand Strain Calculations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ligand strain energy, the energy difference between the bound and unbound conformations of a ligand, is an important component of structure-based small molecule drug design. A large majority of observed ligands in protein-small molecule co-crystal structures bind in low-strain conformations, making strain energy a useful filter for structure-based drug design. In this work we present a tool for calculating ligand strain with a high accuracy. StrainRelief uses a MACE Neural Network Potential (NNP), trained on a large database of Density Functional Theory (DFT) calculations to estimate ligand strain of neutral molecules with quantum accuracy. We show that this tool estimates strain energy differences relative to DFT to within 1.4 kcal/mol, more accurately than alternative NNPs. These results highlight the utility of NNPs in drug discovery, and provide a useful tool for drug discovery teams.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2503.09251.pdf' target='_blank'>https://arxiv.org/pdf/2503.09251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yigang Chen, Xiang Ji, Ziyue Zhang, Yuming Zhou, Yang-Chi-Dung Lin, Hsi-Yuan Huang, Tao Zhang, Yi Lai, Ke Chen, Chang Su, Xingqiao Lin, Zihao Zhu, Yanggyi Zhang, Kangping Wei, Jiehui Fu, Yixian Huang, Shidong Cui, Shih-Chung Yen, Ariel Warshel, Hsien-Da Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09251">SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based drug-target interaction (DTI) prediction methods have demonstrated strong performance; however, real-world applicability remains constrained by limited data diversity and modeling complexity. To address these challenges, we propose SCOPE-DTI, a unified framework combining a large-scale, balanced semi-inductive human DTI dataset with advanced deep learning modeling. Constructed from 13 public repositories, the SCOPE dataset expands data volume by up to 100-fold compared to common benchmarks such as the Human dataset. The SCOPE model integrates three-dimensional protein and compound representations, graph neural networks, and bilinear attention mechanisms to effectively capture cross domain interaction patterns, significantly outperforming state-of-the-art methods across various DTI prediction tasks. Additionally, SCOPE-DTI provides a user-friendly interface and database. We further validate its effectiveness by experimentally identifying anticancer targets of Ginsenoside Rh1. By offering comprehensive data, advanced modeling, and accessible tools, SCOPE-DTI accelerates drug discovery research.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2503.04239.pdf' target='_blank'>https://arxiv.org/pdf/2503.04239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christos Papalitsas, Yanfei Guan, Shreyas Waghe, Athanasios Liakos, Ioannis Balatsos, Vassilios Pantazopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04239">Quantum Approximate Optimization Algorithms for Molecular Docking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular docking is a critical process for drug discovery and challenging due to the complexity and size of biomolecular systems, where the optimal binding configuration of a drug to a target protein is determined. Hybrid classical-quantum computing techniques offer a novel approach to address these challenges. The Quantum Approximate Optimization Algorithm (QAOA) and its variations are hybrid classical-quantum techniques, and a promising tool for combinatorial optimization challenges. This paper presents a Digitized Counterdiabatic QAOA (DC-QAOA) approach to molecular docking. Simulated quantum runs were conducted on a GPU cluster. We examined 14 and 17 nodes instances - to the best of our knowledge the biggest published instance is 12-node at Ding et al. and we present the results. Based on computational results, we conclude that binding interactions represent the anticipated exact solution. Additionally, as the size of the examined instance increases, the computational times exhibit a significant escalation.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2502.21274.pdf' target='_blank'>https://arxiv.org/pdf/2502.21274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roman Klypa, Alberto Bietti, Sergei Grudinin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21274">BAnG: Bidirectional Anchored Generation for Conditional RNA Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing RNA molecules that interact with specific proteins is a critical challenge in experimental and computational biology. Existing computational approaches require a substantial amount of previously known interacting RNA sequences for each specific protein or a detailed knowledge of RNA structure, restricting their utility in practice. To address this limitation, we develop RNA-BAnG, a deep learning-based model designed to generate RNA sequences for protein interactions without these requirements. Central to our approach is a novel generative method, Bidirectional Anchored Generation (BAnG), which leverages the observation that protein-binding RNA sequences often contain functional binding motifs embedded within broader sequence contexts. We first validate our method on generic synthetic tasks involving similar localized motifs to those appearing in RNAs, demonstrating its benefits over existing generative approaches. We then evaluate our model on biological sequences, showing its effectiveness for conditional RNA sequence design given a binding protein.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2502.18305.pdf' target='_blank'>https://arxiv.org/pdf/2502.18305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adolfo Ruiz-SanmartÃ­n, Vicent Ribas, David SuÃ±ol, Luis Chiscano-CamÃ³n, Laura MartÃ­n, IvÃ¡n BajaÃ±a, Juliana Bastida, Nieves Larrosa, Juan JosÃ© GonzÃ¡lez, M Dolores Carrasco, NÃºria Canela, Ricard Ferrer, Juan Carlos Ruiz-RodrÃ­gue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18305">Exploring proteomic signatures in sepsis and non-infectious systemic inflammatory response syndrome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background: The search for new biomarkers that allow an early diagnosis in sepsis has become a necessity in medicine. The objective of this study is to identify potential protein biomarkers of differential expression between sepsis and non-infectious systemic inflammatory response syndrome (NISIRS).
  Methods: Prospective observational study of a cohort of septic patients activated by the Sepsis Code and patients admitted with NISIRS, during the period 2016-2017. A mass spectrometry-based approach was used to analyze the plasma proteins in the enrolled subjects. Subsequently, using recursive feature elimination (RFE) classification and cross-validation with a vector classifier, an association of these proteins in patients with sepsis compared to patients with NISIRS. The protein-protein interaction network was analyzed with String software.
  Results: A total of 277 patients (141 with sepsis and 136 with NISIRS) were included. After performing RFE, 25 proteins in the study patient cohort showed statistical significance, with an accuracy of 0.960, specificity of 0.920, sensitivity of 0.973, and an AUC of 0.985. Of these, 14 proteins (vWF, PPBP, C5, C1RL, FCN3, SAA2, ORM1, ITIH3, GSN, C1QA, CA1, CFB, C3, LBP) have a greater relationship with sepsis while 11 proteins (FN1, IGFALS, SERPINA4, APOE, APOH, C6, SERPINA3, AHSG, LUM, ITIH2, SAA1) are more expressed in NISIRS.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2502.16324.pdf' target='_blank'>https://arxiv.org/pdf/2502.16324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Nourbakhsh, Hoda Mohammadzade
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16324">Deep Time Warping for Multiple Time Series Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time Series Alignment is a critical task in signal processing with numerous real-world applications. In practice, signals often exhibit temporal shifts and scaling, making classification on raw data prone to errors. This paper introduces a novel approach for Multiple Time Series Alignment (MTSA) leveraging Deep Learning techniques. While most existing methods primarily address Multiple Sequence Alignment (MSA) for protein and DNA sequences, there remains a significant gap in alignment methodologies for numerical time series. Additionally, conventional approaches typically focus on pairwise alignment, whereas our proposed method aligns all signals in a multiple manner (all the signals are aligned together at once). This innovation not only enhances alignment efficiency but also significantly improves computational speed. By decomposing into piece-wise linear sections, we introduce varying levels of complexity into the warping function. Additionally, our method ensures the satisfaction of three warping constraints: boundary, monotonicity, and continuity conditions. The utilization of a deep convolutional network allows us to employ a new loss function, addressing some limitations of Dynamic Time Warping (DTW). Experimental results on the UCR Archive 2018, comprising 129 time series datasets, demonstrate that employing our approach to align signals significantly enhances classification accuracy and warping average and also reduces the run time across the majority of these datasets.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2502.13484.pdf' target='_blank'>https://arxiv.org/pdf/2502.13484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuke Uchida, Takaaki Fukui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13484">2.5D U-Net with Depth Reduction for 3D CryoET Object Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryo-electron tomography (cryoET) is a crucial technique for unveiling the structure of protein complexes. Automatically analyzing tomograms captured by cryoET is an essential step toward understanding cellular structures. In this paper, we introduce the 4th place solution from the CZII - CryoET Object Identification competition, which was organized to advance the development of automated tomogram analysis techniques. Our solution adopted a heatmap-based keypoint detection approach, utilizing an ensemble of two different types of 2.5D U-Net models with depth reduction. Despite its highly unified and simple architecture, our method achieved 4th place, demonstrating its effectiveness.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2502.10848.pdf' target='_blank'>https://arxiv.org/pdf/2502.10848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jirka Lhotka, Daniel Probst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10848">Implicit Neural Representations of Molecular Vector-Valued Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecules have various computational representations, including numerical descriptors, strings, graphs, point clouds, and surfaces. Each representation method enables the application of various machine learning methodologies from linear regression to graph neural networks paired with large language models. To complement existing representations, we introduce the representation of molecules through vector-valued functions, or $n$-dimensional vector fields, that are parameterized by neural networks, which we denote molecular neural fields. Unlike surface representations, molecular neural fields capture external features and the hydrophobic core of macromolecules such as proteins. Compared to discrete graph or point representations, molecular neural fields are compact, resolution independent and inherently suited for interpolation in spatial and temporal dimensions. These properties inherited by molecular neural fields lend themselves to tasks including the generation of molecules based on their desired shape, structure, and composition, and the resolution-independent interpolation between molecular conformations in space and time. Here, we provide a framework and proofs-of-concept for molecular neural fields, namely, the parametrization and superresolution reconstruction of a protein-ligand complex using an auto-decoder architecture and the embedding of molecular volumes in latent space using an auto-encoder architecture.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2502.09135.pdf' target='_blank'>https://arxiv.org/pdf/2502.09135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edith Natalia Villegas Garcia, Alessio Ansuini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09135">Interpreting and Steering Protein Language Models through Sparse Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancements in transformer-based language models have revolutionized natural language processing, yet understanding the internal mechanisms of these models remains a significant challenge. This paper explores the application of sparse autoencoders (SAE) to interpret the internal representations of protein language models, specifically focusing on the ESM-2 8M parameter model. By performing a statistical analysis on each latent component's relevance to distinct protein annotations, we identify potential interpretations linked to various protein characteristics, including transmembrane regions, binding sites, and specialized motifs.
  We then leverage these insights to guide sequence generation, shortlisting the relevant latent components that can steer the model towards desired targets such as zinc finger domains. This work contributes to the emerging field of mechanistic interpretability in biological sequence models, offering new perspectives on model steering for sequence design.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2501.17589.pdf' target='_blank'>https://arxiv.org/pdf/2501.17589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiang Li, Yuan-Ting Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17589">Extracting Inter-Protein Interactions Via Multitasking Graph Structure Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying protein-protein interactions (PPI) is crucial for gaining in-depth insights into numerous biological processes within cells and holds significant guiding value in areas such as drug development and disease treatment. Currently, most PPI prediction methods focus primarily on the study of protein sequences, neglecting the critical role of the internal structure of proteins. This paper proposes a novel PPI prediction method named MgslaPPI, which utilizes graph attention to mine protein structural information and enhances the expressive power of the protein encoder through multitask learning strategy. Specifically, we decompose the end-to-end PPI prediction process into two stages: amino acid residue reconstruction (A2RR) and protein interaction prediction (PIP). In the A2RR stage, we employ a graph attention-based residue reconstruction method to explore the internal relationships and features of proteins. In the PIP stage, in addition to the basic interaction prediction task, we introduce two auxiliary tasks, i.e., protein feature reconstruction (PFR) and masked interaction prediction (MIP). The PFR task aims to reconstruct the representation of proteins in the PIP stage, while the MIP task uses partially masked protein features for PPI prediction, with both working in concert to prompt MgslaPPI to capture more useful information. Experimental results demonstrate that MgslaPPI significantly outperforms existing state-of-the-art methods under various data partitioning schemes.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2501.16391.pdf' target='_blank'>https://arxiv.org/pdf/2501.16391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoqing Lian, Jie Zhu, Tianxu Lv, Shiyun Nie, Hang Fan, Guosheng Wu, Yunjun Ge, Lihua Li, Xiangxiang Zeng, Xiang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16391">Inductive-Associative Meta-learning Pipeline with Human Cognitive Patterns for Unseen Drug-Target Interaction Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant differences in protein structures hinder the generalization of existing drug-target interaction (DTI) models, which often rely heavily on pre-learned binding principles or detailed annotations. In contrast, BioBridge designs an Inductive-Associative pipeline inspired by the workflow of scientists who base their accumulated expertise on drawing insights into novel drug-target pairs from weakly related references. BioBridge predicts novel drug-target interactions using limited sequence data, incorporating multi-level encoders with adversarial training to accumulate transferable binding principles. On these principles basis, BioBridge employs a dynamic prototype meta-learning framework to associate insights from weakly related annotations, enabling robust predictions for previously unseen drug-target pairs. Extensive experiments demonstrate that BioBridge surpasses existing models, especially for unseen proteins. Notably, when only homologous protein binding data is available, BioBridge proves effective for virtual screening of the epidermal growth factor receptor and adenosine receptor, underscoring its potential in drug discovery.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2501.14426.pdf' target='_blank'>https://arxiv.org/pdf/2501.14426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Fuest, Alfredo Cuesta, Kalyan Veeramachaneni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14426">CENTS: Generating synthetic electricity consumption time series for rare and unseen scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in large-scale generative modeling have demonstrated the potential of foundation models in domains such as natural language, computer vision, and protein structure prediction. However, their application in the energy and smart grid sector remains limited due to the scarcity and heterogeneity of high-quality data. In this work, we propose a method for creating high-fidelity electricity consumption time series data for rare and unseen context variables (e.g. location, building type, photovoltaics). Our approach, Context Encoding and Normalizing Time Series Generation, or CENTS, includes three key innovations: (i) A context normalization approach that enables inverse transformation for time series context variables unseen during training, (ii) a novel context encoder to condition any state-of-the-art time-series generator on arbitrary numbers and combinations of context variables, (iii) a framework for training this context encoder jointly with a time-series generator using an auxiliary context classification loss designed to increase expressivity of context embeddings and improve model performance. We further provide a comprehensive overview of different evaluation metrics for generative time series models. Our results highlight the efficacy of the proposed method in generating realistic household-level electricity consumption data, paving the way for training larger foundation models in the energy domain on synthetic as well as real-world data.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2501.12309.pdf' target='_blank'>https://arxiv.org/pdf/2501.12309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugenio Borzone, Leandro Di Persia, Matias Gerard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12309">A Hybrid Supervised and Self-Supervised Graph Neural Network for Edge-Centric Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel graph-based deep learning model for tasks involving relations between two nodes (edge-centric tasks), where the focus lies on predicting relationships and interactions between pairs of nodes rather than node properties themselves. This model combines supervised and self-supervised learning, taking into account for the loss function the embeddings learned and patterns with and without ground truth. Additionally it incorporates an attention mechanism that leverages both node and edge features. The architecture, trained end-to-end, comprises two primary components: embedding generation and prediction. First, a graph neural network (GNN) transform raw node features into dense, low-dimensional embeddings, incorporating edge attributes. Then, a feedforward neural model processes the node embeddings to produce the final output. Experiments demonstrate that our model matches or exceeds existing methods for protein-protein interactions prediction and Gene Ontology (GO) terms prediction. The model also performs effectively with one-hot encoding for node features, providing a solution for the previously unsolved problem of predicting similarity between compounds with unknown structures.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2501.07731.pdf' target='_blank'>https://arxiv.org/pdf/2501.07731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sepideh Maleki, Josh Vekhter, Keshav Pingali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07731">HyperQuery: Beyond Binary Link Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Groups with complex set intersection relations are a natural way to model a wide array of data, from the formation of social groups to the complex protein interactions which form the basis of biological life. One approach to representing such higher order relationships is as a hypergraph. However, efforts to apply machine learning techniques to hypergraph structured datasets have been limited thus far. In this paper, we address the problem of link prediction in knowledge hypergraphs as well as simple hypergraphs and develop a novel, simple, and effective optimization architecture that addresses both tasks. Additionally, we introduce a novel feature extraction technique using node level clustering and we show how integrating data from node-level labels can improve system performance. Our self-supervised approach achieves significant improvement over state of the art baselines on several hyperedge prediction and knowledge hypergraph completion benchmarks.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2501.07405.pdf' target='_blank'>https://arxiv.org/pdf/2501.07405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aram Ansary Ogholbake, Qiang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07405">PROTECT: Protein circadian time prediction using unsupervised learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Circadian rhythms regulate the physiology and behavior of humans and animals. Despite advancements in understanding these rhythms and predicting circadian phases at the transcriptional level, predicting circadian phases from proteomic data remains elusive. This challenge is largely due to the scarcity of time labels in proteomic datasets, which are often characterized by small sample sizes, high dimensionality, and significant noise. Furthermore, existing methods for predicting circadian phases from transcriptomic data typically rely on prior knowledge of known rhythmic genes, making them unsuitable for proteomic datasets. To address this gap, we developed a novel computational method using unsupervised deep learning techniques to predict circadian sample phases from proteomic data without requiring time labels or prior knowledge of proteins or genes. Our model involves a two-stage training process optimized for robust circadian phase prediction: an initial greedy one-layer-at-a-time pre-training which generates informative initial parameters followed by fine-tuning. During fine-tuning, a specialized loss function guides the model to align protein expression levels with circadian patterns, enabling it to accurately capture the underlying rhythmic structure within the data. We tested our method on both time-labeled and unlabeled proteomic data. For labeled data, we compared our predictions to the known time labels, achieving high accuracy, while for unlabeled human datasets, including postmortem brain regions and urine samples, we explored circadian disruptions. Notably, our analysis identified disruptions in rhythmic proteins between Alzheimer's disease and control subjects across these samples.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2501.07014.pdf' target='_blank'>https://arxiv.org/pdf/2501.07014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karishma Thakrar, Jiangqin Ma, Max Diamond, Akash Patel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07014">AlgoRxplorers | Precision in Mutation: Enhancing Drug Design with Advanced Protein Stability Prediction Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the impact of single-point amino acid mutations on protein stability is essential for understanding disease mechanisms and advancing drug development. Protein stability, quantified by changes in Gibbs free energy ($ÎÎG$), is influenced by these mutations. However, the scarcity of data and the complexity of model interpretation pose challenges in accurately predicting stability changes. This study proposes the application of deep neural networks, leveraging transfer learning and fusing complementary information from different models, to create a feature-rich representation of the protein stability landscape. We developed four models, with our third model, ThermoMPNN+, demonstrating the best performance in predicting $ÎÎG$ values. This approach, which integrates diverse feature sets and embeddings through latent transfusion techniques, aims to refine $ÎÎG$ predictions and contribute to a deeper understanding of protein dynamics, potentially leading to advancements in disease research and drug discovery.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2501.06615.pdf' target='_blank'>https://arxiv.org/pdf/2501.06615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexuan Xie, Liam Jemison, Yi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06615">A Nonlocal size modified Poisson-Boltzmann Model and Its Finite Element Solver for Protein in Multi-Species Ionic Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Poisson-Boltzmann (PB) model is a widely used implicit solvent model in protein simulations. Although variants, such as the size modified PB and nonlocal modified PB models, have been developed to account for ionic size effects and nonlocal dielectric correlations, no existing PB variants simultaneously incorporate both, due to significant modeling and computational challenges. To address this gap, in this paper, a nonlocal size modified PB (NSMPB) model is introduced and solved using a finite element method for a protein with a three-dimensional molecular structure and an ionic solution containing multiple ion species. In particular, a novel solution decomposition is proposed to overcome the difficulties caused by the increased nonlinearity, nonlocality, and solution singularities of the model. It is then applied to the development of the NSMPB finite element solver, which includes an efficient modified Newton iterative method, an effective damping parameter selection strategy, and good selections of initial iterations. Moreover, the construction of the modified Newton iterative method is mathematically justified. Furthermore, an NSMPB finite element package is developed by integrating a mesh generation tool, a protein data bank file retrieval program, and the PDB2PQR package to simplify and accelerate its usage and application. Finally, numerical experiments are conducted on an ionic solution with four species, proteins with up to 11439 atoms, and irregular interface-fitted tetrahedral box meshes with up to 1188840 vertices. The numerical results confirm the fast convergence and strong robustness of the modified Newton iterative method, demonstrate the high performance of the package, and highlight the crucial roles played by the damping parameter and initial iteration selections in enhancing the method's convergence. The package will be a valuable tool in protein simulations.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2501.04387.pdf' target='_blank'>https://arxiv.org/pdf/2501.04387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni di Sarra, Barbara Bravi, Yasser Roudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04387">The unbearable lightness of Restricted Boltzmann Machines: Theoretical Insights and Biological Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Restricted Boltzmann Machines are simple yet powerful neural networks. They can be used for learning structure in data, and are used as a building block of more complex neural architectures. At the same time, their simplicity makes them easy to use, amenable to theoretical analysis, yielding interpretable models in applications. Here, we focus on reviewing the role that the activation functions, describing the input-output relationship of single neurons in RBM, play in the functionality of these models. We discuss recent theoretical results on the benefits and limitations of different activation functions. We also review applications to biological data analysis, namely neural data analysis, where RBM units are mostly taken to have sigmoid activation functions and binary units, to protein data analysis and immunology where non-binary units and non-sigmoid activation functions have recently been shown to yield important insights into the data. Finally, we discuss open problems addressing which can shed light on broader issues in neural network research.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2412.20329.pdf' target='_blank'>https://arxiv.org/pdf/2412.20329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanny Espitia, Yui Tik Pang, James C. Gumbart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20329">Protein Structure Prediction in the 3D HP Model Using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address protein structure prediction in the 3D Hydrophobic-Polar lattice model through two novel deep learning architectures. For proteins under 36 residues, our hybrid reservoir-based model combines fixed random projections with trainable deep layers, achieving optimal conformations with 25% fewer training episodes. For longer sequences, we employ a long short-term memory network with multi-headed attention, matching best-known energy values. Both architectures leverage a stabilized Deep Q-Learning framework with experience replay and target networks, demonstrating consistent achievement of optimal conformations while significantly improving training efficiency compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2412.19815.pdf' target='_blank'>https://arxiv.org/pdf/2412.19815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Regina Ibragimova, Dimitrios Iliadis, Willem Waegeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19815">Enhancing Drug-Target Interaction Prediction through Transfer Learning from Activity Cliff Prediction Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, machine learning (ML) has gained popularity in the early stages of drug discovery. This trend is unsurprising given the increasing volume of relevant experimental data and the continuous improvement of ML algorithms. However, conventional models, which rely on the principle of molecular similarity, often fail to capture the complexities of chemical interactions, particularly those involving activity cliffs (ACs) - compounds that are structurally similar but exhibit evidently different activity behaviors. In this work, we address two distinct yet related tasks: (1) activity cliff (AC) prediction and (2) drug-target interaction (DTI) prediction. Leveraging insights gained from the AC prediction task, we aim to improve the performance of DTI prediction through transfer learning. A universal model was developed for AC prediction, capable of identifying activity cliffs across diverse targets. Insights from this model were then incorporated into DTI prediction, enabling better handling of challenging cases involving ACs while maintaining similar overall performance. This approach establishes a strong foundation for integrating AC awareness into predictive models for drug discovery. Scientific Contribution This study presents a novel approach that applies transfer learning from AC prediction to enhance DTI prediction, addressing limitations of traditional similarity-based models. By introducing AC-awareness, we improve DTI model performance in structurally complex regions, demonstrating the benefits of integrating compound-specific and protein-contextual information. Unlike previous studies, which treat AC and DTI predictions as separate problems, this work establishes a unified framework to address both data scarcity and prediction challenges in drug discovery.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2412.16664.pdf' target='_blank'>https://arxiv.org/pdf/2412.16664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhu, Shihao Wang, Yong Han, Yao Lu, Shulan Qiu, Ling Jin, Xiangdong Li, Weixiong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16664">Transformer-based toxin-protein interaction analysis prioritizes airborne particulate matter components with potential adverse health effects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Air pollution, particularly airborne particulate matter (PM), poses a significant threat to public health globally. It is crucial to comprehend the association between PM-associated toxic components and their cellular targets in humans to understand the mechanisms by which air pollution impacts health and to establish causal relationships between air pollution and public health consequences. Although many studies have explored the impact of PM on human health, the understanding of the association between toxins and the associated targets remain limited. Leveraging cutting-edge deep learning technologies, we developed tipFormer (toxin-protein interaction prediction based on transformer), a novel deep-learning tool for identifying toxic components capable of penetrating human cells and instigating pathogenic biological activities and signaling cascades. Experimental results show that tipFormer effectively captures interactions between proteins and toxic components. It incorporates dual pre-trained language models to encode protein sequences and chemicals. It employs a convolutional encoder to assimilate the sequential attributes of proteins and chemicals. It then introduces a learning module with a cross-attention mechanism to decode and elucidate the multifaceted interactions pivotal for the hotspots binding proteins and chemicals. Experimental results show that tipFormer effectively captures interactions between proteins and toxic components. This approach offers significant value to air quality and toxicology researchers by allowing high-throughput identification and prioritization of hazards. It supports more targeted laboratory studies and field measurements, ultimately enhancing our understanding of how air pollution impacts human health.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2412.13519.pdf' target='_blank'>https://arxiv.org/pdf/2412.13519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivasankaran Vanaja Pandi, Bharath Ramsundar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13519">Open-Source Protein Language Models for Function Prediction and Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein language models (PLMs) have shown promise in improving the understanding of protein sequences, contributing to advances in areas such as function prediction and protein engineering. However, training these models from scratch requires significant computational resources, limiting their accessibility. To address this, we integrate a PLM into DeepChem, an open-source framework for computational biology and chemistry, to provide a more accessible platform for protein-related tasks.
  We evaluate the performance of the integrated model on various protein prediction tasks, showing that it achieves reasonable results across benchmarks. Additionally, we present an exploration of generating plastic-degrading enzyme candidates using the model's embeddings and latent space manipulation techniques. While the results suggest that further refinement is needed, this approach provides a foundation for future work in enzyme design. This study aims to facilitate the use of PLMs in research fields like synthetic biology and environmental sustainability, even for those with limited computational resources.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2412.10411.pdf' target='_blank'>https://arxiv.org/pdf/2412.10411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Pathak, Guohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10411">Pre-trained protein language model for codon optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: Codon optimization of Open Reading Frame (ORF) sequences is essential for enhancing mRNA stability and expression in applications like mRNA vaccines, where codon choice can significantly impact protein yield which directly impacts immune strength. In this work, we investigate the use of a pre-trained protein language model (PPLM) for getting a rich representation of amino acids which could be utilized for codon optimization. This leaves us with a simpler fine-tuning task over PPLM in optimizing ORF sequences.
  Results: The ORFs generated by our proposed models outperformed their natural counterparts encoding the same proteins on computational metrics for stability and expression. They also demonstrated enhanced performance against the benchmark ORFs used in mRNA vaccines for the SARS-CoV-2 viral spike protein and the varicella-zoster virus (VZV). These results highlight the potential of adapting PPLM for designing ORFs tailored to encode target antigens in mRNA vaccines.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2412.09661.pdf' target='_blank'>https://arxiv.org/pdf/2412.09661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinsong Shao, Qineng Gong, Zeyu Yin, Yu Chen, Yajie Hao, Lei Zhang, Linlin Jiang, Min Yao, Jinlong Li, Fubo Wang, Li Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09661">Language model driven: a PROTAC generation pipeline with dual constraints of structure and property</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The imperfect modeling of ternary complexes has limited the application of computer-aided drug discovery tools in PROTAC research and development. In this study, an AI-assisted approach for PROTAC molecule design pipeline named LM-PROTAC was developed, which stands for language model driven Proteolysis Targeting Chimera, by embedding a transformer-based generative model with dual constraints on structure and properties, referred to as the DCT. This study utilized the fragmentation representation of molecules and developed a language model driven pipeline. Firstly, a language model driven affinity model for protein compounds to screen molecular fragments with high affinity for the target protein. Secondly, structural and physicochemical properties of these fragments were constrained during the generation process to meet specific scenario requirements. Finally, a two-round screening of the preliminary generated molecules using a multidimensional property prediction model to generate a batch of PROTAC molecules capable of degrading disease-relevant target proteins for validation in vitro experiments, thus achieving a complete solution for AI-assisted PROTAC drug generation. Taking the tumor key target Wnt3a as an example, the LM-PROTAC pipeline successfully generated PROTAC molecules capable of inhibiting Wnt3a. The results show that DCT can efficiently generate PROTAC that targets and hydrolyses Wnt3a.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2412.06237.pdf' target='_blank'>https://arxiv.org/pdf/2412.06237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marsha Mariya Kappan, Joby George
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06237">In Silico Pharmacokinetic and Molecular Docking Studies of Natural Plants against Essential Protein KRAS for Treatment of Pancreatic Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A kind of pancreatic cancer called Pancreatic Ductal Adenocarcinoma (PDAC) is anticipated to be one of the main causes of mortality during past years. Evidence from several researches supported the concept that the oncogenic KRAS (Ki-ras2 Kirsten rat sarcoma viral oncogene) mutation is the major cause of pancreatic cancer. KRAS acts as an on-off switch that promotes cell growth. But when the KRAS gene is mutated, it will be in one position, allowing the cell growth uncontrollably. This uncontrollable multiplication of cells causes cancer growth. Therefore, KRAS was selected as the target protein in the study. Fifty plant-derived compounds are selected for the study. To determine whether the examined drugs could bind to the KRAS complex's binding pocket, molecular docking was performed. Computational analyses were used to assess the possible ability of tested substances to pass the Blood Brain Barrier (BBB). To predict the bioactivity of ligands a machine learning model was created. Five machine learning models were created and have chosen the best one among them for analyzing the bioactivity of each ligand. From the fifty plant-derived compounds the compounds with the least binding energies are selected. Then bioactivity of these six compounds is analyzed using Random Forest Regression model. Adsorption, Distribution, Metabolism, Excretion (ADME) properties of compounds are analyzed. The results showed that borneol has powerful effects and acts as a promising agent for the treatment of pancreatic cancer. This suggests that borneol found in plants like mint, ginger, rosemary, etc., is a successful compound for the treatment of pancreatic cancer.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2412.04526.pdf' target='_blank'>https://arxiv.org/pdf/2412.04526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daiheng Zhang, Yan Zeng, Xinyu Hong, Jinbo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04526">Leveraging Multi-modal Representations to Predict Protein Melting Temperatures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately predicting protein melting temperature changes (Delta Tm) is fundamental for assessing protein stability and guiding protein engineering. Leveraging multi-modal protein representations has shown great promise in capturing the complex relationships among protein sequences, structures, and functions. In this study, we develop models based on powerful protein language models, including ESM-2, ESM-3 and AlphaFold, using various feature extraction methods to enhance prediction accuracy. By utilizing the ESM-3 model, we achieve a new state-of-the-art performance on the s571 test dataset, obtaining a Pearson correlation coefficient (PCC) of 0.50. Furthermore, we conduct a fair evaluation to compare the performance of different protein language models in the Delta Tm prediction task. Our results demonstrate that integrating multi-modal protein representations could advance the prediction of protein melting temperatures.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2412.04075.pdf' target='_blank'>https://arxiv.org/pdf/2412.04075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoav Kan-Tor, Michael Morris Danziger, Eden Zohar, Matan Ninio, Yishai Shimoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04075">Does your model understand genes? A benchmark of gene properties for biological and text models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of deep learning methods, particularly foundation models, in biological research has surged in recent years. These models can be text-based or trained on underlying biological data, especially omics data of various types. However, comparing the performance of these models consistently has proven to be a challenge due to differences in training data and downstream tasks. To tackle this problem, we developed an architecture-agnostic benchmarking approach that, instead of evaluating the models directly, leverages entity representation vectors from each model and trains simple predictive models for each benchmarking task. This ensures that all types of models are evaluated using the same input and output types. Here we focus on gene properties collected from professionally curated bioinformatics databases. These gene properties are categorized into five major groups: genomic properties, regulatory functions, localization, biological processes, and protein properties. Overall, we define hundreds of tasks based on these databases, which include binary, multi-label, and multi-class classification tasks. We apply these benchmark tasks to evaluate expression-based models, large language models, protein language models, DNA-based models, and traditional baselines. Our findings suggest that text-based models and protein language models generally outperform expression-based models in genomic properties and regulatory functions tasks, whereas expression-based models demonstrate superior performance in localization tasks. These results should aid in the development of more informed artificial intelligence strategies for biological understanding and therapeutic discovery. To ensure the reproducibility and transparency of our findings, we have made the source code and benchmark data publicly accessible for further investigation and expansion at github.com/BiomedSciAI/gene-benchmark.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2412.02889.pdf' target='_blank'>https://arxiv.org/pdf/2412.02889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajay N. Jain, Ann E. Cleves, W. Patrick Walters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02889">Deep-Learning Based Docking Methods: Fair Comparisons to Conventional Docking Workflows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The diffusion learning method, DiffDock, for docking small-molecule ligands into protein binding sites was recently introduced. Results included comparisons to more conventional docking approaches, with DiffDock showing superior performance. Here, we employ a fully automatic workflow using the Surflex-Dock methods to generate a fair baseline for conventional docking approaches. Results were generated for the common and expected situation where a binding site location is known and also for the condition of an unknown binding site. For the known binding site condition, Surflex-Dock success rates at 2.0 Angstroms RMSD far exceeded those for DiffDock (Top-1/Top-5 success rates, respectively, were 68/81% compared with 45/51%). Glide performed with similar success rates (67/73%) to Surflex-Dock for the known binding site condition, and results for AutoDock Vina and Gnina followed this pattern. For the unknown binding site condition, using an automated method to identify multiple binding pockets, Surflex-Dock success rates again exceeded those of DiffDock, but by a somewhat lesser margin. DiffDock made use of roughly 17,000 co-crystal structures for learning (98% of PDBBind version 2020, pre-2019 structures) for a training set in order to predict on 363 test cases (2% of PDBBind 2020) from 2019 forward. DiffDock's performance was inextricably linked with the presence of near-neighbor cases of close to identical protein-ligand complexes in the training set for over half of the test set cases. DiffDock exhibited a 40 percentage point difference on near-neighbor cases (two-thirds of all test cases) compared with cases with no near-neighbor training case. DiffDock has apparently encoded a type of table-lookup during its learning process, rendering meaningful applications beyond its reach. Further, it does not perform even close to competitively with a competently run modern docking workflow.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2412.01388.pdf' target='_blank'>https://arxiv.org/pdf/2412.01388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katarzyna Janocha, Annabel Ling, Alice Godson, Yulia Lampi, Simon Bornschein, Nils Y. Hammerla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01388">Harnessing Preference Optimisation in Protein LMs for Hit Maturation in Cell Therapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell and immunotherapy offer transformative potential for treating diseases like cancer and autoimmune disorders by modulating the immune system. The development of these therapies is resource-intensive, with the majority of drug candidates failing to progress beyond laboratory testing. While recent advances in machine learning have revolutionised areas such as protein engineering, applications in immunotherapy remain limited due to the scarcity of large-scale, standardised datasets and the complexity of cellular systems. In this work, we address these challenges by leveraging a high-throughput experimental platform to generate data suitable for fine-tuning protein language models. We demonstrate how models fine-tuned using a preference task show surprising correlations to biological assays, and how they can be leveraged for few-shot hit maturation in CARs. This proof-of-concept presents a novel pathway for applying ML to immunotherapy and could generalise to other therapeutic modalities.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2412.01174.pdf' target='_blank'>https://arxiv.org/pdf/2412.01174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daiheng Zhang, Chengyue Gong, Qiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01174">Rectified Flow For Structure Based Drug Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have achieved tremendous success in structure-based drug design in recent years, especially for generating 3D ligand molecules that bind to specific protein pocket. Notably, diffusion models have transformed ligand generation by providing exceptional quality and creativity. However, traditional diffusion models are restricted by their conventional learning objectives, which limit their broader applicability. In this work, we propose a new framework FlowSBDD, which is based on rectified flow model, allows us to flexibly incorporate additional loss to optimize specific target and introduce additional condition either as an extra input condition or replacing the initial Gaussian distribution. Extensive experiments on CrossDocked2020 show that our approach could achieve state-of-the-art performance on generating high-affinity molecules while maintaining proper molecular properties without specifically designing binding site, with up to -8.50 Avg. Vina Dock score and 75.0% Diversity.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2412.00109.pdf' target='_blank'>https://arxiv.org/pdf/2412.00109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Shi, Yixin Tao, Shih-Chi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00109">Deep Neural Network-Based Prediction of B-Cell Epitopes for SARS-CoV and SARS-CoV-2: Enhancing Vaccine Design through Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accurate prediction of B-cell epitopes is critical for guiding vaccine development against infectious diseases, including SARS and COVID-19. This study explores the use of a deep neural network (DNN) model to predict B-cell epitopes for SARS-CoVandSARS-CoV-2,leveraging a dataset that incorporates essential protein and peptide features. Traditional sequence-based methods often struggle with large, complex datasets, but deep learning offers promising improvements in predictive accuracy. Our model employs regularization techniques, such as dropout and early stopping, to enhance generalization, while also analyzing key features, including isoelectric point and aromaticity, that influence epitope recognition. Results indicate an overall accuracy of 82% in predicting COVID-19 negative and positive cases, with room for improvement in detecting positive samples. This research demonstrates the applicability of deep learning in epitope mapping, suggesting that such approaches can enhance the speed and precision of vaccine design for emerging pathogens. Future work could incorporate structural data and diverse viral strains to further refine prediction capabilities.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2411.12853.pdf' target='_blank'>https://arxiv.org/pdf/2411.12853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Poorya Khajouie, Titli Sarkar, Krishna Rauniyar, Li Chen, Wu Xu, Vijay Raghavan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12853">Integrating Secondary Structures Information into Triangular Spatial Relationships (TSR) for Advanced Protein Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structures represent the key to deciphering biological functions. The more detailed form of similarity among these proteins is sometimes overlooked by the conventional structural comparison methods. In contrast, further advanced methods, such as Triangular Spatial Relationship (TSR), have been demonstrated to make finer differentiations. Still, the classical implementation of TSR does not provide for the integration of secondary structure information, which is important for a more detailed understanding of the folding pattern of a protein. To overcome these limitations, we developed the SSE-TSR approach. The proposed method integrates secondary structure elements (SSEs) into TSR-based protein representations. This allows an enriched representation of protein structures by considering 18 different combinations of helix, strand, and coil arrangements. Our results show that using SSEs improves the accuracy and reliability of protein classification to varying degrees. We worked with two large protein datasets of 9.2K and 7.8K samples, respectively. We applied the SSE-TSR approach and used a neural network model for classification. Interestingly, introducing SSEs improved performance statistics for Dataset 1, with accuracy moving from 96.0% to 98.3%. For Dataset 2, where the performance statistics were already good, further small improvements were found with the introduction of SSE, giving an accuracy of 99.5% compared to 99.4%. These results show that SSE integration can dramatically improve TSR key discrimination, with significant benefits in datasets with low initial accuracies and only incremental gains in those with high baseline performance. Thus, SSE-TSR is a powerful bioinformatics tool that improves protein classification and understanding of protein function and interaction.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2411.08766.pdf' target='_blank'>https://arxiv.org/pdf/2411.08766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Bi, Suresh Neethirajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08766">Mapping Methane -- The Impact of Dairy Farm Practices on Emissions Through Satellite Data and Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the correlation between dairy farm characteristics and methane concentrations as derived from satellite observations in Eastern Canada. Utilizing data from 11 dairy farms collected between January 2020 and December 2022, we integrated Sentinel-5P satellite methane data with critical farm-level attributes, including herd genetics, feeding practices, and management strategies. Initial analyses revealed significant correlations with methane concentrations, leading to the application of Variance Inflation Factor (VIF) and Principal Component Analysis (PCA) to address multicollinearity and enhance model stability. Subsequently, machine learning models - specifically Random Forest and Neural Networks - were employed to evaluate feature importance and predict methane emissions. Our findings indicate a strong negative correlation between the Estimated Breeding Value (EBV) for protein percentage and methane concentrations, suggesting that genetic selection for higher milk protein content could be an effective strategy for emissions reduction. The integration of atmospheric transport models with satellite data further refined our emission estimates, significantly enhancing accuracy and spatial resolution. This research underscores the potential of advanced satellite monitoring, machine learning techniques, and atmospheric modeling in improving methane emission assessments within the dairy sector. It emphasizes the critical role of farm-specific characteristics in developing effective mitigation strategies. Future investigations should focus on expanding the dataset and incorporating inversion modeling for more precise emission quantification. Balancing ecological impacts with economic viability will be essential for fostering sustainable dairy farming practices.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2411.07406.pdf' target='_blank'>https://arxiv.org/pdf/2411.07406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Irons, Patrick Cooper, Melanie McGrath, Shahroz Tariq, Andreas Duenser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07406">Towards a criteria-based approach to selecting human-AI interaction mode</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) tools are now prevalent in many knowledge work industries. As AI becomes more capable and interactive, there is a growing need for guidance on how to employ AI most effectively. The A2C framework (Tariq, Chhetri, Nepal & Paris, 2024) distinguishes three decision-making modes for engaging AI: automation (AI completes a task, including decision/action), augmentation (AI supports human to decide) and collaboration (iterative interaction between human and AI). However, selecting the appropriate mode for a specific application is not always straightforward. The goal of the present study was to compile and trial a simple set of criteria to support recommendations about appropriate A2C mode for a given application. Drawing on human factors and computer science literature, we identified key criteria related to elements of the task, impacts on worker and support needs. From these criteria we built a scoring rubric with recommendation for A2C mode. As a preliminary test of this approach, we applied the criteria to cognitive task analysis (CTA) outputs from three tasks in the science domain - genome annotation, biological collections curation and protein crystallization - which provided insights into worker decision points, challenges and expert strategies. This paper describes the method for connecting CTA to A2C, reflecting on the challenges and future directions.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2411.05966.pdf' target='_blank'>https://arxiv.org/pdf/2411.05966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Shah, Shankar Jayaratnam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05966">Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2411.05421.pdf' target='_blank'>https://arxiv.org/pdf/2411.05421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenze Yang, Sarah K. Yorke, Tuomas P. J. Knowles, Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05421">Learning the rules of peptide self-assembly through data mining with large language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptides are ubiquitous and important biologically derived molecules, that have been found to self-assemble to form a wide array of structures. Extensive research has explored the impacts of both internal chemical composition and external environmental stimuli on the self-assembly behaviour of these systems. However, there is yet to be a systematic study that gathers this rich literature data and collectively examines these experimental factors to provide a global picture of the fundamental rules that govern protein self-assembly behavior. In this work, we curate a peptide assembly database through a combination of manual processing by human experts and literature mining facilitated by a large language model. As a result, we collect more than 1,000 experimental data entries with information about peptide sequence, experimental conditions and corresponding self-assembly phases. Utilizing the collected data, ML models are trained and evaluated, demonstrating excellent accuracy (>80\%) and efficiency in peptide assembly phase classification. Moreover, we fine-tune our GPT model for peptide literature mining with the developed dataset, which exhibits markedly superior performance in extracting information from academic publications relative to the pre-trained model. We find that this workflow can substantially improve efficiency when exploring potential self-assembling peptide candidates, through guiding experimental work, while also deepening our understanding of the mechanisms governing peptide self-assembly. In doing so, novel structures can be accessed for a range of applications including sensing, catalysis and biomaterials.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2411.01422.pdf' target='_blank'>https://arxiv.org/pdf/2411.01422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kusal Debnath, Pratip Rana, Preetam Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01422">GramSeq-DTA: A grammar-based drug-target affinity prediction approach fusing gene expression information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drug-target affinity (DTA) prediction is a critical aspect of drug discovery. The meaningful representation of drugs and targets is crucial for accurate prediction. Using 1D string-based representations for drugs and targets is a common approach that has demonstrated good results in drug-target affinity prediction. However, these approach lacks information on the relative position of the atoms and bonds. To address this limitation, graph-based representations have been used to some extent. However, solely considering the structural aspect of drugs and targets may be insufficient for accurate DTA prediction. Integrating the functional aspect of these drugs at the genetic level can enhance the prediction capability of the models. To fill this gap, we propose GramSeq-DTA, which integrates chemical perturbation information with the structural information of drugs and targets. We applied a Grammar Variational Autoencoder (GVAE) for drug feature extraction and utilized two different approaches for protein feature extraction: Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). The chemical perturbation data is obtained from the L1000 project, which provides information on the upregulation and downregulation of genes caused by selected drugs. This chemical perturbation information is processed, and a compact dataset is prepared, serving as the functional feature set of the drugs. By integrating the drug, gene, and target features in the model, our approach outperforms the current state-of-the-art DTA prediction models when validated on widely used DTA datasets (BindingDB, Davis, and KIBA). This work provides a novel and practical approach to DTA prediction by merging the structural and functional aspects of biological entities, and it encourages further research in multi-modal DTA prediction.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2411.00913.pdf' target='_blank'>https://arxiv.org/pdf/2411.00913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boming Kang, Qinghua Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00913">Ratio law: mathematical descriptions for a universal relationship between AI performance and input samples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence based on machine learning and deep learning has made significant advances in various fields such as protein structure prediction and climate modeling. However, a central challenge remains: the "black box" nature of AI, where precise quantitative relationships between inputs and outputs are often lacking. Here, by analyzing 323 AI models trained to predict human essential proteins, we uncovered a ratio law showing that model performance and the ratio of minority to majority samples can be closely linked by two concise equations. Moreover, we mathematically proved that an AI model achieves its optimal performance on a balanced dataset. More importantly, we next explore whether this finding can further guide us to enhance AI models' performance. Therefore, we divided the imbalanced dataset into several balanced subsets to train base classifiers, and then applied a bagging-based ensemble learning strategy to combine these base models. As a result, the equation-guided strategy substantially improved model performance, with increases of 4.06% and 5.28%, respectively, outperforming traditional dataset balancing techniques. Finally, we confirmed the broad applicability and generalization of these equations using different types of classifiers and 10 additional, diverse binary classification tasks. In summary, this study reveals two equations precisely linking AI's input and output, which could be helpful for unboxing the mysterious "black box" of AI.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2410.23321.pdf' target='_blank'>https://arxiv.org/pdf/2410.23321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin, Ma, Dong Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23321">Beyond Current Boundaries: Integrating Deep Learning and AlphaFold for Enhanced Protein Structure Prediction from Low-Resolution Cryo-EM Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constructing atomic models from cryo-electron microscopy (cryo-EM) maps is a crucial yet intricate task in structural biology. While advancements in deep learning, such as convolutional neural networks (CNNs) and graph neural networks (GNNs), have spurred the development of sophisticated map-to-model tools like DeepTracer and ModelAngelo, their efficacy notably diminishes with low-resolution maps beyond 4 Ã. To address this shortfall, our research introduces DeepTracer-LowResEnhance, an innovative framework that synergizes a deep learning-enhanced map refinement technique with the power of AlphaFold. This methodology is designed to markedly improve the construction of models from low-resolution cryo-EM maps. DeepTracer-LowResEnhance was rigorously tested on a set of 37 protein cryo-EM maps, with resolutions ranging between 2.5 to 8.4 Ã, including 22 maps with resolutions lower than 4 Ã. The outcomes were compelling, demonstrating that 95.5\% of the low-resolution maps exhibited a significant uptick in the count of total predicted residues. This denotes a pronounced improvement in atomic model building for low-resolution maps. Additionally, a comparative analysis alongside Phenix's auto-sharpening functionality delineates DeepTracer-LowResEnhance's superior capability in rendering more detailed and precise atomic models, thereby pushing the boundaries of current computational structural biology methodologies.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2410.20318.pdf' target='_blank'>https://arxiv.org/pdf/2410.20318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiangang Cui, Alex Gorodetsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20318">Low-rank Bayesian matrix completion via geodesic Hamiltonian Monte Carlo on Stiefel manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new sampling-based approach for enabling efficient computation of low-rank Bayesian matrix completion and quantifying the associated uncertainty. Firstly, we design a new prior model based on the singular-value-decomposition (SVD) parametrization of low-rank matrices. Our prior is analogous to the seminal nuclear-norm regularization used in non-Bayesian setting and enforces orthogonality in the factor matrices by constraining them to Stiefel manifolds. Then, we design a geodesic Hamiltonian Monte Carlo (-within-Gibbs) algorithm for generating posterior samples of the SVD factor matrices. We demonstrate that our approach resolves the sampling difficulties encountered by standard Gibbs samplers for the common two-matrix factorization used in matrix completion. More importantly, the geodesic Hamiltonian sampler allows for sampling in cases with more general likelihoods than the typical Gaussian likelihood and Gaussian prior assumptions adopted in most of the existing Bayesian matrix completion literature. We demonstrate an applications of our approach to fit the categorical data of a mice protein dataset and the MovieLens recommendation problem. Numerical examples demonstrate superior sampling performance, including better mixing and faster convergence to a stationary distribution. Moreover, they demonstrate improved accuracy on the two real-world benchmark problems we considered.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2410.17293.pdf' target='_blank'>https://arxiv.org/pdf/2410.17293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahar Ali, Anwar Shah, Malik Niaz, Musadaq Mansoord, Sami Ullah, Muhammad Adnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17293">A Fusion-Driven Approach of Attention-Based CNN-BiLSTM for Protein Family Classification -- ProFamNet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advanced automated AI techniques allow us to classify protein sequences and discern their biological families and functions. Conventional approaches for classifying these protein families often focus on extracting N-Gram features from the sequences while overlooking crucial motif information and the interplay between motifs and neighboring amino acids. Recently, convolutional neural networks have been applied to amino acid and motif data, even with a limited dataset of well-characterized proteins, resulting in improved performance. This study presents a model for classifying protein families using the fusion of 1D-CNN, BiLSTM, and an attention mechanism, which combines spatial feature extraction, long-term dependencies, and context-aware representations. The proposed model (ProFamNet) achieved superior model efficiency with 450,953 parameters and a compact size of 1.72 MB, outperforming the state-of-the-art model with 4,578,911 parameters and a size of 17.47 MB. Further, we achieved a higher F1 score (98.30% vs. 97.67%) with more instances (271,160 vs. 55,077) in fewer training epochs (25 vs. 30).
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2410.17173.pdf' target='_blank'>https://arxiv.org/pdf/2410.17173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasha Ektefaie, Olivia Viessmann, Siddharth Narayanan, Drew Dresser, J. Mark Kim, Armen Mkrtchyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17173">Reinforcement learning on structure-conditioned categorical diffusion for protein inverse folding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein inverse folding-that is, predicting an amino acid sequence that will fold into the desired 3D structure-is an important problem for structure-based protein design. Machine learning based methods for inverse folding typically use recovery of the original sequence as the optimization objective. However, inverse folding is a one-to-many problem where several sequences can fold to the same structure. Moreover, for many practical applications, it is often desirable to have multiple, diverse sequences that fold into the target structure since it allows for more candidate sequences for downstream optimizations. Here, we demonstrate that although recent inverse folding methods show increased sequence recovery, their "foldable diversity"-i.e. their ability to generate multiple non-similar sequences that fold into the structures consistent with the target-does not increase. To address this, we present RL-DIF, a categorical diffusion model for inverse folding that is pre-trained on sequence recovery and tuned via reinforcement learning on structural consistency. We find that RL-DIF achieves comparable sequence recovery and structural consistency to benchmark models but shows greater foldable diversity: experiments show RL-DIF can achieve an foldable diversity of 29% on CATH 4.2, compared to 23% from models trained on the same dataset. The PyTorch model weights and sampling code are available on GitHub.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2410.16302.pdf' target='_blank'>https://arxiv.org/pdf/2410.16302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowen Zhao, Francesco A. Aprile, Barbara Bravi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16302">Computational design of target-specific linear peptide binders with TransformerBeta</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The computational prediction and design of peptide binders targeting specific linear epitopes is crucial in biological and biomedical research, yet it remains challenging due to their highly dynamic nature and the scarcity of experimentally solved binding data. To address this problem, we built an unprecedentedly large-scale library of peptide pairs within stable secondary structures (beta sheets), leveraging newly available AlphaFold predicted structures. We then developed a machine learning method based on the Transformer architecture for the design of specific linear binders, in analogy to a language translation task. Our method, TransformerBeta, accurately predicts specific beta strand interactions and samples sequences with beta sheet-like molecular properties, while capturing interpretable physico-chemical interaction patterns. As such, it can propose specific candidate binders targeting linear epitope for experimental validation to inform protein design.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2410.15290.pdf' target='_blank'>https://arxiv.org/pdf/2410.15290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Abbasian, Mahtab Mirmohseni, Masoumeh Nasiri Kenari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15290">On the size of error ball in DNA storage channels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent experiments have demonstrated the feasibility of storing digital information in macromolecules such as DNA and protein. However, the DNA storage channel is prone to errors such as deletions, insertions, and substitutions. During the synthesis and reading phases of DNA strings, many noisy copies of the original string are generated. The problem of recovering the original string from these noisy copies is known as sequence reconstruction. A key concept in this problem is the error ball, which is the set of all possible sequences that can result from a limited number of errors applied to the original sequence. Levenshtein showed that the minimum number of noisy copies required for a given channel to recover the original sequence is equal to one plus the maximum size of the intersection of two error balls. Therefore, deriving the size of the error ball for any channel and any sequence is essential for solving the sequence reconstruction problem. In DNA storage systems, multiple types of errors such as deletion, insertion and substitution in a string could occur simultaneously. In this work, we aim to derive the size of the error ball for channels with multiple types of errors and at most three edits. Specifically, we consider the channels with single-deletion double-substitution, single-deletion double-insertion and single-insertion single-substitution errors.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2410.08938.pdf' target='_blank'>https://arxiv.org/pdf/2410.08938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benson Chen, Tomasz Danel, Gabriel H. S. Dreiman, Patrick J. McEnaney, Nikhil Jain, Kirill Novikov, Spurti Umesh Akki, Joshua L. Turnbull, Virja Atul Pandya, Boris P. Belotserkovskii, Jared Bryce Weaver, Ankita Biswas, Dat Nguyen, Kent Gorday, Mohammad Sultan, Nathaniel Stanley, Daniel M Whalen, Divya Kanichar, Christoph Klein, Emily Fox, R. Edward Watts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08938">KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNA-Encoded Libraries (DELs) represent a transformative technology in drug discovery, facilitating the high-throughput exploration of vast chemical spaces. Despite their potential, the scarcity of publicly available DEL datasets presents a bottleneck for the advancement of machine learning methodologies in this domain. To address this gap, we introduce KinDEL, one of the largest publicly accessible DEL datasets and the first one that includes binding poses from molecular docking experiments. Focused on two kinases, Mitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor Tyrosine Kinase 1 (DDR1), KinDEL includes 81 million compounds, offering a rich resource for computational exploration. Additionally, we provide comprehensive biophysical assay validation data, encompassing both on-DNA and off-DNA measurements, which we use to evaluate a suite of machine learning techniques, including novel structure-based probabilistic models. We hope that our benchmark, encompassing both 2D and 3D structures, will help advance the development of machine learning models for data-driven hit identification using DELs.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2410.07890.pdf' target='_blank'>https://arxiv.org/pdf/2410.07890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio S. Ferreira, John Ashburner, Arabella Bouzigues, Chatrin Suksasilp, Lucy L. Russell, Phoebe H. Foster, Eve Ferry-Bolder, John C. van Swieten, Lize C. Jiskoot, Harro Seelaar, Raquel Sanchez-Valle, Robert Laforce, Caroline Graff, Daniela Galimberti, Rik Vandenberghe, Alexandre de Mendonca, Pietro Tiraboschi, Isabel Santana, Alexander Gerhard, Johannes Levin, Sandro Sorbi, Markus Otto, Florence Pasquier, Simon Ducharme, Chris R. Butler, Isabelle Le Ber, Elizabeth Finger, Maria C. Tartaglia, Mario Masellis, James B. Rowe, Matthis Synofzik, Fermin Moreno, Barbara Borroni, Samuel Kaski, Jonathan D. Rohrer, Janaina Mourao-Miranda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07890">Identifying latent disease factors differently expressed in patient subgroups using group factor analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a novel approach to uncover subgroup-specific and subgroup-common latent factors addressing the challenges posed by the heterogeneity of neurological and mental disorders, which hinder disease understanding, treatment development, and outcome prediction. The proposed approach, sparse Group Factor Analysis (GFA) with regularised horseshoe priors, was implemented with probabilistic programming and can uncover associations (or latent factors) among multiple data modalities differentially expressed in sample subgroups. Synthetic data experiments showed the robustness of our sparse GFA by correctly inferring latent factors and model parameters. When applied to the Genetic Frontotemporal Dementia Initiative (GENFI) dataset, which comprises patients with frontotemporal dementia (FTD) with genetically defined subgroups, the sparse GFA identified latent disease factors differentially expressed across the subgroups, distinguishing between "subgroup-specific" latent factors within homogeneous groups and "subgroup common" latent factors shared across subgroups. The latent disease factors captured associations between brain structure and non-imaging variables (i.e., questionnaires assessing behaviour and disease severity) across the different genetic subgroups, offering insights into disease profiles. Importantly, two latent factors were more pronounced in the two more homogeneous FTD patient subgroups (progranulin (GRN) and microtubule-associated protein tau (MAPT) mutation), showcasing the method's ability to reveal subgroup-specific characteristics. These findings underscore the potential of sparse GFA for integrating multiple data modalities and identifying interpretable latent disease factors that can improve the characterization and stratification of patients with neurological and mental health disorders.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2409.19115.pdf' target='_blank'>https://arxiv.org/pdf/2409.19115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodrigo Henrique Ramos, Yago Augusto Bardelotte, Cynthia de Oliveira Lage Ferreira, Adenilso Simao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19115">Identifying Key Genes in Cancer Networks Using Persistent Homology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying driver genes is crucial for understanding oncogenesis and developing targeted cancer therapies. Driver discovery methods using protein or pathway networks rely on traditional network science measures, focusing on nodes, edges, or community metrics. These methods can overlook the high-dimensional interactions that cancer genes have within cancer networks. This study presents a novel method using Persistent Homology to analyze the role of driver genes in higher-order structures within Cancer Consensus Networks derived from main cellular pathways. We integrate mutation data from six cancer types and three biological functions: DNA Repair, Chromatin Organization, and Programmed Cell Death. We systematically evaluated the impact of gene removal on topological voids ($Î²_2$ structures) within the Cancer Consensus Networks. Our results reveal that only known driver genes and cancer-associated genes influence these structures, while passenger genes do not. Although centrality measures alone proved insufficient to fully characterize impact genes, combining higher-order topological analysis with traditional network metrics can improve the precision of distinguishing between drivers and passengers. This work shows that cancer genes play an important role in higher-order structures, going beyond pairwise measures, and provides an approach to distinguish drivers and cancer-associated genes from passenger genes.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2409.16552.pdf' target='_blank'>https://arxiv.org/pdf/2409.16552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Omidi, Gianluca Fabi, Xiaopeng Wang, James C. M. Hwang, Yevgeny Berdichevsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16552">Device for detection of activity-dependent changes in neural spheroids at MHz and GHz frequencies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intracellular processes triggered by neural activity include changes in ionic concentrations, protein release, and synaptic vesicle cycling. These processes play significant roles in neurological disorders. The beneficial effects of brain stimulation may also be mediated through intracellular changes. There is a lack of label-free techniques for monitoring activity-dependent intracellular changes. Electromagnetic (EM) waves at frequencies larger than 1x10^6 Hz (1 MHz) were previously used to probe intracellular contents of cells, as cell membrane becomes transparent at this frequency range. EM waves interact with membranes of intracellular organelles, proteins, and water in the MHz-GHz range. In this work, we developed a device for probing the interaction between intracellular contents of active neurons and EM waves. The device used an array of grounded coplanar waveguides (GCPWs) to deliver EM waves to a three-dimensional (3D) spheroid of rat cortical neurons. Neural activity was evoked using optogenetics, with synchronous detection of propagation of EM waves. Broadband measurements were conducted in the MHz-GHz range to track changes in transmission coefficients. Neuronal activity was found to reversibly alter EM wave transmission. Pharmacological suppression of neuronal activity abolished changes in transmission. Time constants of changes in transmission were in the range of seconds to tens of seconds, suggesting the presence of relatively slow, activity-dependent intracellular processes. This study provides the first evidence that EM transmission through neuronal tissue is activity-dependent in MHz-GHz range. Device developed in this work may find future applications in studies of the mechanisms of neurological disorders and the development of new therapies.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2409.13467.pdf' target='_blank'>https://arxiv.org/pdf/2409.13467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roman Joeres, Daniel Bojar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13467">Higher-Order Message Passing for Glycan Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Glycans are the most complex biological sequence, with monosaccharides forming extended, non-linear sequences. As post-translational modifications, they modulate protein structure, function, and interactions. Due to their diversity and complexity, predictive models of glycan properties and functions are still insufficient.
  Graph Neural Networks (GNNs) are deep learning models designed to process and analyze graph-structured data. These architectures leverage the connectivity and relational information in graphs to learn effective representations of nodes, edges, and entire graphs. Iteratively aggregating information from neighboring nodes, GNNs capture complex patterns within graph data, making them particularly well-suited for tasks such as link prediction or graph classification across domains.
  This work presents a new model architecture based on combinatorial complexes and higher-order message passing to extract features from glycan structures into a latent space representation. The architecture is evaluated on an improved GlycanML benchmark suite, establishing a new state-of-the-art performance. We envision that these improvements will spur further advances in computational glycosciences and reveal the roles of glycans in biology.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2409.12995.pdf' target='_blank'>https://arxiv.org/pdf/2409.12995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia Buhmann, Ward Haddadin, LukÃ¡Å¡ Pravda, Alan Bilsland, Hagen Triendl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12995">Improving generalisability of 3D binding affinity models in low data regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein-ligand binding affinity is an essential part of computer-aided drug design. However, generalisable and performant global binding affinity models remain elusive, particularly in low data regimes. Despite the evolution of model architectures, current benchmarks are not well-suited to probe the generalisability of 3D binding affinity models. Furthermore, 3D global architectures such as GNNs have not lived up to performance expectations. To investigate these issues, we introduce a novel split of the PDBBind dataset, minimizing similarity leakage between train and test sets and allowing for a fair and direct comparison between various model architectures. On this low similarity split, we demonstrate that, in general, 3D global models are superior to protein-specific local models in low data regimes. We also demonstrate that the performance of GNNs benefits from three novel contributions: supervised pre-training via quantum mechanical data, unsupervised pre-training via small molecule diffusion, and explicitly modeling hydrogen atoms in the input graph. We believe that this work introduces promising new approaches to unlock the potential of GNN architectures for binding affinity modelling.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2409.08729.pdf' target='_blank'>https://arxiv.org/pdf/2409.08729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Plesner, Hans Henrik Brandenborg SÃ¸rensen, SÃ¸ren Hauberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08729">Accurate Computation of the Logarithm of Modified Bessel Functions on GPUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bessel functions are critical in scientific computing for applications such as machine learning, protein structure modeling, and robotics. However, currently, available routines lack precision or fail for certain input ranges, such as when the order $v$ is large, and GPU-specific implementations are limited. We address the precision limitations of current numerical implementations while dramatically improving the runtime. We propose two novel algorithms for computing the logarithm of modified Bessel functions of the first and second kinds by computing intermediate values on a logarithmic scale. Our algorithms are robust and never have issues with underflows or overflows while having relative errors on the order of machine precision, even for inputs where existing libraries fail. In C++/CUDA, our algorithms have median and maximum speedups of 45x and 6150x for GPU and 17x and 3403x for CPU, respectively, over the ranges of inputs and third-party libraries tested. Compared to SciPy, the algorithms have median and maximum speedups of 77x and 300x for GPU and 35x and 98x for CPU, respectively, over the tested inputs.
  The ability to robustly compute a solution and the low relative errors allow us to fit von Mises-Fisher, vMF, distributions to high-dimensional neural network features. This is, e.g., relevant for uncertainty quantification in metric learning. We obtain image feature data by processing CIFAR10 training images with the convolutional layers of a pre-trained ResNet50. We successfully fit vMF distributions to 2048-, 8192-, and 32768-dimensional image feature data using our algorithms. Our approach provides fast and accurate results while existing implementations in SciPy and mpmath fail to fit successfully.
  Our approach is readily implementable on GPUs, and we provide a fast open-source implementation alongside this paper.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2409.04491.pdf' target='_blank'>https://arxiv.org/pdf/2409.04491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huma Perveen, Julie Weeds
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04491">Protein sequence classification using natural language processing techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: This study aimed to enhance protein sequence classification using natural language processing (NLP) techniques while addressing the impact of sequence similarity on model performance. We compared various machine learning and deep learning models under two different data-splitting strategies: random splitting and ECOD family-based splitting, which ensures evolutionary-related sequences are grouped together. Methods: The study evaluated models such as K-Nearest Neighbors (KNN), Multinomial NaÃ¯ve Bayes, Logistic Regression, Multi-Layer Perceptron (MLP), Decision Tree, Random Forest, XGBoost, Voting and Stacking classifiers, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and transformer models (BertForSequenceClassification, DistilBERT, and ProtBert). Performance was tested using different amino acid ranges and sequence lengths with a focus on generalization across unseen evolutionary families. Results: The Voting classifier achieved the highest performance with 74% accuracy, 74% weighted F1 score, and 65% macro F1 score under random splitting, while ProtBERT obtained 77% accuracy, 76% weighted F1 score, and 61% macro F1 score among transformer models. However, performance declined across all models when tested using ECOD-based splitting, revealing the impact of sequence similarity on classification performance. Conclusion: Advanced NLP techniques, particularly ensemble methods like Voting classifiers, and transformer models show significant potential in protein classification, with sufficient training data and sequence similarity management being crucial for optimal performance. However, the use of biologically meaningful splitting methods, such as ECOD family-based splitting, is crucial for realistic performance evaluation and generalization to unseen evolutionary families.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2409.03658.pdf' target='_blank'>https://arxiv.org/pdf/2409.03658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elyssa Sliheet, Md Abu Talha, Weihua Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03658">A DNN Biophysics Model with Topological and Electrostatic Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this project, we provide a deep-learning neural network (DNN) based biophysics model to predict protein properties. The model uses multi-scale and uniform topological and electrostatic features generated with protein structural information and force field, which governs the molecular mechanics. The topological features are generated using the element specified persistent homology (ESPH) while the electrostatic features are fast computed using a Cartesian treecode. These features are uniform in number for proteins with various sizes thus the broadly available protein structure database can be used in training the network. These features are also multi-scale thus the resolution and computational cost can be balanced by the users. The machine learning simulation on over 4000 protein structures shows the efficiency and fidelity of these features in representing the protein structure and force field for the predication of their biophysical properties such as electrostatic solvation energy. Tests on topological or electrostatic features alone and the combination of both showed the optimal performance when both features are used. This model shows its potential as a general tool in assisting biophysical properties and function prediction for the broad biomolecules using data from both theoretical computing and experiments.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2409.00610.pdf' target='_blank'>https://arxiv.org/pdf/2409.00610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shania Mitra, Lei Huang, Manolis Kellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00610">ProteinRPN: Towards Accurate Protein Function Prediction with Graph-Based Region Proposals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein function prediction is a crucial task in bioinformatics, with significant implications for understanding biological processes and disease mechanisms. While the relationship between sequence and function has been extensively explored, translating protein structure to function continues to present substantial challenges. Various models, particularly, CNN and graph-based deep learning approaches that integrate structural and functional data, have been proposed to address these challenges. However, these methods often fall short in elucidating the functional significance of key residues essential for protein functionality, as they predominantly adopt a retrospective perspective, leading to suboptimal performance.
  Inspired by region proposal networks in computer vision, we introduce the Protein Region Proposal Network (ProteinRPN) for accurate protein function prediction. Specifically, the region proposal module component of ProteinRPN identifies potential functional regions (anchors) which are refined through the hierarchy-aware node drop pooling layer favoring nodes with defined secondary structures and spatial proximity. The representations of the predicted functional nodes are enriched using attention mechanisms and subsequently fed into a Graph Multiset Transformer, which is trained with supervised contrastive (SupCon) and InfoNCE losses on perturbed protein structures. Our model demonstrates significant improvements in predicting Gene Ontology (GO) terms, effectively localizing functional residues within protein structures. The proposed framework provides a robust, scalable solution for protein function annotation, advancing the understanding of protein structure-function relationships in computational biology.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2408.16245.pdf' target='_blank'>https://arxiv.org/pdf/2408.16245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sully F. Chen, Robert J. Steele, Glen M. Hocky, Beakal Lemeneh, Shivanand P. Lad, Eric K. Oermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16245">Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. To date, most biosequence transformers have been trained on single-omic data-either proteins or nucleic acids and have seen incredible success in downstream tasks in each domain, with particularly noteworthy breakthroughs in protein structural modeling. However, single-omic pre-training limits the ability of these models to capture cross-modal interactions. Here we present OmniBioTE, the largest open-source multi-omic model trained on over 250 billion tokens of mixed protein and nucleic acid data. We show that despite only being trained on unlabeled sequence data, OmniBioTE learns joint representations mapping genes to their corresponding protein sequences. We further demonstrate that OmbiBioTE achieves state-of-the-art results predicting the change in Gibbs free energy (ÎG) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any a priori structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Lastly, compared to single-omic controls trained with identical compute, OmniBioTE demonstrates superior performance-per-FLOP across both multi-omic and single-omic benchmarks, highlighting the power of a unified modeling approach for biological sequences.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2408.12519.pdf' target='_blank'>https://arxiv.org/pdf/2408.12519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sina Sarparast, Aldo Zaimi, Maximilian Ebert, Michael-Rock Goldsmith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12519">Advanced atom-level representations for protein flexibility prediction utilizing graph neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein dynamics play a crucial role in many biological processes and drug interactions. However, measuring, and simulating protein dynamics is challenging and time-consuming. While machine learning holds promise in deciphering the determinants of protein dynamics from structural information, most existing methods for protein representation learning operate at the residue level, ignoring the finer details of atomic interactions. In this work, we propose for the first time to use graph neural networks (GNNs) to learn protein representations at the atomic level and predict B-factors from protein 3D structures. The B-factor reflects the atomic displacement of atoms in proteins, and can serve as a surrogate for protein flexibility. We compared different GNN architectures to assess their performance. The Meta-GNN model achieves a correlation coefficient of 0.71 on a large and diverse test set of over 4k proteins (17M atoms) from the Protein Data Bank (PDB), outperforming previous methods by a large margin. Our work demonstrates the potential of representations learned by GNNs for protein flexibility prediction and other related tasks.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2408.11356.pdf' target='_blank'>https://arxiv.org/pdf/2408.11356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kelei He, Tiejun Dong, Jinhui Wu, Junfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11356">One-step Structure Prediction and Screening for Protein-Ligand Complexes using Multi-Task Geometric Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the structure of the protein-ligand complex is crucial to drug development. Existing virtual structure measurement and screening methods are dominated by docking and its derived methods combined with deep learning. However, the sampling and scoring methodology have largely restricted the accuracy and efficiency. Here, we show that these two fundamental tasks can be accurately tackled with a single model, namely LigPose, based on multi-task geometric deep learning. By representing the ligand and the protein pair as a graph, LigPose directly optimizes the three-dimensional structure of the complex, with the learning of binding strength and atomic interactions as auxiliary tasks, enabling its one-step prediction ability without docking tools. Extensive experiments show LigPose achieved state-of-the-art performance on major tasks in drug research. Its considerable improvements indicate a promising paradigm of AI-based pipeline for drug development.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2408.04847.pdf' target='_blank'>https://arxiv.org/pdf/2408.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amish Mishra, Francis Motta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04847">A Pipeline for Data-Driven Learning of Topological Features with Applications to Protein Stability Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a data-driven method to learn interpretable topological features of biomolecular data and demonstrate the efficacy of parsimonious models trained on topological features in predicting the stability of synthetic mini proteins. We compare models that leverage automatically-learned structural features against models trained on a large set of biophysical features determined by subject-matter experts (SME). Our models, based only on topological features of the protein structures, achieved 92%-99% of the performance of SME-based models in terms of the average precision score. By interrogating model performance and feature importance metrics, we extract numerous insights that uncover high correlations between topological features and SME features. We further showcase how combining topological features and SME features can lead to improved model performance over either feature set used in isolation, suggesting that, in some settings, topological features may provide new discriminating information not captured in existing SME features that are useful for protein stability prediction.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2407.21298.pdf' target='_blank'>https://arxiv.org/pdf/2407.21298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Wu, Yu Pan, Fuqi Zhou, Jinghui Yan, Chuanlu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21298">A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Persistent homology is an effective method for extracting topological information, represented as persistent diagrams, of spatial structure data. Hence it is well-suited for the study of protein structures. Attempts to incorporate Persistent homology in machine learning methods of protein function prediction have resulted in several techniques for vectorizing persistent diagrams. However, current vectorization methods are excessively artificial and cannot ensure the effective utilization of information or the rationality of the methods. To address this problem, we propose a more geometrical vectorization method of persistent diagrams based on maximal margin classification for Banach space, and additionaly propose a framework that utilizes topological data analysis to identify proteins with specific functions. We evaluated our vectorization method using a binary classification task on proteins and compared it with the statistical methods that exhibit the best performance among thirteen commonly used vectorization methods. The experimental results indicate that our approach surpasses the statistical methods in both robustness and precision.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2407.18317.pdf' target='_blank'>https://arxiv.org/pdf/2407.18317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Swati Adhikari, Parthajit Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18317">CavDetect: A DBSCAN Algorithm based Novel Cavity Detection Model on Protein Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cavities on the structures of proteins are formed due to interaction between proteins and some small molecules, known as ligands. These are basically the locations where ligands bind with proteins. Actual detection of such locations is all-important to succeed in the entire drug design process. This study proposes a Voronoi Tessellation based novel cavity detection model that is used to detect cavities on the structure of proteins. As the atom space of protein structure is dense and of large volumes and the DBSCAN (Density Based Spatial Clustering of Applications with Noise) algorithm can handle such type of data very well as well as it is not mandatory to have knowledge about the numbers of clusters (cavities) in data as priori in this algorithm, this study proposes to implement the proposed algorithm with the DBSCAN algorithm.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2407.11439.pdf' target='_blank'>https://arxiv.org/pdf/2407.11439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhun Lee, Gyumin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11439">Repurformer: Transformers for Repurposing-Aware Molecule Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating as diverse molecules as possible with desired properties is crucial for drug discovery research, which invokes many approaches based on deep generative models today. Despite recent advancements in these models, particularly in variational autoencoders (VAEs), generative adversarial networks (GANs), Transformers, and diffusion models, a significant challenge known as \textit{the sample bias problem} remains. This problem occurs when generated molecules targeting the same protein tend to be structurally similar, reducing the diversity of generation. To address this, we propose leveraging multi-hop relationships among proteins and compounds. Our model, Repurformer, integrates bi-directional pretraining with Fast Fourier Transform (FFT) and low-pass filtering (LPF) to capture complex interactions and generate diverse molecules. A series of experiments on BindingDB dataset confirm that Repurformer successfully creates substitutes for anchor compounds that resemble positive compounds, increasing diversity between the anchor and generated compounds.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2407.10666.pdf' target='_blank'>https://arxiv.org/pdf/2407.10666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Peng, Ang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10666">Flow Perturbation to Accelerate Unbiased Sampling of Boltzmann distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flow-based generative models have been employed for sampling the Boltzmann distribution, but their application to high-dimensional systems is hindered by the significant computational cost of obtaining the Jacobian of the flow. To overcome this challenge, we introduce the flow perturbation method, which incorporates optimized stochastic perturbations into the flow. By reweighting trajectories generated by the perturbed flow, our method achieves unbiased sampling of the Boltzmann distribution with orders of magnitude speedup compared to both brute force Jacobian calculations and the Hutchinson estimator. Notably, it accurately sampled the Chignolin protein with all atomic Cartesian coordinates explicitly represented, which, to our best knowledge, is the largest molecule ever Boltzmann sampled in such detail using generative models.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2407.07817.pdf' target='_blank'>https://arxiv.org/pdf/2407.07817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Bezerra-Brandao, Ronaldo Romario Tunque Cahui, Layla Hirsh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07817">Daisy: An integrated repeat protein curation service</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tandem repeats in proteins identification, classification and curation is a complex process that requires manual processing from experts, processing power and time. There are recent and relevant advances applying machine learning for protein structure prediction and repeat classification that are useful for this process. However, no service contemplates required databases and software to supplement researching on repeat proteins. In this publication we present Daisy, an integrated repeat protein curation web service. This service can process Protein Data Bank (PDB) and the AlphaFold Database entries for tandem repeats identification. In addition, it uses an algorithm to search a sequence against a library of Pfam hidden Markov model (HMM). Repeat classifications are associated with the identified families through RepeatsDB. This prediction is considered for enhancing the ReUPred algorithm execution and hastening the repeat units identification process. The service can also operate every associated PDB and AlphaFold structure with a UniProt proteome registry. Availability: The Daisy web service is freely accessible at daisy.bioinformatica.org.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2407.07718.pdf' target='_blank'>https://arxiv.org/pdf/2407.07718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Li, Giulia Guidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07718">High-Performance Sorting-Based k-mer Counting in Distributed Memory with Flexible Hybrid Parallelism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In generating large quantities of DNA data, high-throughput sequencing technologies require advanced bioinformatics infrastructures for efficient data analysis. k-mer counting, the process of quantifying the frequency of fixed-length k DNA subsequences, is a fundamental step in various bioinformatics pipelines, including genome assembly and protein prediction. Due to the growing volume of data, the scaling of the counting process is critical. In the literature, distributed memory software uses hash tables, which exhibit poor cache friendliness and consume excessive memory. They often also lack support for flexible parallelism, which makes integration into existing bioinformatics pipelines difficult. In this work, we propose HySortK, a highly efficient sorting-based distributed memory k-mer counter. HySortK reduces the communication volume through a carefully designed communication scheme and domain-specific optimization strategies. Furthermore, we introduce an abstract task layer for flexible hybrid parallelism to address load imbalances in different scenarios. HySortK achieves a 2-10x speedup compared to the GPU baseline on 4 and 8 nodes. Compared to state-of-the-art CPU software, HySortK achieves up to 2x speedup while reducing peak memory usage by 30% on 16 nodes. Finally, we integrated HySortK into an existing genome assembly pipeline and achieved up to 1.8x speedup, proving its flexibility and practicality in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2407.01574.pdf' target='_blank'>https://arxiv.org/pdf/2407.01574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Ducrocq, Lukas Grunewald, Sebastian Westenhoff, Fredrik Lindsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01574">cryoSPHERE: Single-particle heterogeneous reconstruction from cryo EM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The three-dimensional structure of proteins plays a crucial role in determining their function. Protein structure prediction methods, like AlphaFold, offer rapid access to a protein structure. However, large protein complexes cannot be reliably predicted, and proteins are dynamic, making it important to resolve their full conformational distribution. Single-particle cryo-electron microscopy (cryo-EM) is a powerful tool for determining the structures of large protein complexes. Importantly, the numerous images of a given protein contain underutilized information about conformational heterogeneity. These images are very noisy projections of the protein, and traditional methods for cryo-EM reconstruction are limited to recovering only one or a few consensus conformations. In this paper, we introduce cryoSPHERE, which is a deep learning method that uses a nominal protein structure (e.g., from AlphaFold) as input, learns how to divide it into segments, and moves these segments as approximately rigid bodies to fit the different conformations present in the cryo-EM dataset. This approach provides enough constraints to enable meaningful reconstructions of single protein structural ensembles. We demonstrate this with two synthetic datasets featuring varying levels of noise, as well as two real dataset. We show that cryoSPHERE is very resilient to the high levels of noise typically encountered in experiments, where we see consistent improvements over the current state-of-the-art for heterogeneous reconstruction.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2406.19521.pdf' target='_blank'>https://arxiv.org/pdf/2406.19521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadeel Elayan, Samar Elmaadawy, Andrew W. Eckford, Raviraj Adve, Josep Jornet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19521">A Thermal Study of Terahertz Induced Protein Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins can be regarded as thermal nanosensors in an intra-body network. Upon being stimulated by Terahertz (THz) frequencies that match their vibrational modes, protein molecules experience resonant absorption and dissipate their energy as heat, undergoing a thermal process. This paper aims to analyze the effect of THz signaling on the protein heat dissipation mechanism. We therefore deploy a mathematical framework based on the heat diffusion model to characterize how proteins absorb THz-electromagnetic (EM) energy from the stimulating EM fields and subsequently release this energy as heat to their immediate surroundings. We also conduct a parametric study to explain the impact of the signal power, pulse duration, and interparticle distance on the protein thermal analysis. In addition, we demonstrate the relationship between the change in temperature and the opening probability of thermally-gated ion channels. Our results indicate that a controlled temperature change can be achieved in an intra-body environment by exciting protein particles at their resonant frequencies. We further verify our results numerically using COMSOL Multiphysics and introduce an experimental framework that assesses the effects of THz radiation on protein particles. We conclude that under controlled heating, protein molecules can serve as hotspots that impact thermally-gated ion channels. Through the presented work, we infer that the heating process can be engineered on different time and length scales by controlling the THz-EM signal input.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2406.18314.pdf' target='_blank'>https://arxiv.org/pdf/2406.18314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matan Halfon, Tomer Cohen, Raanan Fattal, Dina Schneidman-Duhovny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18314">ContactNet: Geometric-Based Deep Learning Model for Predicting Protein-Protein Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning approaches achieved significant progress in predicting protein structures. These methods are often applied to protein-protein interactions (PPIs) yet require Multiple Sequence Alignment (MSA) which is unavailable for various interactions, such as antibody-antigen. Computational docking methods are capable of sampling accurate complex models, but also produce thousands of invalid configurations. The design of scoring functions for identifying accurate models is a long-standing challenge. We develop a novel attention-based Graph Neural Network (GNN), ContactNet, for classifying PPI models obtained from docking algorithms into accurate and incorrect ones. When trained on docked antigen and modeled antibody structures, ContactNet doubles the accuracy of current state-of-the-art scoring functions, achieving accurate models among its Top-10 at 43% of the test cases. When applied to unbound antibodies, its Top-10 accuracy increases to 65%. This performance is achieved without MSA and the approach is applicable to other types of interactions, such as host-pathogens or general PPIs.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2406.16681.pdf' target='_blank'>https://arxiv.org/pdf/2406.16681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yossra Gharbi, RocÃ­o Mercado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16681">A Comprehensive Review of Emerging Approaches in Machine Learning for De Novo PROTAC Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Targeted protein degradation (TPD) is a rapidly growing field in modern drug discovery that aims to regulate the intracellular levels of proteins by harnessing the cell's innate degradation pathways to selectively target and degrade disease-related proteins. This strategy creates new opportunities for therapeutic intervention in cases where occupancy-based inhibitors have not been successful. Proteolysis-targeting chimeras (PROTACs) are at the heart of TPD strategies, which leverage the ubiquitin-proteasome system for the selective targeting and proteasomal degradation of pathogenic proteins. As the field evolves, it becomes increasingly apparent that the traditional methodologies for designing such complex molecules have limitations. This has led to the use of machine learning (ML) and generative modeling to improve and accelerate the development process. In this review, we explore the impact of ML on de novo PROTAC design $-$ an aspect of molecular design that has not been comprehensively reviewed despite its significance. We delve into the distinct characteristics of PROTAC linker design, underscoring the complexities required to create effective bifunctional molecules capable of TPD. We then examine how ML in the context of fragment-based drug design (FBDD), honed in the realm of small-molecule drug discovery, is paving the way for PROTAC linker design. Our review provides a critical evaluation of the limitations inherent in applying this method to the complex field of PROTAC development. Moreover, we review existing ML works applied to PROTAC design, highlighting pioneering efforts and, importantly, the limitations these studies face. By offering insights into the current state of PROTAC development and the integral role of ML in PROTAC design, we aim to provide valuable perspectives for researchers in their pursuit of better design strategies for this new modality.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2406.13106.pdf' target='_blank'>https://arxiv.org/pdf/2406.13106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Abdeen Hamed, Tamer E. Fandy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13106">Accelerating Complex Disease Treatment through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of this research is to introduce a network specialized in predicting drugs that can be repurposed by investigating real-world evidence sources, such as clinical trials and biomedical literature. Specifically, it aims to generate drug combination therapies for complex diseases (e.g., cancer, Alzheimer's). We present a multilayered network medicine approach, empowered by a highly configured ChatGPT prompt engineering system, which is constructed on the fly to extract drug mentions in clinical trials. Additionally, we introduce a novel algorithm that connects real-world evidence with disease-specific signaling pathways (e.g., KEGG database). This sheds light on the repurposability of drugs if they are found to bind with one or more protein constituents of a signaling pathway. To demonstrate, we instantiated the framework for breast cancer and found that, out of 46 breast cancer signaling pathways, the framework identified 38 pathways that were covered by at least two drugs. This evidence signals the potential for combining those drugs. Specifically, the most covered signaling pathway, ID hsa:2064, was covered by 108 drugs, some of which can be combined. Conversely, the signaling pathway ID hsa:1499 was covered by only two drugs, indicating a significant gap for further research. Our network medicine framework, empowered by GenAI, shows promise in identifying drug combinations with a high degree of specificity, knowing the exact signaling pathways and proteins that serve as targets. It is noteworthy that ChatGPT successfully accelerated the process of identifying drug mentions in clinical trials, though further investigations are required to determine the relationships among the drug mentions.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2406.09159.pdf' target='_blank'>https://arxiv.org/pdf/2406.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boshen Wang, Bowei Ye, Lin Xu, Jie Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09159">ALPHAGMUT: A Rationale-Guided Alpha Shape Graph Neural Network to Evaluate Mutation Effects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In silico methods evaluating the mutation effects of missense mutations are providing an important approach for understanding mutations in personal genomes and identifying disease-relevant biomarkers. However, existing methods, including deep learning methods, heavily rely on sequence-aware information, and do not fully leverage the potential of available 3D structural information. In addition, these methods may exhibit an inability to predict mutations in domains difficult to formulate sequence-based embeddings. In this study, we introduce a novel rationale-guided graph neural network AlphaGMut to evaluate mutation effects and to distinguish pathogenic mutations from neutral mutations. We compute the alpha shapes of protein structures to obtain atomic-resolution edge connectivities and map them to an accurate residue-level graph representation. We then compute structural-, topological-, biophysical-, and sequence properties of the mutation sites, which are assigned as node attributes in the graph. These node attributes could effectively guide the graph neural network to learn the difference between pathogenic and neutral mutations using k-hop message passing with a short training period. We demonstrate that AlphaGMut outperforms state-of-the-art methods, including DeepMind's AlphaMissense, in many performance metrics. In addition, AlphaGMut has the advantage of performing well in alignment-free settings, which provides broader prediction coverage and better generalization compared to current methods requiring deep sequence-aware information.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2406.04519.pdf' target='_blank'>https://arxiv.org/pdf/2406.04519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eirini Katsidoniotaki, Biao Su, Eleni Kelasidi, Themistoklis P. Sapsis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04519">Multifidelity digital twin for real-time monitoring of structural dynamics in aquaculture net cages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the global population grows and climate change intensifies, sustainable food production is critical. Marine aquaculture offers a viable solution, providing a sustainable protein source. However, the industry's expansion requires novel technologies for remote management and autonomous operations. Digital twin technology can advance the aquaculture industry, but its adoption has been limited. Fish net cages, which are flexible floating structures, are critical yet vulnerable components of aquaculture farms. Exposed to harsh and dynamic marine environments, the cages experience significant loads and risk damage, leading to fish escapes, environmental impacts, and financial losses. We propose a multifidelity surrogate modeling framework for integration into a digital twin for real-time monitoring of aquaculture net cage structural dynamics under stochastic marine conditions. Central to this framework is the nonlinear autoregressive Gaussian process method, which learns complex, nonlinear cross-correlations between models of varying fidelity. It combines low-fidelity simulation data with a small set of high-fidelity field sensor measurements, which offer the real dynamics but are costly and spatially sparse. Validated at the SINTEF ACE fish farm in Norway, our digital twin receives online metocean data and accurately predicts net cage displacements and mooring line loads, aligning closely with field measurements. The proposed framework is beneficial where application-specific data are scarce, offering rapid predictions and real-time system representation. The developed digital twin prevents potential damages by assessing structural integrity and facilitates remote operations with unmanned underwater vehicles. Our work also compares GP and GCNs for predicting net cage deformation, highlighting the latter's effectiveness in complex structural applications.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2405.16179.pdf' target='_blank'>https://arxiv.org/pdf/2405.16179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elisenda Feliu, Nidhi Kaihnsa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16179">Network reduction and absence of Hopf Bifurcations in dual phosphorylation networks with three Intermediates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phosphorylation networks, representing the mechanisms by which proteins are phosphorylated at one or multiple sites, are ubiquitous in cell signalling and display rich dynamics such as unlimited multistability. Dual-site phosphorylation networks are known to exhibit oscillations in the form of periodic trajectories, when phosphorylation and dephosphorylation occurs as a mixed mechanism: phosphorylation of the two sites requires one encounter of the kinase, while dephosphorylation of the two sites requires two encounters with the phosphatase. A still open question is whether a mechanism requiring two encounters for both phosphorylation and dephosphorylation also admits oscillations. In this work we provide evidence in favor of the absence of oscillations of this network by precluding Hopf bifurcations in any reduced network comprising three out of its four intermediate protein complexes. Our argument relies on a novel network reduction step that preserves the absence of Hopf bifurcations, and on a detailed analysis of the semi-algebraic conditions precluding Hopf bifurcations obtained from Hurwitz determinants of the characteristic polynomial of the Jacobian of the system. We conjecture that the removal of certain reverse reactions appearing in Michaelis-Menten-type mechanisms does not have an impact on the presence or absence of Hopf bifurcations. We prove an implication of the conjecture under certain favorable scenarios and support the conjecture with additional example-based evidence.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2405.09788.pdf' target='_blank'>https://arxiv.org/pdf/2405.09788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Offert, Paul Kim, Qiaoyu Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09788">Synthesizing Proteins on the Graphics Card. Protein Folding and the Limits of Critical AI Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of the transformer architecture in protein folding, as exemplified by DeepMind's AlphaFold project, and its implications for the understanding of so-called large language models. The prevailing discourse often assumes a ready-made analogy between proteins, encoded as sequences of amino acids, and natural language, which we term the language paradigm of computational (structural) biology. Instead of assuming this analogy as given, we critically evaluate it to assess the kind of knowledge-making afforded by the transformer architecture. We first trace the analogy's emergence and historical development, carving out the influence of structural linguistics on structural biology beginning in the mid-20th century. We then examine three often overlooked preprocessing steps essential to the transformer architecture, including subword tokenization, word embedding, and positional encoding, to demonstrate its regime of representation based on continuous, high-dimensional vector spaces, which departs from the discrete nature of language. The successful deployment of transformers in protein folding, we argue, discloses what we consider a non-linguistic approach to token processing intrinsic to the architecture. We contend that through this non-linguistic processing, the transformer architecture carves out unique epistemological territory and produces a new class of knowledge, distinct from established domains. We contend that our search for intelligent machines has to begin with the shape, rather than the place, of intelligence. Consequently, the emerging field of critical AI studies should take methodological inspiration from the history of science in its quest to conceptualize the contributions of artificial intelligence to knowledge-making, within and beyond the domain-specific sciences.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2405.06663.pdf' target='_blank'>https://arxiv.org/pdf/2405.06663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunji Ko, Seul Lee, Minseon Kim, Dongki Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06663">Protein Representation Learning by Capturing Protein Sequence-Structure-Function Relationship</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of protein representation learning is to extract knowledge from protein databases that can be applied to various protein-related downstream tasks. Although protein sequence, structure, and function are the three key modalities for a comprehensive understanding of proteins, existing methods for protein representation learning have utilized only one or two of these modalities due to the difficulty of capturing the asymmetric interrelationships between them. To account for this asymmetry, we introduce our novel asymmetric multi-modal masked autoencoder (AMMA). AMMA adopts (1) a unified multi-modal encoder to integrate all three modalities into a unified representation space and (2) asymmetric decoders to ensure that sequence latent features reflect structural and functional information. The experiments demonstrate that the proposed AMMA is highly effective in learning protein representations that exhibit well-aligned inter-modal relationships, which in turn makes it effective for various downstream protein-related tasks.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2405.06511.pdf' target='_blank'>https://arxiv.org/pdf/2405.06511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonghan Yu, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06511">Towards Less Biased Data-driven Scoring with Deep Learning-Based End-to-end Database Search in Tandem Mass Spectrometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peptide identification in mass spectrometry-based proteomics is crucial for understanding protein function and dynamics. Traditional database search methods, though widely used, rely on heuristic scoring functions and statistical estimations have to be introduced for a higher identification rate. Here, we introduce DeepSearch, the first deep learning-based end-to-end database search method for tandem mass spectrometry. DeepSearch leverages a modified transformer-based encoder-decoder architecture under the contrastive learning framework. Unlike conventional methods that rely on ion-to-ion matching, DeepSearch adopts a data-driven approach to score peptide spectrum matches. DeepSearch is also the first deep learning-based method that can profile variable post-translational modifications in a zero-shot manner. We showed that DeepSearch's scoring scheme expressed less bias and did not require any statistical estimation. We validated DeepSearch's accuracy and robustness across various datasets, including those from species with diverse protein compositions and a modification-enriched dataset. DeepSearch sheds new light on database search methods in tandem mass spectrometry.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2405.01983.pdf' target='_blank'>https://arxiv.org/pdf/2405.01983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frederic Renard, Cyprien Courtot, Alfredo Reichlin, Oliver Bent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01983">Model-based reinforcement learning for protein backbone design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein nanomaterials of predefined shape and characteristics has the potential to dramatically impact the medical industry. Machine learning (ML) has proven successful in protein design, reducing the need for expensive wet lab experiment rounds. However, challenges persist in efficiently exploring the protein fitness landscapes to identify optimal protein designs. In response, we propose the use of AlphaZero to generate protein backbones, meeting shape and structural scoring requirements. We extend an existing Monte Carlo tree search (MCTS) framework by incorporating a novel threshold-based reward and secondary objectives to improve design precision. This innovation considerably outperforms existing approaches, leading to protein backbones that better respect structural scores. The application of AlphaZero is novel in the context of protein backbone design and demonstrates promising performance. AlphaZero consistently surpasses baseline MCTS by more than 100% in top-down protein design tasks. Additionally, our application of AlphaZero with secondary objectives uncovers further promising outcomes, indicating the potential of model-based reinforcement learning (RL) in navigating the intricate and nuanced aspects of protein design
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2405.00283.pdf' target='_blank'>https://arxiv.org/pdf/2405.00283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel A. Isaacson, Ying Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00283">An Unstructured Mesh Reaction-Drift-Diffusion Master Equation with Reversible Reactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a convergent reaction-drift-diffusion master equation (CRDDME) to facilitate the study of reaction processes in which spatial transport is influenced by drift due to one-body potential fields within general domain geometries. The generalized CRDDME is obtained through two steps. We first derive an unstructured grid jump process approximation for reversible diffusions, enabling the simulation of drift-diffusion processes where the drift arises due to a conservative field that biases particle motion. Leveraging the Edge-Averaged Finite Element method, our approach preserves detailed balance of drift-diffusion fluxes at equilibrium, and preserves an equilibrium Gibbs-Boltzmann distribution for particles undergoing drift-diffusion on the unstructured mesh. We next formulate a spatially-continuous volume reactivity particle-based reaction-drift-diffusion model for reversible reactions of the form $\textrm{A} + \textrm{B} \leftrightarrow \textrm{C}$. A finite volume discretization is used to generate jump process approximations to reaction terms in this model. The discretization is developed to ensure the combined reaction-drift-diffusion jump process approximation is consistent with detailed balance of reaction fluxes holding at equilibrium, along with supporting a discrete version of the continuous equilibrium state. The new CRDDME model represents a continuous-time discrete-space jump process approximation to the underlying volume reactivity model. We demonstrate the convergence and accuracy of the new CRDDME through a number of numerical examples, and illustrate its use on an idealized model for membrane protein receptor dynamics in T cell signaling.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2404.17954.pdf' target='_blank'>https://arxiv.org/pdf/2404.17954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giorgos Kritikakis, Ioannis G Tollis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17954">Parameterized Linear Time Transitive Closure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inquiries such as whether a task A depends on a task B, whether an author A has been influenced by a paper B, whether a certain protein is associated with a specific biological process or molecular function, or whether class A inherits from class B, are just a few examples of inquiries that can be modeled as reachability queries on a network (Directed Graph). Digital systems answer myriad such inquiries every day.
  In this paper, we discuss the transitive closure problem. We focus on applicable solutions that enable us to answer queries fast, in constant time, and can serve in real-world applications. In contrast to the majority of research on this topic that revolves around the construction of a two-dimensional adjacency matrix, we present an approach that builds a reachability indexing scheme. This scheme enables us to answer queries in constant time and can be built in parameterized linear time. In addition, it captures a compressed data structure. Our approach and algorithms are validated by extensive experiments that shed light on the factors that play a key role in this problem. To stress the efficiency of this solution and demonstrate the potential to apply our approach to important problems, we use it to speed up Fulkerson's method for finding the width of a DAG. Our results challenge the prevailing belief, reiterated over the last thirty years, regarding the efficiency of this method.
  Our approach is based on the concept of chain decomposition. Before we delve into its description, we introduce, analyze, and utilize a chain decomposition algorithm. Furthermore, we explore how chain decomposition can facilitate transitive closure solutions introducing a general purpose linear time reduction technique that removes a large subset of transitive edges given any chain decomposition.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2404.16911.pdf' target='_blank'>https://arxiv.org/pdf/2404.16911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Angioletti, Stefano Raniolo, Vittorio Limongelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16911">HEroBM: a deep equivariant graph neural network for universal backmapping from coarse-grained to all-atom representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Molecular simulations have assumed a paramount role in the fields of chemistry, biology, and material sciences, being able to capture the intricate dynamic properties of systems. Within this realm, coarse-grained (CG) techniques have emerged as invaluable tools to sample large-scale systems and reach extended timescales by simplifying system representation. However, CG approaches come with a trade-off: they sacrifice atomistic details that might hold significant relevance in deciphering the investigated process. Therefore, a recommended approach is to identify key CG conformations and process them using backmapping methods, which retrieve atomistic coordinates. Currently, rule-based methods yield subpar geometries and rely on energy relaxation, resulting in less-than-optimal outcomes. Conversely, machine learning techniques offer higher accuracy but are either limited in transferability between systems or tied to specific CG mappings. In this work, we introduce HEroBM, a dynamic and scalable method that employs deep equivariant graph neural networks and a hierarchical approach to achieve high-resolution backmapping. HEroBM handles any type of CG mapping, offering a versatile and efficient protocol for reconstructing atomistic structures with high accuracy. Focused on local principles, HEroBM spans the entire chemical space and is transferable to systems of varying sizes. We illustrate the versatility of our framework through diverse biological systems, including a complex real-case scenario. Here, our end-to-end backmapping approach accurately generates the atomistic coordinates of a G protein-coupled receptor bound to an organic small molecule within a cholesterol/phospholipid bilayer.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2404.11068.pdf' target='_blank'>https://arxiv.org/pdf/2404.11068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiwen Zhu, Arkadiusz Nowaczynski, Rundong Li, Jie Xin, Yifei Song, Michal Marcinkiewicz, Sukru Burc Eryilmaz, Jun Yang, Michael Andersch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11068">ScaleFold: Reducing AlphaFold Initial Training Time to 10 Hours</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AlphaFold2 has been hailed as a breakthrough in protein folding. It can rapidly predict protein structures with lab-grade accuracy. However, its implementation does not include the necessary training code. OpenFold is the first trainable public reimplementation of AlphaFold. AlphaFold training procedure is prohibitively time-consuming, and gets diminishing benefits from scaling to more compute resources. In this work, we conducted a comprehensive analysis on the AlphaFold training procedure based on Openfold, identified that inefficient communications and overhead-dominated computations were the key factors that prevented the AlphaFold training from effective scaling. We introduced ScaleFold, a systematic training method that incorporated optimizations specifically for these factors. ScaleFold successfully scaled the AlphaFold training to 2080 NVIDIA H100 GPUs with high resource utilization. In the MLPerf HPC v3.0 benchmark, ScaleFold finished the OpenFold benchmark in 7.51 minutes, shown over $6\times$ speedup than the baseline. For training the AlphaFold model from scratch, ScaleFold completed the pretraining in 10 hours, a significant improvement over the seven days required by the original AlphaFold pretraining baseline.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2404.08108.pdf' target='_blank'>https://arxiv.org/pdf/2404.08108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krzysztof Kotowski, Irena Roterman, Katarzyna Stapor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08108">DisorderUnetLM: Validating ProteinUnet for efficient protein intrinsic disorder prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of intrinsic disorder regions has significant implications for understanding protein functions and dynamics. It can help to discover novel protein-protein interactions essential for designing new drugs and enzymes. Recently, a new generation of predictors based on protein language models (pLMs) is emerging. These algorithms reach state-of-the-art accuracy with-out calculating time-consuming multiple sequence alignments (MSAs). The article introduces the new DisorderUnetLM disorder predictor, which builds upon the idea of ProteinUnet. It uses the Attention U-Net convolutional neural network and incorporates features from the ProtTrans pLM. DisorderUnetLM achieves top results in the direct comparison with recent predictors exploiting MSAs and pLMs. Moreover, among 43 predictors from the latest CAID-2 benchmark, it ranks 1st for the Disorder-NOX subset (ROC-AUC of 0.844) and 10th for the Disorder-PDB subset (ROC-AUC of 0.924). The code and model are publicly available and fully reproducible at doi.org/10.24433/CO.7350682.v1.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2403.14983.pdf' target='_blank'>https://arxiv.org/pdf/2403.14983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junya Wang, Yi-Jiao Zhang, Cong Xu, Jiaze Li, Jiachen Sun, Jiarong Xie, Ling Feng, Tianshou Zhou, Yanqing Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14983">Reconstructing the evolution history of networked complex systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution processes of complex systems carry key information in the systems' functional properties. Applying machine learning algorithms, we demonstrate that the historical formation process of various networked complex systems can be extracted, including protein-protein interaction, ecology, and social network systems. The recovered evolution process has demonstrations of immense scientific values, such as interpreting the evolution of protein-protein interaction network, facilitating structure prediction, and particularly revealing the key co-evolution features of network structures such as preferential attachment, community structure, local clustering, degree-degree correlation that could not be explained collectively by previous theories. Intriguingly, we discover that for large networks, if the performance of the machine learning model is slightly better than a random guess on the pairwise order of links, reliable restoration of the overall network formation process can be achieved. This suggests that evolution history restoration is generally highly feasible on empirical networks.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2403.08797.pdf' target='_blank'>https://arxiv.org/pdf/2403.08797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James S. L. Browning, Daniel R. Tauritz, John Beckmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08797">Evolutionary Algorithms Simulating Molecular Evolution: A New Field Proposal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The genetic blueprint for the essential functions of life is encoded in DNA, which is translated into proteins -- the engines driving most of our metabolic processes. Recent advancements in genome sequencing have unveiled a vast diversity of protein families, but compared to the massive search space of all possible amino acid sequences, the set of known functional families is minimal. One could say nature has a limited protein "vocabulary." The major question for computational biologists, therefore, is whether this vocabulary can be expanded to include useful proteins that went extinct long ago, or maybe never evolved in the first place. We outline a computational approach to solving this problem. By merging evolutionary algorithms, machine learning (ML), and bioinformatics, we can facilitate the development of completely novel proteins which have never existed before. We envision this work forming a new sub-field of computational evolution we dub evolutionary algorithms simulating molecular evolution (EASME).
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2403.07925.pdf' target='_blank'>https://arxiv.org/pdf/2403.07925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David C. Williams, Neil Inala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07925">Physics-informed generative model for drug-like molecule conformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a diffusion-based, generative model for conformer generation. Our model is focused on the reproduction of bonded structure and is constructed from the associated terms traditionally found in classical force fields to ensure a physically relevant representation. Techniques in deep learning are used to infer atom typing and geometric parameters from a training set. Conformer sampling is achieved by taking advantage of recent advancements in diffusion-based generation. By training on large, synthetic data sets of diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method, high accuracy is achieved for bonded parameters, exceeding that of conventional, knowledge-based methods. Results are also compared to experimental structures from the Protein Databank (PDB) and Cambridge Structural Database (CSD).
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2403.04395.pdf' target='_blank'>https://arxiv.org/pdf/2403.04395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqun Li, Jingcheng Yu, Qiwei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04395">SGNet: Folding Symmetrical Protein Complex with Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology. However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination. The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible. In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training. To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies. SGNet conducts feature extraction on a single subunit and generates the whole assembly using our proposed symmetry module, which largely mitigates computational problems caused by sequence length. Thanks to the elaborate design of modeling symmetry consistently, we can model all global symmetry types in quaternary protein structure prediction. Extensive experimental results on a benchmark of symmetrical protein complexes further demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2403.04187.pdf' target='_blank'>https://arxiv.org/pdf/2403.04187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pouria Mistani, Venkatesh Mysore
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04187">Preference optimization of protein language models as a multi-objective binder design paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a multi-objective binder design paradigm based on instruction fine-tuning and direct preference optimization (DPO) of autoregressive protein language models (pLMs). Multiple design objectives are encoded in the language model through direct optimization on expert curated preference sequence datasets comprising preferred and dispreferred distributions. We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion. Generated binder samples demonstrate median isoelectric point (pI) improvements by $17\%-60\%$.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2403.00306.pdf' target='_blank'>https://arxiv.org/pdf/2403.00306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurav Dhar, Amlan Saha, Dhiman Goswami, Md. Abul Kashem Mia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00306">qPMS Sigma -- An Efficient and Exact Parallel Algorithm for the Planted $(l, d)$ Motif Search Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motif finding is an important step for the detection of rare events occurring in a set of DNA or protein sequences. Extraction of information about these rare events can lead to new biological discoveries. Motifs are some important patterns that have numerous applications including the identification of transcription factors and their binding sites, composite regulatory patterns, similarity between families of proteins, etc. Although several flavors of motif searching algorithms have been studied in the literature, we study the version known as $ (l, d) $-motif search or Planted Motif Search (PMS). In PMS, given two integers $ l $, $ d $ and $ n $ input sequences we try to find all the patterns of length $ l $ that appear in each of the $ n $ input sequences with at most $ d $ mismatches. We also discuss the quorum version of PMS in our work that finds motifs that are not planted in all the input sequences but at least in $ q $ of the sequences. Our algorithm is mainly based on the algorithms qPMSPrune, qPMS7, TraverStringRef and PMS8. We introduce some techniques to compress the input strings and make faster comparison between strings with bitwise operations. Our algorithm performs a little better than the existing exact algorithms to solve the qPMS problem in DNA sequence. We have also proposed an idea for parallel implementation of our algorithm.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2403.00043.pdf' target='_blank'>https://arxiv.org/pdf/2403.00043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Josip PeniÄ, Tin VlaÅ¡iÄ, Roland G. Huber, Yue Wan, Mile Å ikiÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00043">RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While RNA has recently been recognized as an interesting small-molecule drug target, many challenges remain to be addressed before we take full advantage of it. This emphasizes the necessity to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides a huge potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date, with 650M parameters pre-trained on 36M non-coding RNA sequences from several databases. It can extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2402.13653.pdf' target='_blank'>https://arxiv.org/pdf/2402.13653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eli M Carrami, Sahand Sharifzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13653">PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding protein structure and function is crucial in biology. However, current computational methods are often task-specific and resource-intensive. To address this, we propose zero-shot Protein Question Answering (PQA), a task designed to answer a wide range of protein-related queries without task-specific training. The success of PQA hinges on high-quality datasets and robust evaluation strategies, both of which are lacking in current research. Existing datasets suffer from biases, noise, and lack of evolutionary context, while current evaluation methods fail to accurately assess model performance. We introduce the Pika framework to overcome these limitations. Pika comprises a curated, debiased dataset tailored for PQA and a biochemically relevant benchmarking strategy. We also propose multimodal large language models as a strong baseline for PQA, leveraging their natural language processing and knowledge. This approach promises a more flexible and efficient way to explore protein properties, advancing protein research. Our comprehensive PQA framework, Pika, including dataset, code, and model checkpoints, is openly accessible on github.com/EMCarrami/Pika, promoting wider research in the field.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2402.07148.pdf' target='_blank'>https://arxiv.org/pdf/2402.07148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric L. Buehler, Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07148">X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable and adaptable models with strong domain knowledge and the capability to integrate across areas of knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, protein biophysics, mechanics and quantum-mechanics based molecular properties, we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, adversarial agentic modeling including ontological knowledge graph construction, as well as molecular design. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins or quantum mechanical molecular properties, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2402.05953.pdf' target='_blank'>https://arxiv.org/pdf/2402.05953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Hwan Park, Vikash Prasad, Sydney Newsom, Fares Najar, Rakhi Rajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05953">idMotif: An Interactive Motif Identification in Protein Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article introduces idMotif, a visual analytics framework designed to aid domain experts in the identification of motifs within protein sequences. Motifs, short sequences of amino acids, are critical for understanding the distinct functions of proteins. Identifying these motifs is pivotal for predicting diseases or infections. idMotif employs a deep learning-based method for the categorization of protein sequences, enabling the discovery of potential motif candidates within protein groups through local explanations of deep learning model decisions. It offers multiple interactive views for the analysis of protein clusters or groups and their sequences. A case study, complemented by expert feedback, illustrates idMotif's utility in facilitating the analysis and identification of protein sequences and motifs.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2402.04268.pdf' target='_blank'>https://arxiv.org/pdf/2402.04268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Ghafarollahi, M. J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04268">ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data -- natural vibrational frequencies -- via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2402.03946.pdf' target='_blank'>https://arxiv.org/pdf/2402.03946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Busra Senderin, Nurcan Tuncbag, Elif Surer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03946">BioNet-XR: Biological Network Visualization Framework for Virtual Reality and Mixed Reality Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein-protein interaction networks (PPIN) enable the study of cellular processes in organisms. Visualizing PPINs in extended reality (XR), including virtual reality (VR) and mixed reality (MR), is crucial for exploring subnetworks, evaluating protein positions, and collaboratively analyzing and discussing on networks with the help of recent technological advancements. Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in VR and MR environments. BioNet-XR was developed with the Unity3D game engine. Our framework provides state-of-the-art methods and visualization features including teleportation between nodes, general and first-person view to explore the network, subnetwork construction via PageRank, Steiner tree, and all-pair shortest path algorithms for a given set of initial nodes. We used usability tests to gather feedback from both specialists (bioinformaticians) and generalists (multidisciplinary groups), addressing the need for usability evaluations of visualization tools. In the MR version of BioNet-XR, users can seamlessly transition to real-world environments and interact with protein interaction networks. BioNet-XR is highly modular and adaptable for visualization of other biological networks, such as metabolic and regulatory networks, and extension with additional network methods.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2402.01829.pdf' target='_blank'>https://arxiv.org/pdf/2402.01829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas V, Swati Agarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01829">Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2401.10211.pdf' target='_blank'>https://arxiv.org/pdf/2401.10211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyi Li, Menglu Li, Lida Zhu, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10211">Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2401.05370.pdf' target='_blank'>https://arxiv.org/pdf/2401.05370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Ghorbani, Leo Gendelev, Paul Beroza, Michael J. Keiser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05370">Autoregressive fragment-based diffusion for pocket-aware ligand design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2401.04246.pdf' target='_blank'>https://arxiv.org/pdf/2401.04246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph C. Kim, David Bloore, Karan Kapoor, Jun Feng, Ming-Hong Hao, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04246">Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2401.02789.pdf' target='_blank'>https://arxiv.org/pdf/2401.02789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hilbert Yuen In Lam, Xing Er Ong, Marek Mutwil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02789">Large Language Models in Plant Biology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs), such as ChatGPT, have taken the world by storm and have passed certain forms of the Turing test. However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multi-purpose prediction tools able to explain cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2401.00014.pdf' target='_blank'>https://arxiv.org/pdf/2401.00014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>J. Gliozzo, G. MarinÃ², A. Bonometti, M. Frasca, D. Malchiodi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00014">Resource-Limited Automated Ki67 Index Estimation in Breast Cancer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of tumor progression and chemotherapy response has been recently tackled exploiting Tumor Infiltrating Lymphocytes (TILs) and the nuclear protein Ki67 as prognostic factors. Recently, deep neural networks (DNNs) have been shown to achieve top results in estimating Ki67 expression and simultaneous determination of intratumoral TILs score in breast cancer cells. However, in the last ten years the extraordinary progress induced by deep models proliferated at least as much as their resource demand. The exorbitant computational costs required to query (and in some cases also to store) a deep model represent a strong limitation in resource-limited contexts, like that of IoT-based applications to support healthcare personnel. To this end, we propose a resource consumption-aware DNN for the effective estimate of the percentage of Ki67-positive cells in breast cancer screenings. Our approach reduced up to 75% and 89% the usage of memory and disk space respectively, up to 1.5x the energy consumption, and preserved or improved the overall accuracy of a benchmark state-of-the-art solution. Encouraged by such positive results, we developed and structured the adopted framework so as to allow its general purpose usage, along with a public software repository to support its usage.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2512.17527.pdf' target='_blank'>https://arxiv.org/pdf/2512.17527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17527">SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2512.13927.pdf' target='_blank'>https://arxiv.org/pdf/2512.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophia Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13927">A Complete Guide to Spherical Equivariant Graph Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2512.08613.pdf' target='_blank'>https://arxiv.org/pdf/2512.08613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manzi Kevin Maxime
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08613">Protein Secondary Structure Prediction Using Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2512.05222.pdf' target='_blank'>https://arxiv.org/pdf/2512.05222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanhua Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05222">Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2511.22893.pdf' target='_blank'>https://arxiv.org/pdf/2511.22893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastián Espinel-Ríos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22893">Switching-time bioprocess control with pulse-width-modulated optogenetics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biotechnology can benefit from dynamic control to improve production efficiency. In this context, optogenetics enables modulation of gene expression using light as an external input, allowing fine-tuning of protein levels to unlock dynamic metabolic control and regulation of cell growth. Optogenetic systems can be actuated by light intensity. However, relying solely on intensity-driven control (i.e., signal amplitude) may fail to properly tune optogenetic bioprocesses when the dose-response relationship (i.e., light intensity versus gene-expression strength) is steep. In these cases, tunability is effectively constrained to either fully active or fully repressed gene expression, with little intermediate regulation. Pulse-width modulation, a concept widely used in electronics, can alleviate this issue by alternating between fully ON and OFF light intensity within forcing periods, thereby smoothing the average response and enhancing process controllability. Naturally, optimizing pulse-width-modulated optogenetics entails a switching-time optimal control problem with a binary input over many forcing periods. While this can be formulated as a mixed-integer program on a refined time grid, the number of decision variables can grow rapidly with increasing time-grid resolution and number of forcing periods, compromising tractability. Here, we propose an alternative solution based on reinforcement learning. We parametrize control actions via the duty cycle, a continuous variable that encodes the ON-to-OFF switching time within each forcing period, thereby respecting the intrinsic binary nature of the light intensity.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2511.08648.pdf' target='_blank'>https://arxiv.org/pdf/2511.08648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stanislav Selitskiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08648">Compact Artificial Neural Network Models for Predicting Protein Residue -- RNA Base Binding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Artificial Neural Network (ANN) models have demonstrated success in various domains, including general text and image generation, drug discovery, and protein-RNA (ribonucleic acid) binding tasks. However, these models typically demand substantial computational resources, time, and data for effective training. Given that such extensive resources are often inaccessible to many researchers and that life sciences data sets are frequently limited, we investigated whether small ANN models could achieve acceptable accuracy in protein-RNA prediction. We experimented with shallow feed-forward ANNs comprising two hidden layers and various non-linearities. These models did not utilize explicit structural information; instead, a sliding window approach was employed to implicitly consider the context of neighboring residues and bases. We explored different training techniques to address the issue of highly unbalanced data. Among the seven most popular non-linearities for feed-forward ANNs, only three: Rectified Linear Unit (ReLU), Gated Linear Unit (GLU), and Hyperbolic Tangent (Tanh) yielded converging models. Common re-balancing techniques, such as under- and over-sampling of training sets, proved ineffective, whereas increasing the volume of training data and using model ensembles significantly improved performance. The optimal context window size, balancing both false negative and false positive errors, was found to be approximately 30 residues and bases. Our findings indicate that high-accuracy protein-RNA binding prediction is achievable using computing hardware accessible to most educational and research institutions.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2511.05483.pdf' target='_blank'>https://arxiv.org/pdf/2511.05483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abigail Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05483">DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2509.25198.pdf' target='_blank'>https://arxiv.org/pdf/2509.25198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elbert Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25198">SOLD: SELFIES-based Objective-driven Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, machine learning has made a significant impact on de novo drug design. However, current approaches to creating novel molecules conditioned on a target protein typically rely on generating molecules directly in the 3D conformational space, which are often slow and overly complex. In this work, we propose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent diffusion model that generates molecules in a latent space derived from 1D SELFIES strings and conditioned on a target protein. In the process, we also train an innovative SELFIES transformer and propose a new way to balance losses when training multi-task machine learning models.Our model generates high-affinity molecules for the target protein in a simple and efficient way, while also leaving room for future improvements through the addition of more data.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2507.14908.pdf' target='_blank'>https://arxiv.org/pdf/2507.14908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Ayomide Olanrewaju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14908">Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research introduces the Theory of Partial Symmetry Enforced Attention Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to seamlessly integrate local symmetry awareness into the core architecture of self-attention mechanisms within Transformer models. We formalize the concept of local permutation subgroup actions on windows of biological data, proving that under such actions, the attention mechanism naturally decomposes into a direct sum of orthogonal irreducible components. Critically, these components are intrinsically aligned with the irreducible representations of the acting permutation subgroup, thereby providing a powerful mathematical basis for disentangling symmetric and asymmetric features. We show that PSEAD offers substantial advantages. These include enhanced generalization capabilities to novel biological motifs exhibiting similar partial symmetries, unprecedented interpretability by allowing direct visualization and analysis of attention contributions from different symmetry channels, and significant computational efficiency gains by focusing representational capacity on relevant symmetric subspaces. Beyond static data analysis, we extend PSEAD's applicability to dynamic biological processes within reinforcement learning paradigms, showcasing its potential to accelerate the discovery and optimization of biologically meaningful policies in complex environments like protein folding and drug discovery. This work lays the groundwork for a new generation of biologically informed, symmetry-aware artificial intelligence models.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2506.16309.pdf' target='_blank'>https://arxiv.org/pdf/2506.16309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gergely Flamich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16309">Data Compression with Relative Entropy Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few years, machine learning unlocked previously infeasible features for compression, such as providing guarantees for users' privacy or tailoring compression to specific data statistics (e.g., satellite images or audio recordings of animals) or users' audiovisual perception. This, in turn, has led to an explosion of theoretical investigations and insights that aim to develop new fundamental theories, methods and algorithms better suited for machine learning-based compressors. In this thesis, I contribute to this trend by investigating relative entropy coding, a mathematical framework that generalises classical source coding theory. Concretely, relative entropy coding deals with the efficient communication of uncertain or randomised information. One of its key advantages is that it extends compression methods to continuous spaces and can thus be integrated more seamlessly into modern machine learning pipelines than classical quantisation-based approaches. Furthermore, it is a natural foundation for developing advanced compression methods that are privacy-preserving or account for the perceptual quality of the reconstructed data. The thesis considers relative entropy coding at three conceptual levels: After introducing the basics of the framework, (1) I prove results that provide new, maximally tight fundamental limits to the communication and computational efficiency of relative entropy coding; (2) I use the theory of Poisson point processes to develop and analyse new relative entropy coding algorithms, whose performance attains the theoretic optima and (3) I showcase the strong practical performance of relative entropy coding by applying it to image, audio, video and protein data compression using small, energy-efficient, probabilistic neural networks called Bayesian implicit neural representations.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2505.24426.pdf' target='_blank'>https://arxiv.org/pdf/2505.24426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Gamez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24426">P: A Universal Measure of Predictive Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last thirty years, considerable progress has been made with the development of systems that can drive cars, play games, predict protein folding and generate natural language. These systems are described as intelligent and there has been a great deal of talk about the rapid increase in artificial intelligence and its potential dangers. However, our theoretical understanding of intelligence and ability to measure it lag far behind our capacity for building systems that mimic intelligent human behaviour. There is no commonly agreed definition of the intelligence that AI systems are said to possess. No-one has developed a practical measure that would enable us to compare the intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on the hypothesis that prediction is the most important component of intelligence. As an agent interacts with its normal environment, the accuracy of its predictions is summed up and the complexity of its predictions and perceived environment is accounted for using Kolmogorov complexity. Two experiments were carried out to evaluate the practical feasibility of the algorithm. These demonstrated that it could measure the intelligence of an agent embodied in a virtual maze and an agent that makes predictions about time-series data. This universal measure could be the starting point for a new comparative science of intelligence that ranks humans, animals and AIs on a single ratio scale.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2505.03105.pdf' target='_blank'>https://arxiv.org/pdf/2505.03105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xule Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03105">Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2504.08526.pdf' target='_blank'>https://arxiv.org/pdf/2504.08526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Rathkopf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08526">Hallucination, reliability, and the role of generative AI in science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is increasingly used in scientific domains, from protein folding to climate modeling. But these models produce distinctive errors known as hallucinations - outputs that are incorrect yet superficially plausible. Worse, some arguments suggest that hallucinations are an inevitable consequence of the mechanisms underlying generative inference. Fortunately, such arguments rely on a conception of hallucination defined solely with respect to internal properties of the model, rather than in reference to the empirical target system. This conception fails to distinguish epistemically benign errors from those that threaten scientific inference. I introduce the concept of corrosive hallucination to capture the epistemically troubling subclass: misrepresentations that are substantively misleading and resistant to systematic anticipation. I argue that although corrosive hallucinations do pose a threat to scientific reliability, they are not inevitable. Scientific workflows such as those surrounding AlphaFold and GenCast, both of which serve as case studies, can neutralize their effects by imposing theoretical constraints during training, and by strategically screening for errors at inference time. When embedded in such workflows, generative AI can reliably contribute to scientific knowledge.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2504.04654.pdf' target='_blank'>https://arxiv.org/pdf/2504.04654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ngoc-Quang Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04654">EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of compound-protein interactions (CPI) remains a cornerstone challenge in computational drug discovery. While existing sequence-based approaches leverage molecular fingerprints or graph representations, they critically overlook three-dimensional (3D) structural determinants of binding affinity. To bridge this gap, we present EquiCPI, an end-to-end geometric deep learning framework that synergizes first-principles structural modeling with SE(3)-equivariant neural networks. Our pipeline transforms raw sequences into 3D atomic coordinates via ESMFold for proteins and DiffDock-L for ligands, followed by physics-guided conformer re-ranking and equivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant message passing over atomic point clouds, preserving symmetry under rotations, translations, and reflections, while hierarchically encoding local interaction patterns through tensor products of spherical harmonics. The proposed model is evaluated on BindingDB (affinity prediction) and DUD-E (virtual screening), EquiCPI achieves performance on par with or exceeding the state-of-the-art deep learning competitors.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2504.01389.pdf' target='_blank'>https://arxiv.org/pdf/2504.01389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01389">De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>De novo molecular design has extensive applications in drug discovery and materials science. The vast chemical space renders direct molecular searches computationally prohibitive, while traditional experimental screening is both time- and labor-intensive. Efficient molecular generation and screening methods are therefore essential for accelerating drug discovery and reducing costs. Although reinforcement learning (RL) has been applied to optimize molecular properties via reward mechanisms, its practical utility is limited by issues in training efficiency, convergence, and stability. To address these challenges, we adopt Direct Preference Optimization (DPO) from NLP, which uses molecular score-based sample pairs to maximize the likelihood difference between high- and low-quality molecules, effectively guiding the model toward better compounds. Moreover, integrating curriculum learning further boosts training efficiency and accelerates convergence. A systematic evaluation of the proposed method on the GuacaMol Benchmark yielded excellent scores. For instance, the method achieved a score of 0.883 on the Perindopril MPO task, representing a 6\% improvement over competing models. And subsequent target protein binding experiments confirmed its practical efficacy. These results demonstrate the strong potential of DPO for molecular design tasks and highlight its effectiveness as a robust and efficient solution for data-driven drug discovery.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2503.17368.pdf' target='_blank'>https://arxiv.org/pdf/2503.17368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Romain Lacombe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17368">Non-Canonical Crosslinks Confound Evolutionary Protein Structure Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolution-based protein structure prediction models have achieved breakthrough success in recent years. However, they struggle to generalize beyond evolutionary priors and on sequences lacking rich homologous data. Here we present a novel, out-of-domain benchmark based on sactipeptides, a rare class of ribosomally synthesized and post-translationally modified peptides (RiPPs) characterized by sulfur-to-$Î±$-carbon thioether bridges creating cross-links between cysteine residues and backbone. We evaluate recent models on predicting conformations compatible with these cross-links bridges for the 10 known sactipeptides with elucidated post-translational modifications. Crucially, the structures of 5 of them have not yet been experimentally resolved. This makes the task a challenging problem for evolution-based models, which we find exhibit limited performance (0.0% to 19.2% GDT-TS on sulfur-to-$Î±$-carbon distance). Our results point at the need for physics-informed models to sustain progress in biomolecular structure prediction.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2503.14806.pdf' target='_blank'>https://arxiv.org/pdf/2503.14806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pawel Rubach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14806">Applying Large-Scale Distributed Computing to Structural Bioinformatics -- Bridging Legacy HPC Clusters With Big Data Technologies Using kafka-slurm-agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Kafka Slurm Agent (KSA), an open source (Apache 2.0 license) distributed computing and stream processing engine designed to help researchers distribute Python-based computational tasks across multiple Slurm-managed HPC clusters and workstations. Written entirely in Python, this extensible framework utilizes an Apache Kafka broker for asynchronous communication between its components. It is intended for non-expert users and does not require administrative privileges or additional libraries to run on Slurm. The framework's development was driven by the introduction of the AlphaFold protein structure prediction model, specifically, it was first created to facilitate the detection of knots in protein chains within structures predicted by AlphaFold. KSA has since been applied to several structural bioinformatics research projects, among others, leading to the discovery of new knotted proteins with previously unknown knot types. These knotted structures are now part of the AlphaKnot 2.0 web server and database, where KSA is applied to manage the knot detection process for user-uploaded structures.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2501.01477.pdf' target='_blank'>https://arxiv.org/pdf/2501.01477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01477">A Survey of Deep Learning Methods in Protein Bioinformatics and its Impact on Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proteins are sequences of amino acids that serve as the basic building blocks of living organisms. Despite rapidly growing databases documenting structural and functional information for various protein sequences, our understanding of proteins remains limited because of the large possible sequence space and the complex inter- and intra-molecular forces. Deep learning, which is characterized by its ability to learn relevant features directly from large datasets, has demonstrated remarkable performance in fields such as computer vision and natural language processing. It has also been increasingly applied in recent years to the data-rich domain of protein sequences with great success, most notably with Alphafold2's breakout performance in the protein structure prediction. The performance improvements achieved by deep learning unlocks new possibilities in the field of protein bioinformatics, including protein design, one of the most difficult but useful tasks. In this paper, we broadly categorize problems in protein bioinformatics into three main categories: 1) structural prediction, 2) functional prediction, and 3) protein design, and review the progress achieved from using deep learning methodologies in each of them. We expand on the main challenges of the protein design problem and highlight how advances in structural and functional prediction have directly contributed to design tasks. Finally, we conclude by identifying important topics and future research directions.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2412.21103.pdf' target='_blank'>https://arxiv.org/pdf/2412.21103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linus Zwaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.21103">Parallel DNA Sequence Alignment on High-Performance Systems with CUDA and MPI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequence alignment is a cornerstone of bioinformatics, widely used to identify similarities between DNA, RNA, and protein sequences and studying evolutionary relationships and functional properties. The Needleman-Wunsch algorithm remains a robust and accurate method for global sequence alignment. However, its computational complexity, O(mn), poses significant challenges when processing large-scale datasets or performing multiple sequence alignments. To address these limitations, a hybrid implementation of the Needleman-Wunsch algorithm that leverages CUDA for parallel execution on GPUs and MPI for distributed computation across multiple nodes on a supercomputer is proposed. CUDA efficiently offloads computationally intensive tasks to GPU cores, while MPI enables communication and workload distribution across nodes to handle large-scale alignments.
  This work details the implementation and performance evaluation of the Needleman-Wunsch algorithm in a massively parallel computing environment. Experimental results demonstrate significant acceleration of the alignment process compared to traditional CPU-based implementations, particularly for large input sizes and multiple sequence alignments. In summary, the combination of CUDA and MPI effectively overcomes the computational bottlenecks inherent to the Needleman-Wunsch algorithm without requiring substantial modifications to the underlying algorithm, highlighting the potential of high-performance computing in advancing sequence alignment workflows.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2412.12214.pdf' target='_blank'>https://arxiv.org/pdf/2412.12214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Zamio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12214">DLSOM: A Deep learning-based strategy for liver cancer subtyping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Liver cancer is a leading cause of cancer-related mortality worldwide, with its high genetic heterogeneity complicating diagnosis and treatment. This study introduces DLSOM, a deep learning framework utilizing stacked autoencoders to analyze the complete somatic mutation landscape of 1,139 liver cancer samples, covering 20,356 protein-coding genes. By transforming high-dimensional mutation data into three low-dimensional features, DLSOM enables robust clustering and identifies five distinct liver cancer subtypes with unique mutational, functional, and biological profiles. Subtypes SC1 and SC2 exhibit higher mutational loads, while SC3 has the lowest, reflecting mutational heterogeneity. Novel and COSMIC-associated mutational signatures reveal subtype-specific molecular mechanisms, including links to hypermutation and chemotherapy resistance. Functional analyses further highlight the biological relevance of each subtype. This comprehensive framework advances precision medicine in liver cancer by enabling the development of subtype-specific diagnostics, biomarkers, and therapies, showcasing the potential of deep learning in addressing cancer complexity.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2412.07678.pdf' target='_blank'>https://arxiv.org/pdf/2412.07678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07678">Can linguists better understand DNA?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multilingual transfer ability, which reflects how well models fine-tuned on one source language can be applied to other languages, has been well studied in multilingual pre-trained models. However, the existence of such capability transfer between natural language and gene sequences/languages remains under explored.This study addresses this gap by drawing inspiration from the sentence-pair classification task used for evaluating sentence similarity in natural language. We constructed two analogous tasks: DNA-pair classification(DNA sequence similarity) and DNA-protein-pair classification(gene coding determination). These tasks were designed to validate the transferability of capabilities from natural language to gene sequences. Even a small-scale pre-trained model like GPT-2-small, which was pre-trained on English, achieved an accuracy of 78% on the DNA-pair classification task after being fine-tuned on English sentence-pair classification data(XTREME PAWS-X). While training a BERT model on multilingual text, the precision reached 89%. On the more complex DNA-protein-pair classification task, however, the model's output was barely distinguishable from random output.Experimental validation has confirmed that the transfer of capabilities from natural language to biological language is unequivocally present. Building on this foundation, we have also investigated the impact of model parameter scale and pre-training on this capability transfer. We provide recommendations for facilitating the transfer of capabilities from natural language to genetic language,as well as new approaches for conducting biological research based on this capability.This study offers an intriguing new perspective on exploring the relationship between natural language and genetic language.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2410.22652.pdf' target='_blank'>https://arxiv.org/pdf/2410.22652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caleb Musfeldt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22652">Development of a Python-Based Software for Calculating the Jones Polynomial: Insights into the Behavior of Polymers and Biopolymers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This thesis details a Python-based software designed to calculate the Jones polynomial, a vital mathematical tool from Knot Theory used for characterizing the topological and geometrical complexity of curves in \( \mathbb{R}^3 \), which is essential in understanding physical systems of filaments, including the behavior of polymers and biopolymers. The Jones polynomial serves as a topological invariant capable of distinguishing between different knot structures. This capability is fundamental to characterizing the architecture of molecular chains, such as proteins and DNA. Traditional computational methods for deriving the Jones polynomial have been limited by closure-schemes and high execution costs, which can be impractical for complex structures like those that appear in real life. This software implements methods that significantly reduce calculation times, allowing for more efficient and practical applications in the study of biological polymers. It utilizes a divide-and-conquer approach combined with parallel computing and applies recursive Reidemeister moves to optimize the computation, transitioning from an exponential to a near-linear runtime for specific configurations. This thesis provides an overview of the software's functions, detailed performance evaluations using protein structures as test cases, and a discussion of the implications for future research and potential algorithmic improvements.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2410.01755.pdf' target='_blank'>https://arxiv.org/pdf/2410.01755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Sholehrasa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01755">Integrating Protein Sequence and Expression Level to Analysis Molecular Characterization of Breast Cancer Subtypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Breast cancer's complexity and variability pose significant challenges in understanding its progression and guiding effective treatment. This study aims to integrate protein sequence data with expression levels to improve the molecular characterization of breast cancer subtypes and predict clinical outcomes. Using ProtGPT2, a language model designed for protein sequences, we generated embeddings that capture the functional and structural properties of proteins sequence. These embeddings were integrated with protein expression level to form enriched biological representations, which were analyzed using machine learning methods like ensemble K-means for clustering and XGBoost for classification. Our approach enabled successful clustering of patients into biologically distinct groups and accurately predicted clinical outcomes such as survival and biomarkers status, achieving high performance metrics, notably an F1 score of 0.88 for survival and 0.87 for biomarkers status prediction. Feature importance analysis identified KMT2C, CLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal remodeling, and therapy resistance in hormone receptor-positive and triple-negative breast cancer, with potential influence on breast cancer subtype behavior and progression. Furthermore, protein-protein interaction networks and correlation analyses revealed functional interdependencies among proteins that may influence breast cancer subtype behavior and progression. These findings suggest that integrating protein sequence and expression data provides valuable insights into tumor biology and has significant potential to enhance personalized treatment strategies in breast cancer care.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2409.16333.pdf' target='_blank'>https://arxiv.org/pdf/2409.16333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16333">Predicting Distance matrix with large language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural prediction has long been considered critical in RNA research, especially following the success of AlphaFold2 in protein studies, which has drawn significant attention to the field. While recent advances in machine learning and data accumulation have effectively addressed many biological tasks, particularly in protein related research. RNA structure prediction remains a significant challenge due to data limitations. Obtaining RNA structural data is difficult because traditional methods such as nuclear magnetic resonance spectroscopy, Xray crystallography, and electron microscopy are expensive and time consuming. Although several RNA 3D structure prediction methods have been proposed, their accuracy is still limited. Predicting RNA structural information at another level, such as distance maps, remains highly valuable. Distance maps provide a simplified representation of spatial constraints between nucleotides, capturing essential relationships without requiring a full 3D model. This intermediate level of structural information can guide more accurate 3D modeling and is computationally less intensive, making it a useful tool for improving structural predictions. In this work, we demonstrate that using only primary sequence information, we can accurately infer the distances between RNA bases by utilizing a large pretrained RNA language model coupled with a well trained downstream transformer.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2409.06428.pdf' target='_blank'>https://arxiv.org/pdf/2409.06428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Rydzewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06428">Spectral Map for Slow Collective Variables, Markovian Dynamics, and Transition State Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the behavior of complex molecular systems is a fundamental problem in physical chemistry. To describe the long-time dynamics of such systems, which is responsible for their most informative characteristics, we can identify a few slow collective variables (CVs) while treating the remaining fast variables as thermal noise. This enables us to simplify the dynamics and treat it as diffusion in a free-energy landscape spanned by slow CVs, effectively rendering the dynamics Markovian. Our recent statistical learning technique, spectral map [Rydzewski, J. Phys. Chem. Lett. 2023, 14, 22, 5216-5220], explores this strategy to learn slow CVs by maximizing a spectral gap of a transition matrix. In this work, we introduce several advancements into our framework, using a high-dimensional reversible folding process of a protein as an example. We implement an algorithm for coarse-graining Markov transition matrices to partition the reduced space of slow CVs kinetically and use it to define a transition state ensemble. We show that slow CVs learned by spectral map closely approach the Markovian limit for an overdamped diffusion. We demonstrate that coordinate-dependent diffusion coefficients only slightly affect the constructed free-energy landscapes. Finally, we present how spectral map can be used to quantify the importance of features and compare slow CVs with structural descriptors commonly used in protein folding. Overall, we demonstrate that a single slow CV learned by spectral map can be used as a physical reaction coordinate to capture essential characteristics of protein folding.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2408.07154.pdf' target='_blank'>https://arxiv.org/pdf/2408.07154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ralph P. Lano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07154">Self-folding Self-replication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by protein folding, we explored the construction of three-dimensional structures and machines from one-dimensional chains of simple building blocks. This approach not only allows us to recreate the self-replication mechanism introduced earlier, but also significantly simplifies the process. We introduced a new set of folding blocks that facilitate the formation of secondary structures such as Î±-helices and \b{eta}-sheets, as well as more advanced tertiary and quaternary structures, including self-replicating machines. The introduction of rotational degrees of freedom leads to a reduced variety of blocks and, most importantly, reduces the overall size of the machines by a factor of five. In addition, we present a universal copier-constructor, a highly efficient self-replicating mechanism composed of approximately 40 blocks, including the restictions posed on it. The paper also addresses evolutionary considerations, outlining several steps on the evolutionary ladder towards more sophisticated self-replicating systems. Finally, this study offers a clear rationale for nature's preference for one-dimensional chains in constructing three-dimensional structures.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2407.10452.pdf' target='_blank'>https://arxiv.org/pdf/2407.10452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amritpal Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10452">GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate drug target affinity prediction can improve drug candidate selection, accelerate the drug discovery process, and reduce drug production costs. Previous work focused on traditional fingerprints or used features extracted based on the amino acid sequence in the protein, ignoring its 3D structure which affects its binding affinity. In this work, we propose GraphPrint: a framework for incorporating 3D protein structure features for drug target affinity prediction. We generate graph representations for protein 3D structures using amino acid residue location coordinates and combine them with drug graph representation and traditional features to jointly learn drug target affinity. Our model achieves a mean square error of 0.1378 and a concordance index of 0.8929 on the KIBA dataset and improves over using traditional protein features alone. Our ablation study shows that the 3D protein structure-based features provide information complementary to traditional features.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2407.00111.pdf' target='_blank'>https://arxiv.org/pdf/2407.00111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Fauber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00111">Accurate Prediction of Ligand-Protein Interaction Affinities with Fine-Tuned Small Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We describe the accurate prediction of ligand-protein interaction (LPI) affinities, also known as drug-target interactions (DTI), with instruction fine-tuned pretrained generative small language models (SLMs). We achieved accurate predictions for a range of affinity values associated with ligand-protein interactions on out-of-sample data in a zero-shot setting. Only the SMILES string of the ligand and the amino acid sequence of the protein were used as the model inputs. Our results demonstrate a clear improvement over machine learning (ML) and free-energy perturbation (FEP+) based methods in accurately predicting a range of ligand-protein interaction affinities, which can be leveraged to further accelerate drug discovery campaigns against challenging therapeutic targets.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2405.19076.pdf' target='_blank'>https://arxiv.org/pdf/2405.19076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus J. Buehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19076">Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Cephalo, a series of multimodal vision large language models (V-LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding. A key innovation of Cephalo is its advanced dataset generation method. Cephalo is trained on integrated image and text data from thousands of scientific papers and science-focused Wikipedia data demonstrates can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports multimodal natural language understanding, which can be coupled with other generative methods to create an image-to-text-to-3D pipeline. To develop more capable models from smaller ones, we report both mixture-of-expert methods and model merging. We examine the models in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio-inspired design based on insect behavior. Generative applications include bio-inspired designs, including pollen-inspired architected materials, as well as the synthesis of bio-inspired material microstructures from a photograph of a solar eclipse. Additional model fine-tuning with a series of molecular dynamics results demonstrate Cephalo's enhanced capabilities to accurately predict statistical features of stress and atomic energy distributions, as well as crack dynamics and damage in materials.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2405.12986.pdf' target='_blank'>https://arxiv.org/pdf/2405.12986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saddam Hussain Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12986">A Novel Feature Map Enhancement Technique Integrating Residual CNN and Transformer for Alzheimer Diseases Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer diseases (ADs) involves cognitive decline and abnormal brain protein accumulation, necessitating timely diagnosis for effective treatment. Therefore, CAD systems leveraging deep learning advancements have demonstrated success in AD detection but pose computational intricacies and the dataset minor contrast, structural, and texture variations. In this regard, a novel hybrid FME-Residual-HSCMT technique is introduced, comprised of residual CNN and Transformer concepts to capture global and local fine-grained AD analysis in MRI. This approach integrates three distinct elements: a novel CNN Meet Transformer (HSCMT), customized residual learning CNN, and a new Feature Map Enhancement (FME) strategy to learn diverse morphological, contrast, and texture variations of ADs. The proposed HSCMT at the initial stage utilizes stem convolution blocks that are integrated with CMT blocks followed by systematic homogenous and structural (HS) operations. The customized CMT block encapsulates each element with global contextual interactions through multi-head attention and facilitates computational efficiency through lightweight. Moreover, inverse residual and stem CNN in customized CMT enables effective extraction of local texture information and handling vanishing gradients. Furthermore, in the FME strategy, residual CNN blocks utilize TL-based generated auxiliary and are combined with the proposed HSCMT channels at the target level to achieve diverse enriched feature space. Finally, diverse enhanced channels are fed into a novel spatial attention mechanism for optimal pixel selection to reduce redundancy and discriminate minor contrast and texture inter-class variation. The proposed achieves an F1-score (98.55%), an accuracy of 98.42% and a sensitivity of 98.50%, a precision of 98.60% on the standard Kaggle dataset, and demonstrates outperformance existing ViTs and CNNs methods.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2405.10824.pdf' target='_blank'>https://arxiv.org/pdf/2405.10824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Rucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10824">Real-World Graph Analysis: Techniques for Static, Dynamic, and Temporal Communities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphs are widely used in various fields of computer science. They have also found application in unrelated areas, leading to a diverse range of problems. These problems can be modeled as relationships between entities in various contexts, such as social networks, protein interactions in cells, and route maps. Therefore it is logical to analyze these data structures with diverse approaches, whether they are numerical or structural, global or local, approximate or exact. In particular, the concept of community plays an important role in local structural analysis, as it is able to highlight the composition of the underlying graph while providing insights into what the organization and importance of the nodes in a network look like. This thesis pursues the goal of extracting knowledge from different kinds of graphs, including static, dynamic, and temporal graphs, with a particular focus on their community substructures. To tackle this task we use combinatorial algorithms that can list all the communities in a graph according to different formalizations, such as cliques, $k$-graphlets, and $k$-cores. We first develop new algorithms to enumerate subgraphs, using traditional and novel techniques such as push-out amortization, and CPU cache analysis to boost their efficiency. We then extend these concepts to the analysis of real-world graphs across diverse domains, ranging from social networks to autonomous systems modeled as temporal graphs. In this field, there is currently no widely accepted adaptation, even for straightforward subgraphs like $k$-cores, and the available data is expanding both in terms of quantity and scale. As a result, our findings advance the state of the art both from a theoretical and a practical perspective and can be used in a static or dynamic setting to further speed up and refine graph analysis techniques.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2405.06651.pdf' target='_blank'>https://arxiv.org/pdf/2405.06651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Swaroop
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06651">Using GANs for De Novo Protein Design Targeting Microglial IL-3R$Î±$ to Inhibit Alzheimer's Progression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>IL-3 is a hemopoietic growth factor that usually targets blood cell precursors; IL-3R is a cytokine receptor that binds to IL-3. However, IL-3 takes on a different role in the context of glial cells in the nervous system, where studies show that the protein IL-3 protects against Alzheimer's disease by activating microglia at their IL-3R receptors, causing the microglia to clear out the tangles caused by the build-up of misfolded Tau proteins. In this study, we seek to ascertain what role the secondary structure of IL-3 plays in its binding with the receptor. The motivation behind this study is to learn more about the mechanism and identify possible drugs that might be able to activate it, in hopes of inhibiting the spread of Alzheimer's Disease. From a preliminary analysis of complexes containing IL-3 and IL-3R, we hypothesized that the binding is largely due to the interactions of three alpha helix structures stretching towards the active site on the receptor. The original Il-3 protein serves as the control in this experiment; the other proteins being tested are generated through several types of computational de novo protein design, where machine learning allows for the production of entirely novel structures. The efficacy of the generated proteins is assessed through docking simulations with the IL-3R receptor, and the binding poses are also qualitatively examined to gain insight into the function of the binding. From the docking data and poses, the most successful proteins were those with similar secondary structure to IL-3.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2405.01619.pdf' target='_blank'>https://arxiv.org/pdf/2405.01619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexuan Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01619">An Efficient Finite Element Solver for a Nonuniform size-modified Poisson-Nernst-Planck Ion Channel Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an efficient finite element iterative method for solving a nonuniform size-modified Poisson-Nernst-Planck ion channel (SMPNPIC) model, along with a SMPNPIC program package that works for an ion channel protein with a three-dimensional crystallographic structure and an ionic solvent with multiple ionic species. In particular, the SMPNPIC model is constructed and then reformulated by novel mathematical techniques so that each iteration of the method only involves linear boundary value problems and nonlinear algebraic systems, circumventing the numerical difficulties caused by the strong nonlinearities, strong asymmetries, and strong differential equation coupling of the SMPNPIC model. To further improve the method's efficiency, an efficient modified Newton iterative method is adapted to the numerical solution of each related nonlinear algebraic system. Numerical results for a voltage-dependent anion channel (VDAC) and a mixture solution of four ionic species demonstrate the method's convergence, the package's high performance, and the importance of considering nonuniform ion size effects. They also partially validate the SMPNPIC model by the anion selectivity property of VDAC.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2404.14169.pdf' target='_blank'>https://arxiv.org/pdf/2404.14169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Corti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14169">Exploring tau protein and amyloid-beta propagation: a sensitivity analysis of mathematical models based on biological data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer's disease is the most common dementia worldwide. Its pathological development is well known to be connected with the accumulation of two toxic proteins: tau protein and amyloid-$Î²$. Mathematical models and numerical simulations can predict the spreading patterns of misfolded proteins in this context. However, the calibration of the model parameters plays a crucial role in the final solution. In this work, we perform a sensitivity analysis of heterodimer and Fisher-Kolmogorov models to evaluate the impact of the equilibrium values of protein concentration on the solution patterns. We adopt advanced numerical methods such as the IMEX-DG method to accurately describe the propagating fronts in the propagation phenomena in a polygonal mesh of sagittal patient-specific brain geometry derived from magnetic resonance images. We calibrate the model parameters using biological measurements in the brain cortex for the tau protein and the amyloid-$Î²$ in Alzheimer's patients and controls. Finally, using the sensitivity analysis results, we discuss the applicability of both models in the correct simulation of the spreading of the two proteins.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2403.17293.pdf' target='_blank'>https://arxiv.org/pdf/2403.17293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salim Sazzed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17293">Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the structures of biological macromolecules is highly important as they are closely associated with cellular functionalities. Comprehending the precise organization actin filaments is crucial because they form the dynamic cytoskeleton, which offers structural support to cells and connects the cell's interior with its surroundings. However, determining the precise organization of actin filaments is challenging due to the poor quality of cryo-electron tomography (cryo-ET) images, which suffer from low signal-to-noise (SNR) ratios and the presence of missing wedge, as well as diverse shape characteristics of actin filaments. To address these formidable challenges, the primary component of this dissertation focuses on developing sophisticated computational techniques for tracing actin filaments. In particular, three novel methodologies have been developed: i) BundleTrac, for tracing bundle-like actin filaments found in Stereocilium, ii) Spaghetti Tracer, for tracing filaments that move individually with loosely cohesive movements, and iii) Struwwel Tracer, for tracing randomly orientated actin filaments in the actin network. The second component of the dissertation introduces a convolutional neural network (CNN) based segmentation model to determine the location of protein secondary structures, such as helices and beta-sheets, in medium-resolution (5-10 Angstrom) 3-dimensional cryo-electron microscopy (cryo-EM) images. This methodology later evolved into a tool named DeepSSETracer. The final component of the dissertation presents a novel algorithm, cylindrical fit measure, to estimate image structure match at helix regions in medium-resolution cryo-EM images. Overall, my dissertation has made significant contributions to addressing critical research challenges in structural biology by introducing various computational methods and tools.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2403.15419.pdf' target='_blank'>https://arxiv.org/pdf/2403.15419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinwei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15419">Attention is all you need for boosting graph convolutional neural network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Convolutional Neural Networks (GCNs) possess strong capabilities for processing graph data in non-grid domains. They can capture the topological logical structure and node features in graphs and integrate them into nodes' final representations. GCNs have been extensively studied in various fields, such as recommendation systems, social networks, and protein molecular structures. With the increasing application of graph neural networks, research has focused on improving their performance while compressing their size. In this work, a plug-in module named Graph Knowledge Enhancement and Distillation Module (GKEDM) is proposed. GKEDM can enhance node representations and improve the performance of GCNs by extracting and aggregating graph information via multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary transferor for knowledge distillation. With a specially designed attention distillation method, GKEDM can distill the knowledge of large teacher models into high-performance and compact student models. Experiments on multiple datasets demonstrate that GKEDM can significantly improve the performance of various GCNs with minimal overhead. Furthermore, it can efficiently transfer distilled knowledge from large teacher networks to small student networks via attention distillation.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2402.04944.pdf' target='_blank'>https://arxiv.org/pdf/2402.04944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Esfandiar Nava-Yazdani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04944">Elastic Analysis of Augmented Curves and Constrained Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The square root velocity transformation provides a convenient and numerically efficient approach to functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces, spherical strips, protein molecules and hurricane tracks.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2401.11562.pdf' target='_blank'>https://arxiv.org/pdf/2401.11562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratik Worah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11562">Enhancing selectivity using Wasserstein distance based reweighing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.
  On the theoretical side, we prove that when the metric entropy of the input datasets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.
  As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). In our example dataset, of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\% but not MNK1, at 10$Î¼$M -- a 5\% success rate.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2401.08668.pdf' target='_blank'>https://arxiv.org/pdf/2401.08668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Neukart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08668">Thermodynamic Perspectives on Computational Complexity: Exploring the P vs. NP Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The resolution of the P vs. NP problem, a cornerstone in computational theory, remains elusive despite extensive exploration through mathematical logic and algorithmic theory. This paper takes a novel approach by integrating information theory, thermodynamics, and computational complexity, offering a comprehensive landscape of interdisciplinary study. We focus on entropy, a concept traditionally linked with uncertainty and disorder, and reinterpret it to assess the complexity of computational problems. Our research presents a structured framework for establishing entropy profiles within computational tasks, enabling a clear distinction between P and NP-classified problems. This framework quantifies the 'information cost' associated with these problem categories, highlighting their intrinsic computational complexity. We introduce Entropy-Driven Annealing (EDA) as a new method to decipher the energy landscapes of computational problems, focusing on the unique characteristics of NP problems. This method proposes a differential thermodynamic profile for NP problems in contrast to P problems and explores potential thermodynamic routes for finding polynomial-time solutions to NP challenges. Our introduction of EDA and its application to complex computational problems like the Boolean satisfiability problem (SAT) and protein-DNA complexes suggests a potential pathway toward unraveling the intricacies of the P vs. NP problem.
